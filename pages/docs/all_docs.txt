Directory structure:
└── docs/
    ├── _meta.ts
    ├── index.mdx
    ├── configuration/
    │   ├── _meta.ts
    │   ├── azure.mdx
    │   ├── docker_override.mdx
    │   ├── dotenv.mdx
    │   ├── index.mdx
    │   ├── logging.mdx
    │   ├── meilisearch.mdx
    │   ├── metrics.mdx
    │   ├── mod_system.mdx
    │   ├── rag_api.mdx
    │   ├── stt_tts.mdx
    │   ├── token_usage.mdx
    │   ├── authentication/
    │   │   ├── _meta.ts
    │   │   ├── email.mdx
    │   │   ├── index.mdx
    │   │   ├── ldap.mdx
    │   │   └── OAuth2-OIDC/
    │   │       ├── _meta.ts
    │   │       ├── apple.mdx
    │   │       ├── authelia.mdx
    │   │       ├── authentik.mdx
    │   │       ├── aws.mdx
    │   │       ├── azure.mdx
    │   │       ├── discord.mdx
    │   │       ├── facebook.mdx
    │   │       ├── github.mdx
    │   │       ├── google.mdx
    │   │       ├── index.mdx
    │   │       └── keycloak.mdx
    │   ├── cdn/
    │   │   ├── _meta.ts
    │   │   ├── azure.mdx
    │   │   ├── firebase.mdx
    │   │   ├── index.mdx
    │   │   └── s3.mdx
    │   ├── librechat_yaml/
    │   │   ├── _meta.ts
    │   │   ├── example.mdx
    │   │   ├── index.mdx
    │   │   ├── setup.mdx
    │   │   ├── ai_endpoints/
    │   │   │   ├── _meta.ts
    │   │   │   ├── anyscale.mdx
    │   │   │   ├── apipie.mdx
    │   │   │   ├── cohere.mdx
    │   │   │   ├── databricks.mdx
    │   │   │   ├── deepseek.mdx
    │   │   │   ├── fireworks.mdx
    │   │   │   ├── groq.mdx
    │   │   │   ├── huggingface.mdx
    │   │   │   ├── index.mdx
    │   │   │   ├── litellm.mdx
    │   │   │   ├── mistral.mdx
    │   │   │   ├── mlx.mdx
    │   │   │   ├── neurochain.mdx
    │   │   │   ├── ollama.mdx
    │   │   │   ├── openrouter.mdx
    │   │   │   ├── perplexity.mdx
    │   │   │   ├── portkey.mdx
    │   │   │   ├── shuttleai.mdx
    │   │   │   ├── togetherai.mdx
    │   │   │   ├── vultrcloudinference.mdx
    │   │   │   └── xai.mdx
    │   │   └── object_structure/
    │   │       ├── _meta.ts
    │   │       ├── actions.mdx
    │   │       ├── agents.mdx
    │   │       ├── assistants_endpoint.mdx
    │   │       ├── aws_bedrock.mdx
    │   │       ├── azure_openai.mdx
    │   │       ├── balance.mdx
    │   │       ├── config.mdx
    │   │       ├── custom_endpoint.mdx
    │   │       ├── default_params.mdx
    │   │       ├── file_config.mdx
    │   │       ├── interface.mdx
    │   │       ├── mcp_servers.mdx
    │   │       ├── model_config.mdx
    │   │       ├── model_specs.mdx
    │   │       ├── ocr.mdx
    │   │       ├── registration.mdx
    │   │       └── shared_endpoint_settings.mdx
    │   ├── mongodb/
    │   │   ├── mongodb_atlas.mdx
    │   │   ├── mongodb_auth.mdx
    │   │   └── mongodb_community.mdx
    │   ├── pre_configured_ai/
    │   │   ├── _meta.ts
    │   │   ├── anthropic.mdx
    │   │   ├── assistants.mdx
    │   │   ├── bedrock.mdx
    │   │   ├── google.mdx
    │   │   ├── index.mdx
    │   │   └── openai.mdx
    │   └── tools/
    │       ├── _meta.ts
    │       ├── azure_ai_search.mdx
    │       ├── google_search.mdx
    │       ├── index.mdx
    │       ├── openweather.mdx
    │       ├── stable_diffusion.mdx
    │       └── wolfram.mdx
    ├── development/
    │   ├── _meta.ts
    │   ├── architecture.mdx
    │   ├── conventions.mdx
    │   ├── debugging.mdx
    │   ├── get_started.mdx
    │   ├── index.mdx
    │   ├── testing.mdx
    │   └── tools_and_plugins.mdx
    ├── documentation/
    │   ├── _meta.ts
    │   ├── examples.mdx
    │   ├── index.mdx
    │   └── syntax_highlighting.mdx
    ├── features/
    │   ├── _meta.ts
    │   ├── agents.mdx
    │   ├── artifacts.mdx
    │   ├── authentication.mdx
    │   ├── code_interpreter.mdx
    │   ├── fork.mdx
    │   ├── import_convos.mdx
    │   ├── index.mdx
    │   ├── mod_system.mdx
    │   ├── ocr.mdx
    │   ├── password_reset.mdx
    │   ├── plugins.mdx
    │   ├── rag_api.mdx
    │   ├── search.mdx
    │   ├── temporary_chat.mdx
    │   └── url_query.mdx
    ├── local/
    │   ├── _meta.ts
    │   ├── docker.mdx
    │   ├── helm_chart.mdx
    │   ├── index.mdx
    │   └── npm.mdx
    ├── quick_start/
    │   ├── _meta.ts
    │   ├── custom_endpoints.mdx
    │   ├── index.mdx
    │   └── local_setup.mdx
    ├── remote/
    │   ├── _meta.ts
    │   ├── cloudflare.mdx
    │   ├── digitalocean.mdx
    │   ├── docker_linux.mdx
    │   ├── huggingface.mdx
    │   ├── index.mdx
    │   ├── nginx.mdx
    │   ├── ngrok.mdx
    │   ├── railway.mdx
    │   └── traefik.mdx
    ├── translation/
    │   ├── _meta.ts
    │   └── index.mdx
    └── user_guides/
        ├── _meta.ts
        ├── ai_overview.mdx
        ├── index.mdx
        ├── mongodb.mdx
        └── presets.mdx

================================================
FILE: pages/docs/_meta.ts
================================================
export default {
  index: 'Get Started',
  quick_start: {
    title: '⚡ Quick Start',
    children: {
      index: 'Quick Start Overview',
    },
  },
  features: {
    title: '✨ Features',
    children: {
      index: 'Features Overview',
    },
  },
  local: '📦 Local Installation',
  remote: '☁️ Remote Hosting',
  configuration: '⚙️ Configuration',
  user_guides: '📘 User Guides',
  translation: '🌍 Translation',
  contributing: {
    // "title": "Contributing",
    type: 'separator',
  },
  development: 'Development',
  documentation: 'Documentation',
  other: {
    // "title": "Other",
    type: 'separator',
  },
}



================================================
FILE: pages/docs/index.mdx
================================================
---
title: Get Started
---

import {
  QuickStartLocal,
  CustomEndpoints,
  Logo,
  ToolKit,
  Changelog,
  Roadmap,
  Features,
  OurAuthors,
} from '@/components/CardIcons'

# Get Started

### Quick Start Guides

<div>
  <Cards num={2}>
    <Cards.Card title="" href="/docs/quick_start/local_setup" icon={<QuickStartLocal />} image />
    <Cards.Card
      title=""
      href="/docs/quick_start/custom_endpoints"
      icon={<CustomEndpoints />}
      image
    />
  </Cards>
</div>

### Explore our Documentation

<Cards num={6}>
  <Cards.Card href="/docs/local" title="Local Installation" image arrow>
    ![Local Installation](/images/cards/local.svg)
  </Cards.Card>
  <Cards.Card href="/docs/remote" title="Remote Hosting" image arrow>
    ![Remote Hosting](/images/cards/remote.svg)
  </Cards.Card>
  <Cards.Card href="/docs/configuration" title="Configuration" image arrow>
    ![Configuration](/images/cards/config.svg)
  </Cards.Card>
  <Cards.Card href="/docs/user_guides" title="User Guides" image arrow>
    ![Features](/images/cards/guides.svg)
  </Cards.Card>
  <Cards.Card href="/docs/development" title="Development" image arrow>
    ![Development](/images/cards/code.svg)
  </Cards.Card>
  <Cards.Card href="/docs/documentation" title="Documentation" image arrow>
    ![Documentation](/images/cards/docs.svg)
  </Cards.Card>
</Cards>

---

<div>
  <Cards num={3}>
    <Cards.Card title="" href="/about" icon={<Logo />} image />
    <Cards.Card title="" href="/changelog" icon={<Changelog />} image />
    <Cards.Card title="" href="/docs/features" icon={<Features />} image />
    <Cards.Card title="" href="/toolkit" icon={<ToolKit />} image />
    <Cards.Card title="" href="/blog/2024-02-19_2024_roadmap" icon={<Roadmap />} image />
    <Cards.Card title="" href="/authors" icon={<OurAuthors />} image />
  </Cards>
</div>



================================================
FILE: pages/docs/configuration/_meta.ts
================================================
export default {
  index: 'Intro',
  dotenv: 'Environment Variables',
  librechat_yaml: 'librechat.yaml',
  authentication: 'Authentication',
  mongodb: 'MongoDB',
  pre_configured_ai: 'AI Providers',
  tools: 'Tools and Plugins',
  cdn: 'CDN',
  azure: 'Azure OpenAI',
  docker_override: 'Docker Override',
  mod_system: 'Automated Moderation',
}



================================================
FILE: pages/docs/configuration/azure.mdx
================================================
---
title: Azure OpenAI
description: Comprehensive guide for configuring Azure OpenAI through the `librechat.yaml` file AKA the LibreChat Config file. This document is your one-stop resource for understanding and customizing Azure settings and models.
---

# Azure OpenAI

**Azure OpenAI Integration for LibreChat**

LibreChat boasts compatibility with Azure OpenAI API services, treating the endpoint as a first-class citizen. To properly utilize Azure OpenAI within LibreChat, it's crucial to configure the [`librechat.yaml` file](/docs/configuration/librechat_yaml/object_structure/azure_openai) according to your specific needs. This document guides you through the essential setup process which allows seamless use of multiple deployments and models with as much flexibility as needed.

## Example

Here's a quick snapshot of what a comprehensive configuration might look like, including many of the options and features discussed below.

```yaml filename="librechat.yaml"
endpoints:
  azureOpenAI:
    # Endpoint-level configuration
    titleModel: "llama-70b-chat"
    plugins: true
    assistants: true
    groups:
    # Group-level configuration
    - group: "my-resource-westus"
      apiKey: "${WESTUS_API_KEY}"
      instanceName: "my-resource-westus"
      version: "2024-03-01-preview"
      # Model-level configuration
      models:
        gpt-4-vision-preview:
          deploymentName: gpt-4-vision-preview
          version: "2024-03-01-preview"
        gpt-3.5-turbo:
          deploymentName: gpt-35-turbo
        gpt-4-1106-preview:
          deploymentName: gpt-4-1106-preview
    # Group-level configuration
    - group: "mistral-inference"
      apiKey: "${AZURE_MISTRAL_API_KEY}"
      baseURL: "https://Mistral-large-vnpet-serverless.region.inference.ai.azure.com/v1/chat/completions"
      serverless: true
      # Model-level configuration
      models:
        mistral-large: true
    # Group-level configuration
    - group: "my-resource-sweden"
      apiKey: "${SWEDEN_API_KEY}"
      instanceName: "my-resource-sweden"
      deploymentName: gpt-4-1106-preview
      version: "2024-03-01-preview"
      assistants: true
      # Model-level configuration
      models:
        gpt-4-turbo: true
```

Here's another working example configured according to the specifications of the [Azure OpenAI Endpoint Configuration Docs:](/docs/configuration/librechat_yaml/object_structure/azure_openai)

Each level of configuration is extensively detailed in their respective sections:

1. [Endpoint-level config](#endpoint-level-configuration)

2. [Group-level config](#group-level-configuration)

3. [Model-level config](#model-level-configuration)

## Setup

1. **Open `librechat.yaml` for Editing**: Use your preferred text editor or IDE to open and edit the `librechat.yaml` file.

    - Optional: use a remote or custom file path with the following environment variable:

    ```sh filename=".env"
    CONFIG_PATH="/alternative/path/to/librechat.yaml"
    ```

2. **Configure Azure OpenAI Settings**: Follow the detailed structure outlined below to populate your Azure OpenAI settings appropriately. This includes specifying API keys, instance names, model groups, and other essential configurations.

3. **Make sure to Remove Legacy Settings**: If you are using any of the [legacy configurations](#legacy-setup), be sure to remove. The LibreChat server will also detect these and remind you.

4. **Save Your Changes**: After accurately inputting your settings, save the `librechat.yaml` file.

5. **Restart LibreChat**: For the changes to take effect, restart your LibreChat application. This ensures that the updated configurations are loaded and utilized.

## Required Fields

To properly integrate Azure OpenAI with LibreChat, specific fields must be accurately configured in your `librechat.yaml` file. These fields are validated through a combination of custom and environmental variables to ensure the correct setup. Here are the detailed requirements based on the validation process:

## Endpoint-Level Configuration

Here's the conversion of the provided settings into the new option table format:

#### Global Azure Settings:

**Title and Conversation Settings:**
<OptionTable
  options={[
    ['titleModel', 'string', 'Specifies the model to use for generating conversation titles. If not provided, the default model is set as `gpt-3.5-turbo`, which will result in no titles if lacking this model. You can also set this to dynamically use the current model by setting it to `current_model`.', 'titleModel:'],
    ['plugins', 'boolean', 'Enables the use of plugins through Azure. Set to `true` to activate Plugins endpoint support through your Azure config. Default: `false`.', 'plugins:false'],
    ['assistants', 'boolean', 'Enables the use of assistants through Azure. Set to `true` to activate Assistants endpoint through your Azure config. Default: `false`. Note: this requires an assistants-compatible region.', 'assistants:false'],
    ['summarize', 'boolean', 'Enables conversation summarization for all Azure models. Set to `true` to activate summarization. Default: `false`.', 'summarize:false'],
    ['summaryModel', 'string', 'Specifies the model to use for generating conversation summaries. If not provided, the default behavior is to use the first model in the `default` array of the first group.', 'summaryModel:'],
    ['titleConvo', 'boolean', 'Enables conversation title generation for all Azure models. Set to `true` to activate title generation. Default: `false`.', 'titleConvo:false'],
    ['titleMethod', 'string', 'Specifies the method to use for generating conversation titles. Valid options are `"completion"` and `"functions"`. If not provided, the default behavior is to use the `"completion"` method.', 'titleMethod:completion'],
  ]}
/>

**Group Configuration:**
<OptionTable
  options={[
    ['groups', 'array', 'Specifies the list of Azure OpenAI model groups. Each group represents a set of models with shared configurations. The groups field is an array of objects, where each object defines the settings for a specific group. This is a required field at the endpoint level, and at least one group must be defined. The group-level configurations are detailed in the Group-Level Configuration section.', '# groups:[]'],
  ]}
/>

**Custom Order (Optional):**
<OptionTable
  options={[
    ['customOrder', 'number', 'Allows you to specify a custom order for the Azure endpoint in the user interface. Higher numbers will appear lower in the list. If not provided, the default order is determined by the order in which the endpoints are defined in the `librechat.yaml` file.', 'customOrder:'],
  ]}
/>

Please note that the `customOrder` option is commented out, as it was mentioned as optional in the original text. 

Here's an example of how you can configure these endpoint-level settings in your `librechat.yaml` file:

```yaml filename="librechat.yaml"
endpoints:
  azureOpenAI:
    titleModel: "gpt-3.5-turbo-1106"
    plugins: true
    assistants: true
    summarize: true
    summaryModel: "gpt-3.5-turbo-1106"
    titleConvo: true
    titleMethod: "functions"
    groups:
      # ... (group-level and model-level configurations)
```

## Group-Level Configuration

This is a breakdown of the fields configurable as defined for the Custom Config (`librechat.yaml`) file. For more information on each field, see the [Azure OpenAI section in the Custom Config Docs](./librechat_yaml/object_structure/azure_openai).

Group-Level Configuration:
Group Identification:
<OptionTable
options={[
['group', 'string', 'Unique identifier name for a group of models. Duplicate group names are not allowed and will result in validation errors.', 'group: default'],
]}
/>

Authentication:
<OptionTable
options={[
['apiKey', 'string', 'Must be a valid API key for Azure OpenAI services. It could be a direct key string or an environment variable reference (e.g., ${WESTUS_API_KEY}).', 'apiKey: ${AZURE_API_KEY}'],
]}
/>

Azure OpenAI Instance:
<OptionTable
options={[
['instanceName', 'string', 'Name of the Azure OpenAI instance. This field can also support environment variable references.', 'instanceName: ${AZURE_OPENAI_INSTANCE}'],
]}
/>

Deployment Configuration:
<OptionTable
options={[
['deploymentName', 'string', 'The deployment name at the group level is optional but required if any model within the group is set to true.', 'deploymentName: my-deployment'],
['version', 'string', 'The Azure OpenAI API version at the group level is optional but required if any model within the group is set to true.', 'version: 2023-03-15-preview'],
]}
/>

Advanced Settings:
<OptionTable
options={[
['baseURL', 'string', 'Custom base URL for the Azure OpenAI API requests. Environment variable references are supported. This is optional and can be used for advanced routing scenarios.', 'baseURL: https://my-custom-base-url.com'],
['additionalHeaders', 'object', 'Specifies any extra headers for Azure OpenAI API requests as key-value pairs. Environment variable references can be included as values.', 'additionalHeaders: {Authorization: ${AUTH_HEADER}}'],
['serverless', 'boolean', 'Specifies if the group is a serverless inference chat completions endpoint from Azure Model Catalog, for which only a model identifier, baseURL, and apiKey are needed. For more info, see serverless inference endpoints.', 'serverless: true'],
['addParams', 'object', 'Adds or overrides additional parameters for Azure OpenAI API requests. Useful for specifying API-specific options as key-value pairs.', 'addParams: {temperature: 0.7}'],
['dropParams', 'array', 'Allows for the exclusion of certain default parameters from Azure OpenAI API requests. Useful for APIs that do not accept or recognize specific parameters. This should be specified as a list of strings.', 'dropParams: [top_p, stop]'],
['forcePrompt', 'boolean', 'Dictates whether to send a prompt parameter instead of messages in the request body. This option is useful when needing to format the request in a manner consistent with OpenAI API expectations, particularly for scenarios preferring a single text payload.', 'forcePrompt: true'],
]}
/>

Model Configuration:
<OptionTable
options={[
['models', 'object', 'Specifies the mapping of model identifiers to their configurations within the group. The keys represent the model identifiers, which must match the corresponding OpenAI model names. The values can be either boolean (true) or objects containing model-specific settings. If a model is set to true, it inherits the group-level deploymentName and version. If a model is configured as an object, it can have its own deploymentName and version. This field is required, and at least one model must be defined within each group. More info here', 'models: {gpt-3.5-turbo: true, text-davinci-003: {}}'],
]}
/>

Here's an example of a group-level configuration in the librechat.yaml file

```yaml  filename="librechat.yaml"
endpoints:
  azureOpenAI:
    # ... (endpoint-level configurations)
    groups:
      - group: "my-resource-group"
        apiKey: "${AZURE_API_KEY}"
        instanceName: "my-instance"
        deploymentName: "gpt-35-turbo"
        version: "2023-03-15-preview"
        baseURL: "https://my-instance.openai.azure.com/"
        additionalHeaders:
          CustomHeader: "HeaderValue"
        addParams:
          max_tokens: 2048
          temperature: 0.7
        dropParams:
          - "frequency_penalty"
          - "presence_penalty"
        forcePrompt: false
        models:
        # ... (model-level configurations)
```

## Model-Level Configuration

Within each group, the `models` field contains a mapping of model identifiers to their configurations:

Model Identification:
<OptionTable
options={[
['Model Identifier', 'string', 'Must match the corresponding OpenAI model name. Can be a partial match.', 'gpt-3.5-turbo: true'],
]}
/>

Model Configuration:
<OptionTable
options={[
['Model Configuration', 'boolean/object', 'Boolean true: Uses the group-level deploymentName and version. Object: Specifies model-specific deploymentName and version. If not provided, inherits from the group.', 'text-davinci-003: {deploymentName: my-model-deployment, version: 2023-03-15-preview}'],
['deploymentName', 'string', 'The deployment name for this specific model.', 'deploymentName: my-model-deployment'],
['version', 'string', 'The Azure OpenAI API version for this specific model.', 'version: 2023-03-15-preview'],
]}
/>

Serverless Inference Endpoints:
<OptionTable
options={[
['Serverless Inference Endpoints', 'note', 'For serverless models, set the model to true.', 'gpt-4: true'],
]}
/>

- The **model identifier must match its corresponding OpenAI model name** in order for it to properly reflect its known context limits and/or function in the case of vision. For example, if you intend to use gpt-4-vision, it must be configured like so:

```yaml filename="librechat.yaml"
endpoints:
  azureOpenAI:
    # ... (endpoint-level configurations)
    groups:
    # ... (group-level configurations)
    - group: "example_group"
    models:
     # Model identifiers must match OpenAI Model name (can be a partial match)
      gpt-4-vision-preview:
      # Object setting: must include at least "deploymentName" and/or "version"
        deploymentName: "arbitrary-deployment-name"
        version: "2024-02-15-preview" # version can be any that supports vision
      # Boolean setting, must be "true"
      gpt-4-turbo: true
```

- See [Model Deployments](#model-deployments) for more examples.

- If a model is set to `true`, it implies using the group-level `deploymentName` and `version` for this model. Both must be defined at the group level in this case.
  
- If a model is configured as an object, it can specify its own `deploymentName` and `version`. If these are not provided, the model inherits the group's `deploymentName` and `version`.

- If the group represents a [serverless inference endpoint](#serverless-inference-endpoints), the singular model should be set to `true` to add it to the models list.

### Special Considerations

1. **Unique Names**: Both model and group names must be unique across the entire configuration. Duplicate names lead to validation failures.

2. **Missing Required Fields**: Lack of required `deploymentName` or `version` either at the group level (for boolean-flagged models) or within the models' configurations (if not inheriting or explicitly specified) will result in validation errors, unless the group represents a [serverless inference endpoint](#serverless-inference-endpoints).

3. **Environment Variable References**: The configuration supports environment variable references (e.g., `${VARIABLE_NAME}`). Ensure that all referenced variables are present in your environment to avoid runtime errors. The absence of defined environment variables referenced in the config will cause errors.`${INSTANCE_NAME}` and `${DEPLOYMENT_NAME}` are unique placeholders, and do not correspond to environment variables, but instead correspond to the instance and deployment name of the currently selected model. It is not recommended you use `INSTANCE_NAME` and `DEPLOYMENT_NAME` as environment variable names to avoid any potential conflicts.

4. **Error Handling**: Any issues in the config, like duplicate names, undefined environment variables, or missing required fields, will invalidate the setup and generate descriptive error messages aiming for prompt resolution. You will not be allowed to run the server with an invalid configuration.

5. **Model identifiers**: An unknown model (to the project) can be used as a model identifier, but it must match a known model to reflect its known context length, which is crucial for message/token handling; e.g., `gpt-7000` will be valid but default to a 4k token limit, whereas `gpt-4-turbo` will be recognized as having a 128k context limit.

Applying these setup requirements thoughtfully will ensure a correct and efficient integration of Azure OpenAI services with LibreChat through the `librechat.yaml` configuration. Always validate your configuration against the latest schema definitions and guidelines to maintain compatibility and functionality.


### Model Deployments

The list of models available to your users are determined by the model groupings specified in your [`azureOpenAI` endpoint config.](./custom_config.md#models_1)

For example:

```yaml filename="librechat.yaml"
# Example Azure OpenAI Object Structure
endpoints:
  azureOpenAI:
    groups:
      - group: "my-westus" # arbitrary name
        apiKey: "${WESTUS_API_KEY}"
        instanceName: "actual-instance-name" # name of the resource group or instance
        version: "2023-12-01-preview"
        models:
          gpt-4-vision-preview:
            deploymentName: gpt-4-vision-preview
            version: "2024-02-15-preview"
          gpt-3.5-turbo: true
      - group: "my-eastus"
        apiKey: "${EASTUS_API_KEY}"
        instanceName: "actual-eastus-instance-name"
        deploymentName: gpt-4-turbo
        version: "2024-02-15-preview"
        models:
          gpt-4-turbo: true
```

The above configuration would enable `gpt-4-vision-preview`, `gpt-3.5-turbo` and `gpt-4-turbo` for your users in the order they were defined.

### Using Assistants with Azure

To enable use of Assistants with Azure OpenAI, there are 2 main steps.

1) Set the `assistants` field, **under** the `azureOpenAI` endpoint, i.e, at the [Endpoint-level](#endpoint-level-configuration) to `true`, like so:

```yaml filename="librechat.yaml"
endpoints:
  azureOpenAI:
  # Enable use of Assistants with Azure
    assistants: true
```

2) Add the `assistants` field to groups compatible with Azure's Assistants API integration.

- At least one of your group configurations must be compatible.
- You can check the [compatible regions and models in the Azure docs here](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#assistants-preview).
- The version must also be "2024-02-15-preview" or later, preferably later for access to the latest features.

```yaml filename="librechat.yaml"
endpoints:
  azureOpenAI:
    assistants: true
    groups:
      - group: "my-sweden-group"
        apiKey: "${SWEDEN_API_KEY}"
        instanceName: "actual-instance-name"
      # Mark this group as assistants compatible
        assistants: true
      # version must be "2024-02-15-preview" or later
        version: "2024-03-01-preview"
        models:
          # ... (model-level configuration)
```

**Notes:**

- For credentials, rely on custom envrionment variables specified at each assistants-compatible group configuration.
- If you mark multiple regions as assistants-compatible, assistants you create will be aggregated across regions to the main assistant selection list.
- Files you upload to Azure OpenAI, whether at the message or assistant level, will only be available in the region the current assistant's model is part of.
    - For this reason, it's recommended you use only one region or resource group for Azure OpenAI Assistants, or you will experience an error.
    - Uploading to "OpenAI" is the default behavior for official `code_interpeter` and `retrieval` capabilities.
- Downloading files that assistants generate will soon be supported.
- As of May 19th 2024, retrieval and streaming are not yet supported through Azure OpenAI.
    - To avoid any errors with retrieval while it's not supported, it's recommended to disable the capability altogether through the `azureAssistants` endpoint config:

    ```yaml filename="librechat.yaml"
    endpoints:
      azureOpenAI:
        # ...rest

      azureAssistants:
      # "retrieval" omitted.
        capabilities: ["code_interpreter", "actions", "tools"]
    ```

    - By default, all capabilities, except retrieval, are enabled.

### Using Plugins with Azure

To use the Plugins endpoint with Azure OpenAI, you need a deployment supporting **[function calling](https://techcommunity.microsoft.com/t5/azure-ai-services-blog/function-calling-is-now-available-in-azure-openai-service/ba-p/3879241)**. Otherwise, you need to set "Functions" off in the Agent settings. When you are not using "functions" mode, it's recommend to have "skip completion" off as well, which is a review step of what the agent generated.

To use Azure with the Plugins endpoint, make sure the field `plugins` is set to `true` in your Azure OpenAI endpoing config:

```yaml filename="librechat.yaml"
# Example Azure OpenAI Object Structure
endpoints:
  azureOpenAI:
    plugins: true # <------- Set this
    groups:
    # omitted for brevity
```

Configuring the `plugins` field will configure Plugins to use Azure models.

**NOTE**: The current configuration through `librechat.yaml` uses the primary model you select from the frontend for Plugin use, which is not usually how it works without Azure, where instead the "Agent" model is used. The Agent model setting can be ignored when using Plugins through Azure.

### Using a Specified Base URL with Azure

The base URL for Azure OpenAI API requests can be dynamically configured. This is useful for proxying services such as [Cloudflare AI Gateway](https://developers.cloudflare.com/ai-gateway/providers/azureopenai/), or if you wish to explicitly override the baseURL handling of the app.

LibreChat will use the baseURL field for your Azure model grouping, which can include placeholders for the Azure OpenAI API instance and deployment names.

In the configuration, the base URL can be customized like so:

```yaml filename="librechat.yaml"
# librechat.yaml file, under an Azure group:
endpoints:
  azureOpenAI:
    groups:
      - group: "group-with-custom-base-url"
      baseURL: "https://example.azure-api.net/${INSTANCE_NAME}/${DEPLOYMENT_NAME}"

# OR
      baseURL: "https://${INSTANCE_NAME}.openai.azure.com/openai/deployments/${DEPLOYMENT_NAME}"

# Cloudflare example
      baseURL: "https://gateway.ai.cloudflare.com/v1/ACCOUNT_TAG/GATEWAY/azure-openai/${INSTANCE_NAME}/${DEPLOYMENT_NAME}"
```

**NOTE**: `${INSTANCE_NAME}` and `${DEPLOYMENT_NAME}` are unique placeholders, and do not correspond to environment variables, but instead correspond to the instance and deployment name of the currently selected model. It is not recommended you use INSTANCE_NAME and DEPLOYMENT_NAME as environment variable names to avoid any potential conflicts.

**You can also omit the placeholders completely and simply construct the baseURL with your credentials:**

```yaml filename="librechat.yaml"
      baseURL: "https://gateway.ai.cloudflare.com/v1/ACCOUNT_TAG/GATEWAY/azure-openai/my-secret-instance/my-deployment"
```
**Lastly, you can specify the entire baseURL through a custom environment variable**

```yaml filename="librechat.yaml"
      baseURL: "${MY_CUSTOM_BASEURL}"
```


### Enabling Auto-Generated Titles with Azure

To enable titling for Azure, set `titleConvo` to `true`.

```yaml filename="librechat.yaml"
# Example Azure OpenAI Object Structure
endpoints:
  azureOpenAI:
    titleConvo: true # <------- Set this
    groups:
    # omitted for brevity
```

**You can also specify the model to use for titling, with `titleModel`** provided you have configured it in your group(s).

```yaml filename="titleModel"
    titleModel: "gpt-3.5-turbo"
```

**Note**: "gpt-3.5-turbo" is the default value, so you can omit it if you want to use this exact model and have it configured. If not configured and `titleConvo` is set to `true`, the titling process will result in an error and no title will be generated. You can also set this to dynamically use the current model by setting it to `current_model`.

```yaml filename="titleModel"
    titleModel: "current_model"
```

### Using GPT-4 Vision with Azure

To use Vision (image analysis) with Azure OpenAI, you need to make sure `gpt-4-vision-preview` is a specified model [in one of your groupings](#model-deployments)

This will work seamlessly as it does with the [OpenAI endpoint](./ai_setup.md#openai) (no need to select the vision model, it will be switched behind the scenes)

### Generate images with Azure OpenAI Service (DALL-E)

| Model ID | Feature Availability | Max Request (characters) |
|----------|----------------------|-------------------------|
| dalle2   | East US              | 1000                    |
| dalle3   | Sweden Central       | 4000                    |

- First you need to create an Azure resource that hosts DALL-E
    - At the time of writing, dall-e-3 is available in the `SwedenCentral` region, dall-e-2 in the `EastUS` region.
- Then, you need to deploy the image generation model in one of the above regions.
    - Read the [Azure OpenAI Image Generation Quickstart Guide](https://learn.microsoft.com/en-us/azure/ai-services/openai/dall-e-quickstart) for further assistance
- Configure your environment variables based on Azure credentials:

Here's the updated layout for the DALL-E configuration options:

#### DALL-E:

**API Keys:**
<OptionTable
  options={[
    ['DALLE_API_KEY', 'string', 'The OpenAI API key for DALL-E 2 and DALL-E 3 services.','# DALLE_API_KEY='],
  ]}
/>

**API Keys (Version Specific):**
<OptionTable
  options={[
    ['DALLE3_API_KEY', 'string', 'The OpenAI API key for DALL-E 3.','# DALLE3_API_KEY='],
    ['DALLE2_API_KEY', 'string', 'The OpenAI API key for DALL-E 2.','# DALLE2_API_KEY='],
  ]}
/>

**System Prompts:**
<OptionTable
  options={[
    ['DALLE3_SYSTEM_PROMPT', 'string', 'The system prompt for DALL-E 3.','# DALLE3_SYSTEM_PROMPT="Your DALL-E-3 System Prompt here"'],
    ['DALLE2_SYSTEM_PROMPT', 'string', 'The system prompt for DALL-E 2.','# DALLE2_SYSTEM_PROMPT="Your DALL-E-2 System Prompt here"'],
  ]}
/>

**Reverse Proxy Settings:**
<OptionTable
  options={[
    ['DALLE_REVERSE_PROXY', 'string', 'The reverse proxy URL for DALL-E API requests.','# DALLE_REVERSE_PROXY='],
  ]}
/>

**Base URLs:**
<OptionTable
  options={[
    ['DALLE3_BASEURL', 'string', 'The base URL for DALL-E 3 API endpoints.','# DALLE3_BASEURL=https://<AZURE_OPENAI_API_INSTANCE_NAME>.openai.azure.com/openai/deployments/<DALLE3_DEPLOYMENT_NAME>/'],
    ['DALLE2_BASEURL', 'string', 'The base URL for DALL-E 2 API endpoints.','# DALLE2_BASEURL=https://<AZURE_OPENAI_API_INSTANCE_NAME>.openai.azure.com/openai/deployments/<DALLE2_DEPLOYMENT_NAME>/'],
  ]}
/>

**Azure OpenAI Integration (Optional):**
<OptionTable
  options={[
    ['DALLE3_AZURE_API_VERSION', 'string', 'The API version for DALL-E 3 with Azure OpenAI service.','# DALLE3_AZURE_API_VERSION=the-api-version # e.g.: 2023-12-01-preview'],
    ['DALLE2_AZURE_API_VERSION', 'string', 'The API version for DALL-E 2 with Azure OpenAI service.','# DALLE2_AZURE_API_VERSION=the-api-version # e.g.: 2023-12-01-preview'],
  ]}
/>

Remember to replace placeholder text with actual prompts or instructions and provide your actual API keys if you choose to include them directly in the file (though managing sensitive keys outside of the codebase is a best practice). Always review and respect OpenAI's usage policies when embedding API keys in software.

> Note: if you have PROXY set, it will be used for DALL-E calls also, which is universal for the app.

### Serverless Inference Endpoints

Through the `librechat.yaml` file, you can configure Azure AI Studio serverless inference endpoints to access models from the [Azure AI Foundry.](https://ai.azure.com/explore) Only a model identifier, `baseURL`, and `apiKey` are needed along with the `serverless` field to indicate the special handling these endpoints need.

- You will need to follow the instructions in the compatible model cards to set up **MaaS** ("Models as a Service") access on Azure AI Studio.

    - For reference, here are some known compatible model cards:

    - [Mistral-large](https://aka.ms/aistudio/landing/mistral-large) | [Meta-Llama-3.1-8B-Instruct](https://ai.azure.com/explore/models/Meta-Llama-3.1-8B-Instruct/version/4/) | [Phi-3-medium-128k-instruct](https://ai.azure.com/explore/models/Phi-3-medium-128k-instruct/version/1/registry/azureml)

- You can also review [the technical blog for the "Mistral-large" model release](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/mistral-large-mistral-ai-s-flagship-llm-debuts-on-azure-ai/ba-p/4066996) for more info.

- Then, you will need to add them to your `azureOpenAI` config in the librechat.yaml file.

- Here is an example configuration for `Meta-Llama-3.1-8B-Instruct`:

```yaml filename="librechat.yaml"
endpoints:
  azureOpenAI:
    groups:
    - group: "serverless-example"
      apiKey: "${LLAMA318B_API_KEY}"  # arbitrary env var name
      baseURL: "https://example.services.ai.azure.com/models/"
      version: "2024-05-01-preview" # Optional: specify API version
      serverless: true
      models:
        # Must match the deployment name of the model
        Meta-Llama-3.1-8B-Instruct: true
```

**Notes**:

- Azure AI Foundry models now provision endpoints under `/models/chat/completions?api-version=version` for serverless inference.
  - The `baseURL` field should be set to the root of the endpoint, without anything after `/models/`, i.e., the `/chat/completions` path.
  - Example: `https://example.services.ai.azure.com/models/` for `https://example.services.ai.azure.com/models/chat/completions?api-version=2024-05-01-preview`
  - The `version` query parameter is optional and can be specified in the `baseURL` field.
- The model name used in the `models` field must match the deployment name of the model in the Azure AI Foundry. 
- Compatibility with LibreChat relies on parity with OpenAI API specs, which at the time of writing, are typically **"Pay-as-you-go"** or "Models as a Service" (MaaS) deployments on Azure AI Studio, that are OpenAI-SDK-compatible with either `v1/completions` or `models/chat/completions` endpoint handling.
- All models that offer serverless deployments ("Serverless APIs") are compatible from the Azure model catalog. You can filter by "Serverless API" under Deployment options and "Chat completion" under inference tasks to see the full list; however, real time endpoint models have not been tested.
- These serverless inference endpoint/models may or may not support function calling according to OpenAI API specs, which enables their use with Agents.
- If using legacy "/v1/completions" (without "chat"), you need to set the `forcePrompt` field to `true` in your [group config.](#group-level-configuration)



================================================
FILE: pages/docs/configuration/docker_override.mdx
================================================
---
title: Docker Override
description: "How to Use the Docker Compose Override File: In Docker Compose, an override file is a powerful feature that allows you to modify the default configuration provided by the main `docker-compose.yml` without the need to directly edit or duplicate the whole file."
---

# How to Use the Docker Compose Override File

In Docker Compose, an override file is a powerful feature that allows you to modify the default configuration provided by the main `docker-compose.yml` without the need to directly edit or duplicate the whole file. The primary use of the override file is for local development customizations, and Docker Compose merges the configurations of the `docker-compose.yml` and the `docker-compose.override.yml` files when you run `docker compose up`.

Here's a quick guide on how to use the `docker-compose.override.yml`:

> Note: Please consult the `docker-compose.override.yml.example` for more examples 

See the official docker documentation for more info:

- **[docker docs - understanding-multiple-compose-files](https://docs.docker.com/compose/multiple-compose-files/extends/#understanding-multiple-compose-files)**
- **[docker docs - merge-compose-files](https://docs.docker.com/compose/multiple-compose-files/merge/#merge-compose-files)**
- **[docker docs - specifying-multiple-compose-files](https://docs.docker.com/compose/reference/#specifying-multiple-compose-files)**

## Step 1: Create a `docker-compose.override.yml` file

If you don't already have a `docker-compose.override.yml` file, you can create one by copying the example override content:

```bash filename="Copying the example override file"
cp docker-compose.override.yml.example docker-compose.override.yml
```

This file will be picked up by Docker Compose automatically when you run docker-compose commands.

## Step 2: Edit the override file

Open your `docker-compose.override.yml` file with vscode or any text editor.

Make your desired changes by uncommenting the relevant sections and customizing them as needed.

> Warning: You can only specify every service name once (api, mongodb, meilisearch, ...) If you want to override multiple settings in one service you will have to edit accordingly.

### Examples

If you want to make sure Docker can use your `librechat.yaml` file for [Custom Endpoints & Configuration](/docs/configuration/librechat_yaml), it would look like this:

```yaml filename="docker-compose.override.yml"
version: '3.4'

services:
  api:
    volumes:
      - ./librechat.yaml:/app/librechat.yaml
```

Or, if you want to locally build the image for the `api` service, use the LibreChat config file, and use the older Mongo that doesn't requires AVX support, your `docker-compose.override.yml` might look like this:

```yaml filename="docker-compose.override.yml"
version: '3.4'

services:
  api:
    volumes:
      - ./librechat.yaml:/app/librechat.yaml
    image: librechat
    build:
      context: .
      target: node

  mongodb:
    image: mongo:4.4.18
```

> Note: Be cautious if you expose ports for MongoDB or Meilisearch to the public, as it can make your data vulnerable.

## Step 3: Apply the changes

To apply your configuration changes, simply run Docker Compose as usual. Docker Compose automatically takes into account both the `docker-compose.yml` and the `docker-compose.override.yml` files:

```bash filename="Apply the changes"
docker compose up -d
```

## Step 4: Verify the changes

After starting your services with the modified configuration, you can verify that the changes have been applied using the `docker ps` command to list the running containers and their properties, such as ports.

## Important Considerations

- **Order of Precedence**: Values defined in the override file take precedence over those specified in the original `docker-compose.yml` file.
- **Security**: When customizing ports and publicly exposing services, always be conscious of the security implications. Avoid using defaults for production or sensitive environments.

By following these steps and considerations, you can easily and safely modify your Docker Compose configuration without altering the original `docker-compose.yml` file, making it simpler to manage and maintain different environments or local customizations.


## `deploy-compose.yml`

To use an override file with a non-default Docker Compose file, such as `deploy-compose.yml`, you will have to explicitly specify both files when running Docker Compose commands.

Docker Compose allows you to specify multiple `-f` or `--file` options to include multiple compose files, where settings in later files override or add to those in the first.

If you use `deploy-compose.yml` as your main Docker Compose configuration and you have an override file named `docker-compose.override.yml` (you can name the override file whatever you want, but you may have this specific file already), you would run Docker Compose commands like so:

```bash
docker compose -f deploy-compose.yml -f docker-compose.override.yml pull
docker compose -f deploy-compose.yml -f docker-compose.override.yml up
```


================================================
FILE: pages/docs/configuration/dotenv.mdx
================================================
---
title: Environment Variables
description: Comprehensive guide for configuring your application's environment with the `.env` file. This document is your one-stop resource for understanding and customizing the environment variables that will shape your application's behavior in different contexts.
---

# .env File Configuration
Welcome to the comprehensive guide for configuring your application's environment with the `.env` file. This document is your one-stop resource for understanding and customizing the environment variables that will shape your application's behavior in different contexts.

While the default settings provide a solid foundation for a standard `docker` installation, delving into this guide will unveil the full potential of LibreChat. This guide empowers you to tailor LibreChat to your precise needs. Discover how to adjust language model availability, integrate social logins, manage the automatic moderation system, and much more. It's all about giving you the control to fine-tune LibreChat for an optimal user experience.

> **Reminder: Please restart LibreChat for the configuration changes to take effect**

Alternatively, you can create a new file named `docker-compose.override.yml` in the same directory as your main `docker-compose.yml` file for LibreChat, where you can set your .env variables as needed under `environment`, or modify the default configuration provided by the main `docker-compose.yml`, without the need to directly edit or duplicate the whole file.

For more info see:

- Our quick guide:
    - **[Docker Override](/docs/configuration/docker_override)**

- The official docker documentation:
    - **[docker docs - understanding-multiple-compose-files](https://docs.docker.com/compose/multiple-compose-files/extends/#understanding-multiple-compose-files)**
    - **[docker docs - merge-compose-files](https://docs.docker.com/compose/multiple-compose-files/merge/#merge-compose-files)**
    - **[docker docs - specifying-multiple-compose-files](https://docs.docker.com/compose/reference/#specifying-multiple-compose-files)**

- You can also view an example of an override file for LibreChat in your LibreChat folder and on GitHub:
    - **[docker-compose.override.example](https://github.com/danny-avila/LibreChat/blob/main/docker-compose.override.yml.example)**

---

## Server Configuration

### Port

- The server listens on a specific port.
- The `PORT` environment variable sets the port where the server listens. By default, it is set to `3080`.


<OptionTable
  options={[
    ['HOST', 'string', 'Specifies the host.', 'HOST=localhost'],
    ['PORT', 'number', 'Specifies the port.', 'PORT=3080'],
  ]}
/>

### Trust proxy
Use the address that is at most n number of hops away from the Express application. 
req.socket.remoteAddress is the first hop, and the rest are looked for in the X-Forwarded-For header from right to left. 
A value of 0 means that the first untrusted address would be req.socket.remoteAddress, i.e. there is no reverse proxy.
The `TRUST_PROXY` environment variable default is set to `1`.

Refer to [Express.js - trust proxy](https://expressjs.com/en/guide/behind-proxies.html) for more information about this.

<OptionTable
  options={[
    ['TRUST_PROXY', 'number', 'Specifies the number of hops.', 'HOST=1'],
  ]}
/>

### Credentials Configuration

To securely store credentials, you need a fixed key and IV. You can set them here for prod and dev environments.

<OptionTable
  options={[
    ['CREDS_KEY', 'string', '32-byte key (64 characters in hex) for securely storing credentials. Required for app startup.', 'CREDS_KEY=f34be427ebb29de8d88c107a71546019685ed8b241d8f2ed00c3df97ad2566f0'],
    ['CREDS_IV', 'string', '16-byte IV (32 characters in hex) for securely storing credentials. Required for app startup.', 'CREDS_IV=e2341419ec3dd3d19b13a1a87fafcbfb'],
  ]}
/>

<Callout type="warning" title="Warning">
**Warning:** If you don't set `CREDS_KEY` and `CREDS_IV`, the app will crash on startup.
- You can use this [Key Generator](/toolkit/creds_generator) to generate them quickly.
</Callout>

### Static File Handling

<OptionTable
  options={[
    ['STATIC_CACHE_MAX_AGE', 'string', 'Cache-Control max-age in seconds','STATIC_CACHE_MAX_AGE=172800'],
    ['STATIC_CACHE_S_MAX_AGE', 'string', 'Cache-Control s-maxage in seconds for shared caches (CDNs and proxies)','STATIC_CACHE_S_MAX_AGE="86400"'],
    ['DISABLE_COMPRESSION', 'boolean', 'Disables compression for static files.','DISABLE_COMPRESSION=false'],
  ]}
/>

**Behaviour:**

Sets the [Cache-Control](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control) headers for static files. These configurations only trigger when the `NODE_ENV` is set to `production`.

* Uncomment `STATIC_CACHE_MAX_AGE` to change the local `max-age` for static files. By default this is set to 2 days (172800 seconds).
* Uncomment `STATIC_CACHE_S_MAX_AGE` to set the `s-maxage` for shared caches (CDNs and proxies). By default this is set to 1 day (86400 seconds).
* Uncomment `DISABLE_COMPRESSION` to disable compression for static files. By default, compression is enabled.

<Callout type="warning" title="Warning">
- This only affects static files served by the API server and is not applicable to _Firebase_, _NGINX_, or any other configurations.
</Callout>

### Index HTML Cache Control

<OptionTable
  options={[
    ['INDEX_HTML_CACHE_CONTROL', 'string', 'Cache-Control header for index.html','INDEX_HTML_CACHE_CONTROL=no-cache, no-store, must-revalidate'],
    ['INDEX_HTML_PRAGMA', 'string', 'Pragma header for index.html','INDEX_HTML_PRAGMA=no-cache'],
    ['INDEX_HTML_EXPIRES', 'string', 'Expires header for index.html','INDEX_HTML_EXPIRES=0'],
  ]}
/>

**Behaviour:**

Controls caching headers specifically for the index.html response. By default, these settings prevent caching to ensure users always get the latest version of the application.

<Callout type="note" title="Note">
Unlike static assets which are cached for performance, the index.html file's cache headers are configured separately to ensure users always get the latest application shell.
</Callout>

### MongoDB Database

<OptionTable
  options={[
    ['MONGO_URI', 'string', 'Specifies the MongoDB URI.','MONGO_URI=mongodb://127.0.0.1:27017/LibreChat'],
  ]}
/>
Change this to your MongoDB URI if different. You should add `LibreChat` or your own `APP_TITLE` as the database name in the URI.

If you are using an online database, the URI format is `mongodb+srv://<username>:<password>@<host>/<database>?<options>`. Your `MONGO_URI` should look like this:
* `mongodb+srv://username:password@host.mongodb.net/LibreChat?retryWrites=true` (`retryWrites` is the only option you need when using the online database.)

Alternatively you can use `documentDb` that emulates `mongoDb` but it:

* does not support `retryWrites` - use `retryWrites=false`
* requires TLS connection, hence use parameters `tls=true` to enable TLS and `tlsCAFile=/path-to-ca/bundle.pem` to point to the AWS provided CA bundle file

The URI for `documentDb` will look like:
* `mongodb+srv://username:password@domain/dbname?retryWrites=false&tls=true&tlsCAFile=/path-to-ca/bundle.pem`

See also:

* [MongoDB Atlas](/docs/configuration/mongodb/mongodb_atlas) for instructions on how to create an online MongoDB Atlas database (useful for use without Docker)
* [MongoDB Community Server](/docs/configuration/mongodb/mongodb_community) for instructions on how to create a local MongoDB database (without Docker)
* [MongoDB Authentication](/docs/configuration/mongodb/mongodb_auth) To enable explicit authentication for MongoDB in Docker.
* [Manage your database with Mongo Express](/blog/2023-11-30_mongoexpress) for securely accessing your Docker MongoDB database

### Application Domains

To configure LibreChat for local use or custom domain deployment, set the following environment variables:

<OptionTable
  options={[
    ['DOMAIN_CLIENT', 'string', 'Specifies the client-side domain.', 'DOMAIN_CLIENT=http://localhost:3080'],
    ['DOMAIN_SERVER', 'string', 'Specifies the server-side domain.', 'DOMAIN_SERVER=http://localhost:3080'],
  ]}
/>

When deploying LibreChat to a custom domain, replace `http://localhost:3080` with your deployed URL
- e.g. `https://librechat.example.com`.

### Prevent Public Search Engines Indexing

By default, your website will not be indexed by public search engines (e.g. Google, Bing, …). This means that people will not be able to find your website through these search engines. If you want to make your website more visible and searchable, you can change the following setting to `false`

<OptionTable
  options={[
    ['NO_INDEX', 'boolean', 'Prevents public search engines from indexing your website.', 'NO_INDEX=true'],
  ]}
/>

❗**Note:** This method is not guaranteed to work for all search engines, and some search engines may still index your website or web page for other purposes, such as caching or archiving. Therefore, you should not rely solely on this method to protect sensitive or confidential information on your website or web page.

### Logging

LibreChat has built-in central logging, see [Logging System](/docs/configuration/logging) for more info.

#### Log Files

* Debug logging is enabled by default and crucial for development.
* To report issues, reproduce the error and submit logs from `./api/logs/debug-%DATE%.log` at: **[LibreChat GitHub Issues](https://github.com/danny-avila/LibreChat/issues)**
* Error logs are stored in the same location.

#### Environment Variables

<OptionTable
  options={[
    ['DEBUG_LOGGING', 'boolean', 'Keep debug logs active.','DEBUG_LOGGING=true'],
    ['DEBUG_CONSOLE', 'boolean', 'Enable verbose console/stdout logs in the same format as file debug logs.', 'DEBUG_CONSOLE=false'],
    ['CONSOLE_JSON', 'boolean', 'Enable verbose JSON console/stdout logs suitable for cloud deployments like GCP/AWS.', 'CONSOLE_JSON=false'],
    ['CONSOLE_JSON_STRING_LENGTH', 'number', 'Configure the truncation size for console/stdout logs, defaults to 255', 'CONSOLE_JSON_STRING_LENGTH=1000'],
  ]}
/>

Note:
* `DEBUG_LOGGING` can be used with either `DEBUG_CONSOLE` or `CONSOLE_JSON` but not both.
* `DEBUG_CONSOLE` and `CONSOLE_JSON` are mutually exclusive.
* `CONSOLE_JSON`: When handling console logs in cloud deployments (such as GCP or AWS), enabling this will dump the logs with a UTC timestamp and format them as JSON.
  * See: [feat: Add CONSOLE_JSON](https://github.com/danny-avila/LibreChat/pull/2146)

Note: `DEBUG_CONSOLE` is not recommended, as the outputs can be quite verbose, and so it's disabled by default.

### Permission
> UID and GID are numbers assigned by Linux to each user and group on the system. If you have permission problems, set here the UID and GID of the user running the Docker Compose command. The applications in the container will run with these UID/GID.

<OptionTable
  options={[
    ['UID', 'number', 'The user ID.', '# UID=1000'],
    ['GID', 'number', 'The group ID.', '# GID=1000'],
  ]}
/>

### Configuration Path - `librechat.yaml`
Specify an alternative location for the LibreChat configuration file.
You may specify an **absolute path**, a **relative path**, or a **URL**. The filename in the path is flexible and does not have to be `librechat.yaml`; any valid configuration file will work.

> **Note**: If you prefer LibreChat to search for the configuration file in the root directory (which is the default behavior), simply leave this option commented out.

<OptionTable
  options={[
    ['CONFIG_PATH', 'string', 'An alternative location for the LibreChat configuration file.', '# CONFIG_PATH=https://raw.githubusercontent.com/danny-avila/LibreChat/main/librechat.example.yaml'],
  ]}
/>

## Endpoints
In this section, you can configure the endpoints and models selection, their API keys, and the proxy and reverse proxy settings for the endpoints that support it.

### General Config
Uncomment `ENDPOINTS` to customize the available endpoints in LibreChat.

<OptionTable
  options={[
    ['ENDPOINTS', 'string', 'Comma-separated list of available endpoints.', '# ENDPOINTS=openAI,agents,assistants,gptPlugins,azureOpenAI,google,anthropic,bingAI,custom'],
    ['PROXY', 'string', 'Proxy setting for all endpoints.', 'PROXY='],
    ['TITLE_CONVO', 'boolean', 'Enable titling for all endpoints.', 'TITLE_CONVO=true'],
  ]}
/>

### Known Endpoints - `librechat.yaml`
- see also: [Custom Endpoints & Configuration](/docs/configuration/librechat_yaml)

<OptionTable
  options={[
    ['ANYSCALE_API_KEY', 'string', 'API key for Anyscale.', '# ANYSCALE_API_KEY='],
    ['APIPIE_API_KEY', 'string', 'API key for Apipie.', '# APIPIE_API_KEY='],
    ['COHERE_API_KEY', 'string', 'API key for Cohere.', '# COHERE_API_KEY='],
    ['FIREWORKS_API_KEY', 'string', 'API key for Fireworks.', '# FIREWORKS_API_KEY='],
    ['GROQ_API_KEY', 'string', 'API key for Groq.', '# GROQ_API_KEY='],
    ['MISTRAL_API_KEY', 'string', 'API key for Mistral.', '# MISTRAL_API_KEY='],
    ['OPENROUTER_KEY', 'string', 'API key for OpenRouter.', '# OPENROUTER_KEY='],
    ['PERPLEXITY_API_KEY', 'string', 'API key for Perplexity.', '# PERPLEXITY_API_KEY='],
    ['SHUTTLEAI_API_KEY', 'string', 'API key for ShuttleAI.', '# SHUTTLEAI_API_KEY='],
    ['TOGETHERAI_API_KEY', 'string', 'API key for TogetherAI.', '# TOGETHERAI_API_KEY='],
    ['DEEPSEEK_API_KEY', 'string', 'API key for Deepseek API', '# DEEPSEEK_API_KEY='],
  ]}
/>

### Anthropic
see: [Anthropic Endpoint](./ai_setup.md#anthropic)
- You can request an access key from https://console.anthropic.com/
- Leave `ANTHROPIC_API_KEY=` blank to disable this endpoint
- Set `ANTHROPIC_API_KEY=` to "user_provided" to allow users to provide their own API key from the WebUI
- If you have access to a reverse proxy for `Anthropic`, you can set it with `ANTHROPIC_REVERSE_PROXY=`
    - leave blank or comment it out to use default base url

<OptionTable
  options={[
    ['ANTHROPIC_API_KEY', 'string', 'Anthropic API key or "user_provided" to allow users to provide their own API key.', 'Defaults to an empty string.'],
    ['ANTHROPIC_MODELS', 'string', 'Comma-separated list of Anthropic models to use.', '# ANTHROPIC_MODELS=claude-3-opus-20240229,claude-3-sonnet-20240229,claude-3-haiku-20240307,claude-2.1,claude-2,claude-1.2,claude-1,claude-1-100k,claude-instant-1,claude-instant-1-100k'],
    ['ANTHROPIC_REVERSE_PROXY', 'string', 'Reverse proxy for Anthropic.', '# ANTHROPIC_REVERSE_PROXY='],
    ['ANTHROPIC_TITLE_MODEL', 'string', 'DEPRECATED: Model to use for titling with Anthropic.', '# ANTHROPIC_TITLE_MODEL=claude-3-haiku-20240307'],
  ]}
/>

- `ANTHROPIC_TITLE_MODEL` is now deprecated and will be removed in future versions. Use the [`titleModel` Endpoint Setting](/docs/configuration/librechat_yaml/object_structure/shared_endpoint_settings#titlemodel) instead in the `librechat.yaml` config instead.

> **Note:** Must be compatible with the Anthropic Endpoint. Also, Claude 2 and Claude 3 models perform best at this task, with `claude-3-haiku` models being the cheapest.

### BingAI
Bing, also used for Sydney, jailbreak, and Bing Image Creator

<OptionTable
  options={[
    ['BINGAI_TOKEN', 'string', 'Bing access token. Leave blank to disable. Can be set to "user_provided" to allow users to provide their own token from the WebUI.', 'BINGAI_TOKEN=user_provided'],
    ['BINGAI_HOST', 'string', 'Bing host URL. Leave commented out to use default server.', '# BINGAI_HOST=https://cn.bing.com'],
  ]}
/>

Note: It is recommended to leave it as "user_provided" and provide the token from the WebUI.

### Google

Follow these instructions to setup the [Google Endpoint](/docs/configuration/pre_configured_ai/google)

<OptionTable
  options={[
    ['GOOGLE_KEY', 'string', 'Google API key. Set to "user_provided" to allow users to provide their own API key from the WebUI.', 'GOOGLE_KEY=user_provided'],
    ['GOOGLE_REVERSE_PROXY', 'string', 'Google reverse proxy URL.', 'GOOGLE_REVERSE_PROXY='],
    ['GOOGLE_MODELS', 'string', 'Available Gemini API Google models, separated by commas.', 'GOOGLE_MODELS=gemini-1.0-pro,gemini-1.0-pro-001,gemini-1.0-pro-latest,gemini-1.0-pro-vision-latest,gemini-1.5-pro-latest,gemini-pro,gemini-pro-vision'],
    ['GOOGLE_MODELS', 'string', 'Available Vertex AI Google models, separated by commas.', 'GOOGLE_MODELS=gemini-1.5-pro-preview-0409,gemini-1.0-pro-vision-001,gemini-pro,gemini-pro-vision,chat-bison,chat-bison-32k,codechat-bison,codechat-bison-32k,text-bison,text-bison-32k,text-unicorn,code-gecko,code-bison,code-bison-32k'],
    ['GOOGLE_TITLE_MODEL', 'string', 'DEPRECATED: The model used for titling with Google.', 'GOOGLE_TITLE_MODEL=gemini-pro'],
    ['GOOGLE_LOC', 'string', 'Specifies the Google Cloud location for processing API requests', 'GOOGLE_LOC=us-central1'],
    ['GOOGLE_EXCLUDE_SAFETY_SETTINGS', 'string', 'Completely omit the safety settings that are included by default, which will use provider defaults', 'GOOGLE_EXCLUDE_SAFETY_SETTINGS=true'],
    ['GOOGLE_SAFETY_SEXUALLY_EXPLICIT', 'string', 'Safety setting for sexually explicit content. Options are BLOCK_ALL, BLOCK_ONLY_HIGH, WARN_ONLY, and OFF.', 'GOOGLE_SAFETY_SEXUALLY_EXPLICIT=BLOCK_ONLY_HIGH'],
    ['GOOGLE_SAFETY_HATE_SPEECH', 'string', 'Safety setting for hate speech content. Options are BLOCK_ALL, BLOCK_ONLY_HIGH, WARN_ONLY, and OFF.', 'GOOGLE_SAFETY_HATE_SPEECH=BLOCK_ONLY_HIGH'],
    ['GOOGLE_SAFETY_HARASSMENT', 'string', 'Safety setting for harassment content. Options are BLOCK_ALL, BLOCK_ONLY_HIGH, WARN_ONLY, and OFF.', 'GOOGLE_SAFETY_HARASSMENT=BLOCK_ONLY_HIGH'],
    ['GOOGLE_SAFETY_DANGEROUS_CONTENT', 'string', 'Safety setting for dangerous content. Options are BLOCK_ALL, BLOCK_ONLY_HIGH, WARN_ONLY, and OFF.', 'GOOGLE_SAFETY_DANGEROUS_CONTENT=BLOCK_ONLY_HIGH'],
  ]}
/>

Customize the available models, separated by commas, **without spaces**. The first will be default. Leave it blank or commented out to use internal settings.

- `GOOGLE_TITLE_MODEL` is now deprecated and will be removed in future versions. Use the [`titleModel` Endpoint Setting](/docs/configuration/librechat_yaml/object_structure/shared_endpoint_settings#titlemodel) instead in the `librechat.yaml` config instead.

**Note:** For the Vertex AI `GOOGLE_SAFETY` variables, you do not have access to the `BLOCK_NONE` setting by default. To use this restricted `HarmBlockThreshold` setting, you will need to either:
- (a) Get access through an allowlist via your Google account team
- (b) Switch your account type to monthly invoiced billing following this instruction:
    https://cloud.google.com/billing/docs/how-to/invoiced-billing

### OpenAI

See: [OpenAI Setup](/docs/configuration/pre_configured_ai/openai)

<OptionTable
  options={[
    ['OPENAI_API_KEY', 'string', 'Your OpenAI API key. Leave blank to disable this endpoint or set to "user_provided" to allow users to provide their own API key from the WebUI.', 'OPENAI_API_KEY=user_provided'],
    ['OPENAI_MODELS', 'string', 'Customize the available models, separated by commas, without spaces. The first will be default. Leave commented out to use internal settings.', '# OPENAI_MODELS=gpt-3.5-turbo-0125,gpt-3.5-turbo-0301,gpt-3.5-turbo,gpt-4,gpt-4-0613,gpt-4-vision-preview,gpt-3.5-turbo-0613,gpt-3.5-turbo-16k-0613,gpt-4-0125-preview,gpt-4-turbo-preview,gpt-4-1106-preview,gpt-3.5-turbo-1106,gpt-3.5-turbo-instruct,gpt-3.5-turbo-instruct-0914,gpt-3.5-turbo-16k'],
    ['DEBUG_OPENAI', 'boolean', 'Enable debug mode for the OpenAI endpoint.', 'DEBUG_OPENAI=false'],
    ['OPENAI_SUMMARIZE', 'boolean', 'Enable message summarization. False by default', '# OPENAI_SUMMARIZE=true'],
    ['OPENAI_SUMMARY_MODEL', 'string', 'The model used for OpenAI summarization.', '# OPENAI_SUMMARY_MODEL=gpt-3.5-turbo'],
    ['OPENAI_FORCE_PROMPT', 'boolean', 'Force the API to be called with a prompt payload instead of a messages payload.', '# OPENAI_FORCE_PROMPT=false'],
    ['OPENAI_ORGANIZATION', 'string', 'Specify which organization to use for each API request to OpenAI. Optional', '# OPENAI_ORGANIZATION='],
    ['OPENAI_REVERSE_PROXY', 'string', 'DEPRECATED: Reverse proxy settings for OpenAI.', '# OPENAI_REVERSE_PROXY='],
    ['OPENAI_TITLE_MODEL', 'string', 'DEPRECATED: The model used for OpenAI titling.', '# OPENAI_TITLE_MODEL=gpt-3.5-turbo'],
  ]}
/>

- `OPENAI_TITLE_MODEL` is now deprecated and will be removed in future versions. Use the [`titleModel` Endpoint Setting](/docs/configuration/librechat_yaml/object_structure/shared_endpoint_settings#titlemodel) instead in the `librechat.yaml` config instead.
- `OPENAI_REVERSE_PROXY` is now deprecated and will be removed in future versions. Use a [custom endpoint](/docs/quick_start/custom_endpoints) instead.

### Assistants

See: [Assistants Setup](/docs/configuration/pre_configured_ai/assistants)

<OptionTable
  options={[
    ['ASSISTANTS_API_KEY', 'string', 'Your OpenAI API key for Assistants API. Leave blank to disable this endpoint or set to "user_provided" to allow users to provide their own API key from the WebUI.', 'ASSISTANTS_API_KEY=user_provided'],
    ['ASSISTANTS_MODELS', 'string', 'Customize the available models, separated by commas, without spaces. The first will be default. Leave blank to use internal settings.', '# ASSISTANTS_MODELS=gpt-3.5-turbo-0125,gpt-3.5-turbo-16k-0613,gpt-3.5-turbo-16k,gpt-3.5-turbo,gpt-4,gpt-4-0314,gpt-4-32k-0314,gpt-4-0613,gpt-3.5-turbo-0613,gpt-3.5-turbo-1106,gpt-4-0125-preview,gpt-4-turbo-preview,gpt-4-1106-preview'],
    ['ASSISTANTS_BASE_URL', 'string', 'Alternate base URL for Assistants API.', '# ASSISTANTS_BASE_URL='],
  ]}
/>

Note: You can customize the available models, separated by commas, without spaces. The first will be default. Leave it blank or commented out to use internal settings.

### Plugins

**Note:** Plugins are now deprecated. Use [Agents](/docs/features/agents) instead.

Here are some useful resources about plugins:

* [Introduction](/docs/features/plugins)
* [Make Your Own](/docs/development/tools_and_plugins)

#### Environment Variables

<OptionTable
  options={[
    ['PLUGIN_MODELS', 'string', 'Identify available models, separated by commas without spaces. The first model in the list will be set as default. Defaults to internal settings.', '# PLUGIN_MODELS=gpt-4,gpt-4-turbo,gpt-4-turbo-preview,gpt-4-0125-preview,gpt-4-1106-preview,gpt-4-0613,gpt-3.5-turbo,gpt-3.5-turbo-0125,gpt-3.5-turbo-1106,gpt-3.5-turbo-0613'],
  ]}
/>

<OptionTable
  options={[
    ['DEBUG_PLUGINS', 'boolean', 'Set to false to disable debug mode for plugins.', 'DEBUG_PLUGINS=true'],
  ]}
/>

<Callout type="warning" title="Warning">
- The API keys are "user_provided" through the webUI when commented out or empty. Do not set them to "user_provided", either provide the API key or leave them blank/commented out.
</Callout>

<Callout type="note" title="Note">
**Note:** Make sure the `gptPlugins` endpoint is set in the [`ENDPOINTS`](#endpoints) environment variable if it was configured before.
</Callout>

#### Azure AI Search
This plugin supports searching Azure AI Search for answers to your questions. See: [Azure AI Search](/docs/configuration/tools/azure_ai_search)

<OptionTable
  options={[
    ['AZURE_AI_SEARCH_SERVICE_ENDPOINT', 'string', 'The service endpoint for Azure AI Search.','AZURE_AI_SEARCH_SERVICE_ENDPOINT='],
    ['AZURE_AI_SEARCH_INDEX_NAME', 'string', 'The index name for Azure AI Search.','AZURE_AI_SEARCH_INDEX_NAME='],
    ['AZURE_AI_SEARCH_API_KEY', 'string', 'The API key for Azure AI Search.','AZURE_AI_SEARCH_API_KEY='],
    ['AZURE_AI_SEARCH_API_VERSION', 'string', 'The API version for Azure AI Search.','AZURE_AI_SEARCH_API_VERSION='],
    ['AZURE_AI_SEARCH_SEARCH_OPTION_QUERY_TYPE', 'string', 'The query type for Azure AI Search.','AZURE_AI_SEARCH_SEARCH_OPTION_QUERY_TYPE='],
    ['AZURE_AI_SEARCH_SEARCH_OPTION_TOP', 'number', 'The top count for Azure AI Search.','AZURE_AI_SEARCH_SEARCH_OPTION_TOP='],
    ['AZURE_AI_SEARCH_SEARCH_OPTION_SELECT', 'string', 'The select fields for Azure AI Search.','AZURE_AI_SEARCH_SEARCH_OPTION_SELECT='],
  ]}
/>

#### DALL-E:

**API Keys:**
<OptionTable
  options={[
    ['DALLE_API_KEY', 'string', 'The OpenAI API key for DALL-E 2 and DALL-E 3 services.','# DALLE2_API_KEY='],
  ]}
/>

**API Keys (Version Specific):**
<OptionTable
  options={[
    ['DALLE3_API_KEY', 'string', 'The OpenAI API key for DALL-E 3.','# DALLE3_API_KEY='],
    ['DALLE2_API_KEY', 'string', 'The OpenAI API key for DALL-E 2.','# DALLE2_API_KEY='],
  ]}
/>

**System Prompts:**
<OptionTable
  options={[
    ['DALLE3_SYSTEM_PROMPT', 'string', 'The system prompt for DALL-E 3.','# DALLE3_SYSTEM_PROMPT='],
    ['DALLE2_SYSTEM_PROMPT', 'string', 'The system prompt for DALL-E 2.','# DALLE2_SYSTEM_PROMPT='],
  ]}
/>

**Reverse Proxy Settings:**
<OptionTable
  options={[
    ['DALLE_REVERSE_PROXY', 'string', 'The reverse proxy URL for DALL-E API requests.','# DALLE_REVERSE_PROXY='],
  ]}
/>

**Base URLs:**
<OptionTable
  options={[
    ['DALLE3_BASEURL', 'string', 'The base URL for DALL-E 3 API endpoints.','# DALLE3_BASEURL='],
    ['DALLE2_BASEURL', 'string', 'The base URL for DALL-E 2 API endpoints.','# DALLE2_BASEURL='],
  ]}
/>

**Azure OpenAI Integration (Optional):**
<OptionTable
  options={[
    ['DALLE3_AZURE_API_VERSION', 'string', 'The API version for DALL-E 3 with Azure OpenAI service.','# DALLE3_AZURE_API_VERSION='],
    ['DALLE2_AZURE_API_VERSION', 'string', 'The API version for DALL-E 2 with Azure OpenAI service.','# DALLE2_AZURE_API_VERSION='],
  ]}
/>

Remember to replace placeholder text with actual prompts or instructions and provide your actual API keys if you choose to include them directly in the file (though managing sensitive keys outside of the codebase is a best practice). Always review and respect OpenAI's usage policies when embedding API keys in software.

> Note: if you have PROXY set, it will be used for DALL-E calls also, which is universal for the app.


#### DALL-E (Azure)

Here's the updated layout for the DALL-E configuration options:

**API Keys:**
<OptionTable
  options={[
    ['DALLE_API_KEY', 'string', 'The OpenAI API key for DALL-E 2 and DALL-E 3 services.','# DALLE_API_KEY='],
  ]}
/>

**API Keys (Version Specific):**
<OptionTable
  options={[
    ['DALLE3_API_KEY', 'string', 'The OpenAI API key for DALL-E 3.','# DALLE3_API_KEY='],
    ['DALLE2_API_KEY', 'string', 'The OpenAI API key for DALL-E 2.','# DALLE2_API_KEY='],
  ]}
/>

**System Prompts:**
<OptionTable
  options={[
    ['DALLE3_SYSTEM_PROMPT', 'string', 'The system prompt for DALL-E 3.','# DALLE3_SYSTEM_PROMPT="Your DALL-E-3 System Prompt here"'],
    ['DALLE2_SYSTEM_PROMPT', 'string', 'The system prompt for DALL-E 2.','# DALLE2_SYSTEM_PROMPT="Your DALL-E-2 System Prompt here"'],
  ]}
/>

**Reverse Proxy Settings:**
<OptionTable
  options={[
    ['DALLE_REVERSE_PROXY', 'string', 'The reverse proxy URL for DALL-E API requests.','# DALLE_REVERSE_PROXY='],
  ]}
/>

**Base URLs:**
<OptionTable
  options={[
    ['DALLE3_BASEURL', 'string', 'The base URL for DALL-E 3 API endpoints.','# DALLE3_BASEURL=https://<AZURE_OPENAI_API_INSTANCE_NAME>.openai.azure.com/openai/deployments/<DALLE3_DEPLOYMENT_NAME>/'],
    ['DALLE2_BASEURL', 'string', 'The base URL for DALL-E 2 API endpoints.','# DALLE2_BASEURL=https://<AZURE_OPENAI_API_INSTANCE_NAME>.openai.azure.com/openai/deployments/<DALLE2_DEPLOYMENT_NAME>/'],
  ]}
/>

**Azure OpenAI Integration (Optional):**
<OptionTable
  options={[
    ['DALLE3_AZURE_API_VERSION', 'string', 'The API version for DALL-E 3 with Azure OpenAI service.','# DALLE3_AZURE_API_VERSION=the-api-version # e.g.: 2023-12-01-preview'],
    ['DALLE2_AZURE_API_VERSION', 'string', 'The API version for DALL-E 2 with Azure OpenAI service.','# DALLE2_AZURE_API_VERSION=the-api-version # e.g.: 2023-12-01-preview'],
  ]}
/>

Remember to replace placeholder text with actual prompts or instructions and provide your actual API keys if you choose to include them directly in the file (though managing sensitive keys outside of the codebase is a best practice). Always review and respect OpenAI's usage policies when embedding API keys in software.

> Note: if you have PROXY set, it will be used for DALL-E calls also, which is universal for the app.

#### Google Search

See detailed instructions here: **[Google Search](/docs/configuration/tools/google_search)**

**Environment Variables:**

<OptionTable
  options={[
    ['GOOGLE_SEARCH_API_KEY', 'string', 'Google Search API key.','GOOGLE_SEARCH_API_KEY='],
    ['GOOGLE_CSE_ID', 'string', 'Google Custom Search Engine ID.','GOOGLE_CSE_ID='],
  ]}
/>

#### SerpAPI

**Description:** SerpApi is a real-time API to access Google search results (not as performant)

**Environment Variables:**

<OptionTable
  options={[
    ['SERPAPI_API_KEY', 'string', 'Your SerpAPI API key.','SERPAPI_API_KEY='],
  ]}
/>

#### Stable Diffusion (Automatic1111)

See detailed instructions here: **[Stable Diffusion](/docs/configuration/tools/stable_diffusion)**

**Description:** Use `http://127.0.0.1:7860` with local install and `http://host.docker.internal:7860` for docker

**Environment Variables:**

<OptionTable
  options={[
    ['SD_WEBUI_URL', 'string', 'Stable Diffusion web UI URL.','SD_WEBUI_URL=http://host.docker.internal:7860'],
  ]}
/>

### Tavily

Get your API key here: **[https://tavily.com/#api](https://tavily.com/#api)**

**Environment Variables:**

<OptionTable
  options={[    ['TAVILY_API_KEY', 'string', 'Tavily API key.','TAVILY_API_KEY='],
  ]}
/>

### Traversaal

**Description:** LLM-enhanced search tool.

Get API key here: **https://api.traversaal.ai/dashboard**

**Environment Variables:**

<OptionTable
  options={[
    ['TRAVERSAAL_API_KEY', 'string', 'Traversaal API key.','TRAVERSAAL_API_KEY='],
  ]}
/>

### WolframAlpha

See detailed instructions here: **[Wolfram Alpha](/docs/configuration/tools/wolfram)**

**Environment Variables:**

<OptionTable
  options={[
    ['WOLFRAM_APP_ID', 'string', 'Wolfram Alpha App ID.','WOLFRAM_APP_ID='],
  ]}
/>

### Zapier

**Description:** - You need a Zapier account. Get your API key from here: **[Zapier](https://nla.zapier.com/credentials/)**
- Create allowed actions - Follow step 3 in this getting start guide from Zapier

**Note:** Zapier is known to be finicky with certain actions. Writing email drafts is probably the best use of it.

**Environment Variables:**

<OptionTable
  options={[
    ['ZAPIER_NLA_API_KEY', 'string', 'Zapier NLA API key.','ZAPIER_NLA_API_KEY='],
  ]}
/>

## Artifacts

Artifacts leverage the CodeSandbox library for secure rendering of HTML/JS code. By default, the public CDN hosted by CodeSandbox is used.

Fortunately, for those with internal network requirements, you can [self-host the bundler](https://sandpack.codesandbox.io/docs/guides/hosting-the-bundler) that compiles the frontend code and specify a custom bundler URL for Sandpack.

For more info, including pre-made container images for self-hosting with metric requests removed, see: https://github.com/LibreChat-AI/codesandbox-client

<OptionTable
  options={[
    ['SANDPACK_BUNDLER_URL', 'string', 'Specifies a custom bundler URL for Sandpack, used by Artifacts','SANDPACK_BUNDLER_URL=your-bundler-url'],
  ]}
/>

## Search (Meilisearch)

Enables search in messages and conversations:

<OptionTable
  options={[
    ['SEARCH', 'boolean', 'Enables search in messages and conversations.','SEARCH=true'],
  ]}
/>

> Note: If you're not using docker, it requires the installation of the free self-hosted Meilisearch or a paid remote plan

To disable anonymized telemetry analytics for MeiliSearch for absolute privacy, set to true:

<OptionTable
  options={[
    ['MEILI_NO_ANALYTICS', 'boolean', 'Disables anonymized telemetry analytics for MeiliSearch.','MEILI_NO_ANALYTICS=true'],
  ]}
/>

For the API server to connect to the search server. Replace '0.0.0.0' with 'meilisearch' if serving MeiliSearch with docker-compose.

<OptionTable
  options={[
    ['MEILI_HOST', 'string', 'The API server connection to the search server.','MEILI_HOST=http://0.0.0.0:7700'],
  ]}
/>

This master key must be at least 16 bytes, composed of valid UTF-8 characters. MeiliSearch will throw an error and refuse to launch if no master key is provided or if it is under 16 bytes. MeiliSearch will suggest a secure autogenerated master key. This is a ready-made secure key for docker-compose, you can replace it with your own.

<OptionTable
  options={[
    ['MEILI_MASTER_KEY', 'string', 'The master key for MeiliSearch.','MEILI_MASTER_KEY=DrhYf7zENyR6AlUCKmnz0eYASOQdl6zxH7s7MKFSfFCt'],
  ]}
/>

To prevent LibreChat from attempting a database indexing sync with Meilisearch, you can set the following environment variable to `true`. This is useful in a node cluster, or multi-node setup, where only one instance should be responsible for indexing.

<OptionTable
  options={[
    ['MEILI_NO_SYNC', 'string', 'Toggle for disabling Mellisearch index sync','MEILI_NO_SYNC=true'],
  ]}
/>

## User System
This section contains the configuration for:

  - [Automated Moderation](#moderation)
  - [Balance/Token Usage](#balance)
  - [Registration and Social Logins](#registration-and-login)
  - [Email Password Reset](#email-password-reset)

### Moderation
The Automated Moderation System uses a scoring mechanism to track user violations. As users commit actions like excessive logins, registrations, or messaging, they accumulate violation scores. Upon reaching a set threshold, the user and their IP are temporarily banned. This system ensures platform security by monitoring and penalizing rapid or suspicious activities.

see: **[Automated Moderation](/docs/configuration/mod_system)**

#### Basic Moderation Settings
<OptionTable
  options={[
    ['OPENAI_MODERATION', 'boolean', 'Whether or not to enable OpenAI moderation on the **OpenAI** and **Plugins** endpoints.','OPENAI_MODERATION=false'],
    ['OPENAI_MODERATION_API_KEY', 'string', 'Your OpenAI API key.','OPENAI_MODERATION_API_KEY='],
    ['OPENAI_MODERATION_REVERSE_PROXY', 'string', 'Note: Commented out by default, this is not working with all reverse proxys.','# OPENAI_MODERATION_REVERSE_PROXY='],
  ]}
/>

#### Banning Settings
<OptionTable
  options={[
    ['BAN_VIOLATIONS', 'boolean', 'Whether or not to enable banning users for violations (they will still be logged).','BAN_VIOLATIONS=true'],
    ['BAN_DURATION', 'integer', 'How long the user and associated IP are banned for (in milliseconds).','BAN_DURATION=1000 * 60 * 60 * 2'],
    ['BAN_INTERVAL', 'integer', 'The user will be banned every time their score reaches/crosses over the interval threshold.','BAN_INTERVAL=20'],
  ]}
/>

#### Score for each violation
<OptionTable
  options={[
    ['LOGIN_VIOLATION_SCORE', 'integer', 'Score for login violations.','LOGIN_VIOLATION_SCORE=1'],
    ['REGISTRATION_VIOLATION_SCORE', 'integer', 'Score for registration violations.','REGISTRATION_VIOLATION_SCORE=1'],
    ['CONCURRENT_VIOLATION_SCORE', 'integer', 'Score for concurrent violations.','CONCURRENT_VIOLATION_SCORE=1'],
    ['MESSAGE_VIOLATION_SCORE', 'integer', 'Score for message violations.','MESSAGE_VIOLATION_SCORE=1'],
    ['NON_BROWSER_VIOLATION_SCORE', 'integer', 'Score for non-browser violations.','NON_BROWSER_VIOLATION_SCORE=20'],
    ['ILLEGAL_MODEL_REQ_SCORE', 'integer', 'Score for illegal model requests.','ILLEGAL_MODEL_REQ_SCORE=5'],
  ]}
/>

> Note: Non-browser access and Illegal model requests are almost always nefarious as it means a 3rd party is attempting to access the server through an automated script.


#### Message rate limiting (per user & IP)

<OptionTable
  options={[
    ['LIMIT_CONCURRENT_MESSAGES', 'boolean', 'Whether to limit the amount of messages a user can send per request.','LIMIT_CONCURRENT_MESSAGES=true'],
    ['CONCURRENT_MESSAGE_MAX', 'integer', 'The max amount of messages a user can send per request.','CONCURRENT_MESSAGE_MAX=2'],
  ]}
/>

#### Limiters

> Note: You can utilize both limiters, but default is to limit by IP only.

##### IP Limiter:

<OptionTable
  options={[
    ['LIMIT_MESSAGE_IP', 'boolean', 'Whether to limit the amount of messages an IP can send per `MESSAGE_IP_WINDOW`.','LIMIT_MESSAGE_IP=true'],
    ['MESSAGE_IP_MAX', 'integer', 'The max amount of messages an IP can send per `MESSAGE_IP_WINDOW`.','MESSAGE_IP_MAX=40'],
    ['MESSAGE_IP_WINDOW', 'integer', 'In minutes, determines the window of time for `MESSAGE_IP_MAX` messages.','MESSAGE_IP_WINDOW=1'],
  ]}
/>

##### User Limiter:

<OptionTable
  options={[
    ['LIMIT_MESSAGE_USER', 'boolean', 'Whether to limit the amount of messages an user can send per `MESSAGE_USER_WINDOW`.','LIMIT_MESSAGE_USER=false'],
    ['MESSAGE_USER_MAX', 'integer', 'The max amount of messages an user can send per `MESSAGE_USER_WINDOW`.','MESSAGE_USER_MAX=40'],
    ['MESSAGE_USER_WINDOW', 'integer', 'In minutes, determines the window of time for `MESSAGE_USER_MAX` messages.','MESSAGE_USER_WINDOW=1'],
  ]}
/>

### Balance

The following feature allows for the management of user balances within the system's endpoints. You have the option to add balances manually, or you may choose to implement a system that accumulates balances automatically for users. If a specific initial balance is defined in the configuration, tokens will be credited to the user's balance automatically when they register.

see: **[Token Usage](/docs/configuration/token_usage)**

<OptionTable
  options={[
    ['CHECK_BALANCE', 'boolean', 'Enable token credit balances for the OpenAI/Plugins endpoints.','CHECK_BALANCE=false'],
    ['START_BALANCE', 'integer', 'If the value is set, tokens will be credited to the user\'s balance after registration.', 'START_BALANCE=20000']
  ]}
/>

#### Managing Balances

- Run `npm run add-balance` to manually add balances.
    - You can also specify the email and token credit amount to add, e.g.: `npm run add-balance example@example.com 1000`
- Run `npm run set-balance` to manually set balances, similar to `add-balance`.
- Run `npm run list-balances` to list the balance of every user.

> **Note:** 1000 credits = $0.001 (1 mill USD)


### Registration and Login
see: **[Authentication System](/docs/configuration/authentication)**

<div style={{display: "flex", justifyContent: "center", alignItems: "center", flexDirection: "column"}}>
  <div className="image-light-theme">
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/4c51dc25-31d3-4c51-8c2a-0cdfb5a25033" style={{ width: "75%", height: "75%" }} alt="Image for Light Theme" />
  </div>

  <div className="image-dark-theme">
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/3bc5371d-e51d-4e91-ac68-56db6e85bb2c" style={{ width: "75%", height: "75%" }} alt="Image for Dark Theme" />
  </div>
</div>

- General Settings:

<OptionTable
  options={[
    ['ALLOW_EMAIL_LOGIN', 'boolean', 'Enable or disable ONLY email login.','ALLOW_EMAIL_LOGIN=true'],
    ['ALLOW_REGISTRATION', 'boolean', 'Enable or disable Email registration of new users.','ALLOW_REGISTRATION=true'],
    ['ALLOW_SOCIAL_LOGIN', 'boolean', 'Allow users to connect to LibreChat with various social networks.','ALLOW_SOCIAL_LOGIN=false'],
    ['ALLOW_SOCIAL_REGISTRATION', 'boolean', 'Enable or disable registration of new users using various social networks.','ALLOW_SOCIAL_REGISTRATION=false'],
    ['ALLOW_PASSWORD_RESET', 'boolean', 'Enable or disable the ability for users to reset their password by themselves','ALLOW_PASSWORD_RESET=false'],
    ['ALLOW_ACCOUNT_DELETION', 'boolean', 'Enable or disable the ability for users to delete their account by themselves. Enabled by default if omitted/commented out','ALLOW_ACCOUNT_DELETION=true'],
    ['ALLOW_UNVERIFIED_EMAIL_LOGIN', 'boolean', 'Set to true to allow users to log in without verifying their email address. If set to false, users will be required to verify their email before logging in.', 'ALLOW_UNVERIFIED_EMAIL_LOGIN=true'],
  ]}
/>

> **Quick Tip:** Even with registration disabled, add users directly to the database using `npm run create-user`.
> **Quick Tip:** With registration disabled, you can delete a user with `npm run delete-user email@domain.com`.

- Session and Refresh Token Settings:

<OptionTable
  options={[
    ['SESSION_EXPIRY', 'integer (milliseconds)', 'Session expiry time.','SESSION_EXPIRY=1000 * 60 * 15'],
    ['REFRESH_TOKEN_EXPIRY', 'integer (milliseconds)', 'Refresh token expiry time.','REFRESH_TOKEN_EXPIRY=(1000 * 60 * 60 * 24) * 7'],
  ]}
/>

- For more information: **[Refresh Token](https://github.com/danny-avila/LibreChat/pull/927)**

- JWT Settings:

You should use new secure values. The examples given are 32-byte keys (64 characters in hex).
Use this replit to generate some quickly: **[JWT Keys](/toolkit/creds_generator)**

<OptionTable
  options={[
    ['JWT_SECRET', 'string (hex)', 'JWT secret key.','JWT_SECRET=16f8c0ef4a5d391b26034086c628469d3f9f497f08163ab9b40137092f2909ef'],
    ['JWT_REFRESH_SECRET', 'string (hex)', 'JWT refresh secret key.','JWT_REFRESH_SECRET=eaa5191f2914e30b9387fd84e254e4ba6fc51b4654968a9b0803b456a54b8418'],
  ]}
/>

### Social Logins
For more details: [OAuth2-OIDC](/docs/configuration/authentication/OAuth2-OIDC)

#### [Apple Authentication](/docs/configuration/authentication/OAuth2-OIDC/apple)

For more information: **[Apple Authentication](/docs/configuration/authentication/OAuth2-OIDC/apple)**

<OptionTable
  options={[
    ['APPLE_CLIENT_ID', 'string', 'Your Apple Services ID (e.g., com.yourdomain.librechat.services).', 'APPLE_CLIENT_ID=com.yourdomain.librechat.services'],
    ['APPLE_TEAM_ID', 'string', 'Your Apple Developer Team ID.', 'APPLE_TEAM_ID=YOUR_TEAM_ID'],
    ['APPLE_KEY_ID', 'string', 'Your Apple Key ID from the downloaded key.', 'APPLE_KEY_ID=YOUR_KEY_ID'],
    ['APPLE_PRIVATE_KEY_PATH', 'string', 'Absolute path to your downloaded .p8 file.', 'APPLE_PRIVATE_KEY_PATH=/path/to/AuthKey.p8'],
    ['APPLE_CALLBACK_URL', 'string', 'The callback URL for Apple authentication.', 'APPLE_CALLBACK_URL=/oauth/apple/callback'],
  ]}
/>


#### [Discord Authentication](/docs/configuration/authentication/OAuth2-OIDC/discord)

For more information: **[Discord](/docs/configuration/authentication/OAuth2-OIDC/discord)**

<OptionTable
  options={[
    ['DISCORD_CLIENT_ID', 'string', 'Your Discord client ID.','DISCORD_CLIENT_ID='],
    ['DISCORD_CLIENT_SECRET', 'string', 'Your Discord client secret.','DISCORD_CLIENT_SECRET='],
    ['DISCORD_CALLBACK_URL', 'string', 'The callback URL for Discord authentication.','DISCORD_CALLBACK_URL=/oauth/discord/callback'],
  ]}
/>

#### [Facebook Authentication](/docs/configuration/authentication/OAuth2-OIDC/facebook)

For more information: **[Facebook Authentication](/docs/configuration/authentication/OAuth2-OIDC/facebook)**

<OptionTable
  options={[
    ['FACEBOOK_CLIENT_ID', 'string', 'Your Facebook client ID.','FACEBOOK_CLIENT_ID='],
    ['FACEBOOK_CLIENT_SECRET', 'string', 'Your Facebook client secret.','FACEBOOK_CLIENT_SECRET='],
    ['FACEBOOK_CALLBACK_URL', 'string', 'The callback URL for Facebook authentication.','FACEBOOK_CALLBACK_URL=/oauth/facebook/callback'],
  ]}
/>

#### [GitHub Authentication](/docs/configuration/authentication/OAuth2-OIDC/github)

For more information: **[GitHub Authentication](/docs/configuration/authentication/OAuth2-OIDC/github)**

<OptionTable
  options={[
    ['GITHUB_CLIENT_ID', 'string', 'Your GitHub client ID.','GITHUB_CLIENT_ID='],
    ['GITHUB_CLIENT_SECRET', 'string', 'Your GitHub client secret.','GITHUB_CLIENT_SECRET='],
    ['GITHUB_CALLBACK_URL', 'string', 'The callback URL for GitHub authentication.','GITHUB_CALLBACK_URL=/oauth/github/callback'],
    ['GITHUB_ENTERPRISE_BASE_URL', 'string', 'Optional: The base URL for your GitHub Enterprise instance.', 'GITHUB_ENTERPRISE_BASE_URL='],
    ['GITHUB_ENTERPRISE_USER_AGENT', 'string', 'Optional: The user agent for GitHub Enterprise requests.', 'GITHUB_ENTERPRISE_USER_AGENT='],
  ]}
/>

#### [Google Authentication](/docs/configuration/authentication/OAuth2-OIDC/google)

For more information: **[Google Authentication](/docs/configuration/authentication/OAuth2-OIDC/google)**

<OptionTable
  options={[
    ['GOOGLE_CLIENT_ID', 'string', 'Your Google client ID.','GOOGLE_CLIENT_ID='],
    ['GOOGLE_CLIENT_SECRET', 'string', 'Your Google client secret.','GOOGLE_CLIENT_SECRET='],
    ['GOOGLE_CALLBACK_URL', 'string', 'The callback URL for Google authentication.','GOOGLE_CALLBACK_URL=/oauth/google/callback'],
  ]}
/>

#### [OpenID Connect](/docs/configuration/authentication/OAuth2-OIDC#openid-connect)

For more information:
  - [AWS Cognito](/docs/configuration/authentication/OAuth2-OIDC/aws)
  - [Azure Entra/AD](/docs/configuration/authentication/OAuth2-OIDC/azure)
  - [Keycloak](/docs/configuration/authentication/OAuth2-OIDC/keycloak)

<OptionTable
  options={[
    ['OPENID_CLIENT_ID', 'string', 'Your OpenID client ID.','OPENID_CLIENT_ID='],
    ['OPENID_CLIENT_SECRET', 'string', 'Your OpenID client secret.','OPENID_CLIENT_SECRET='],
    ['OPENID_ISSUER', 'string', 'The OpenID issuer URL.','OPENID_ISSUER='],
    ['OPENID_SESSION_SECRET', 'string', 'The secret for OpenID session storage.','OPENID_SESSION_SECRET='],
    ['OPENID_SCOPE', 'string', 'The OpenID scope.', 'OPENID_SCOPE="openid profile email"'],
    ['OPENID_CALLBACK_URL', 'string', 'The callback URL for OpenID authentication.','OPENID_CALLBACK_URL=/oauth/openid/callback'],
    ['OPENID_REQUIRED_ROLE', 'string', 'The required role for validation.','OPENID_REQUIRED_ROLE='],
    ['OPENID_REQUIRED_ROLE_TOKEN_KIND', 'string', 'The token kind for required role validation.','OPENID_REQUIRED_ROLE_TOKEN_KIND='],
    ['OPENID_REQUIRED_ROLE_PARAMETER_PATH', 'string', 'The parameter path for required role validation.','OPENID_REQUIRED_ROLE_PARAMETER_PATH='],
    ['OPENID_BUTTON_LABEL', 'string', 'The label for the OpenID login button.','OPENID_BUTTON_LABEL='],
    ['OPENID_IMAGE_URL', 'string', 'The URL of the OpenID login button image.','OPENID_IMAGE_URL='],
    ['OPENID_USE_END_SESSION_ENDPOINT', 'string', 'Whether to use the Issuer End Session Endpoint as a Logout Redirect','OPENID_USE_END_SESSION_ENDPOINT=TRUE'],
    ['OPENID_AUTO_REDIRECT', 'boolean', 'Whether to automatically redirect to the OpenID provider.','OPENID_AUTO_REDIRECT=true'],
  ]}
/>

#### [LDAP/AD Authentication](/docs/configuration/authentication/ldap)

For more information: **[LDAP/AD Authentication](/docs/configuration/authentication/ldap)**

<OptionTable
    options={[
        ['LDAP_URL', 'string', 'LDAP server URL.', 'LDAP_URL=ldap://localhost:389'],
        ['LDAP_BIND_DN', 'string', 'Bind DN', 'LDAP_BIND_DN=cn=root'],
        ['LDAP_BIND_CREDENTIALS', 'string', 'Password for bindDN', 'LDAP_BIND_CREDENTIALS=password'],
        [
            'LDAP_USER_SEARCH_BASE',
            'string',
            'LDAP user search base',
            'LDAP_USER_SEARCH_BASE=o=users,o=example.com',
        ],
        ['LDAP_SEARCH_FILTER', 'string', 'LDAP search filter', 'LDAP_SEARCH_FILTER=mail={{username}}'],
        [
            'LDAP_CA_CERT_PATH',
            'string',
            'CA certificate path.',
            'LDAP_CA_CERT_PATH=/path/to/root_ca_cert.crt',
        ],
        [
            'LDAP_TLS_REJECT_UNAUTHORIZED',
            'string',
            'LDAP TLS verification',
            'LDAP_TLS_REJECT_UNAUTHORIZED=true',
        ],
        [
            'LDAP_STARTTLS',
            'string',
            'Enable LDAP StartTLS for upgrading the connection to TLS. Set to true to enable this feature.',
            'LDAP_STARTTLS=true',
        ],
    ]}
/>

### Password Reset

Email is used for account verification and password reset. See: **[Email setup](/docs/configuration/authentication/email)**

**Important Note**: All of the service or host, username, and password, and the From address must be set for email to work.

> **Warning**: If using `EMAIL_SERVICE`, **do NOT** set the extended connection parameters:
> HOST, PORT, ENCRYPTION, ENCRYPTION_HOSTNAME, ALLOW_SELFSIGNED.
> Failing to set valid values here will result in LibreChat using the unsecured password reset!

See: **[nodemailer well-known-services](https://community.nodemailer.com/2-0-0-beta/setup-smtp/well-known-services/)**

<OptionTable
  options={[
    ['EMAIL_SERVICE', 'string', 'Email service (e.g., Gmail, Outlook).','EMAIL_SERVICE='],
    ['EMAIL_HOST', 'string', 'Mail server host.','EMAIL_HOST='],
    ['EMAIL_PORT', 'number', 'Mail server port.','EMAIL_PORT=25'],
    ['EMAIL_ENCRYPTION', 'string', 'Encryption method (starttls, tls, etc.).','EMAIL_ENCRYPTION='],
    ['EMAIL_ENCRYPTION_HOSTNAME', 'string', 'Hostname for encryption.','EMAIL_ENCRYPTION_HOSTNAME='],
    ['EMAIL_ALLOW_SELFSIGNED', 'boolean', 'Allow self-signed certificates.','EMAIL_ALLOW_SELFSIGNED='],
    ['EMAIL_USERNAME', 'string', 'Username for authentication.','EMAIL_USERNAME='],
    ['EMAIL_PASSWORD', 'string', 'Password for authentication.','EMAIL_PASSWORD='],
    ['EMAIL_FROM_NAME', 'string', 'From name.','EMAIL_FROM_NAME='],
    ['EMAIL_FROM', 'string', 'From email address. Required.','EMAIL_FROM=noreply@librechat.ai'],
  ]}
/>

### Firebase CDN

See: **[Firebase CDN Configuration](/docs/configuration/firebase)**

<Callout type="warning" title="Important">
- If you are using Firebase as your file storage strategy, make sure to set the `file_strategy` option to `firebase` in your `librechat.yaml` configuration file. - For more information on configuring the `librechat.yaml` file, please refer to the YAML Configuration Guide: [Custom Endpoints & Configuration](/docs/configuration/librechat_yaml)
</Callout>

<OptionTable
  options={[
    ['FIREBASE_API_KEY', 'string', 'The API key for your Firebase project.', 'FIREBASE_API_KEY='],
    ['FIREBASE_AUTH_DOMAIN', 'string', 'The Firebase Auth domain for your project.', 'FIREBASE_AUTH_DOMAIN='],
    ['FIREBASE_PROJECT_ID', 'string', 'The ID of your Firebase project.', 'FIREBASE_PROJECT_ID='],
    ['FIREBASE_STORAGE_BUCKET', 'string', 'The Firebase Storage bucket for your project.', 'FIREBASE_STORAGE_BUCKET='],
    ['FIREBASE_MESSAGING_SENDER_ID', 'string', 'The Firebase Cloud Messaging sender ID.', 'FIREBASE_MESSAGING_SENDER_ID='],
    ['FIREBASE_APP_ID', 'string', 'The Firebase App ID for your project.', 'FIREBASE_APP_ID='],
  ]}
/>

### UI

#### Help and FAQ Button

<OptionTable
  options={[
    ['HELP_AND_FAQ_URL', 'string', 'Help and FAQ URL. If empty or commented, the button is enabled.','HELP_AND_FAQ_URL=https://librechat.ai'],
  ]}
/>

**Behaviour:**

Sets the [Cache-Control](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control) headers for static files. These configurations only trigger when the `NODE_ENV` is set to `production`.

Properly setting cache headers is crucial for optimizing the performance and efficiency of your web application. By controlling how long browsers and CDNs store copies of your static files, you can significantly reduce server load, decrease page load times, and improve the overall user experience.

* Uncomment `STATIC_CACHE_MAX_AGE` to change the `max-age` for static files. By default this is set to 4 weeks.
* Uncomment `STATIC_CACHE_S_MAX_AGE` to change the `s-maxage` for static files. By default this is set to 1 week.
  - This is for the _shared cache_, which is used by CDNs and proxies.

#### App Title and Footer

<OptionTable
  options={[
    ['APP_TITLE', 'string', 'App title.','APP_TITLE=LibreChat'],
    ['CUSTOM_FOOTER', 'string', 'Custom footer.','# CUSTOM_FOOTER="My custom footer"'],
  ]}
/>

**Behaviour:**

 * Uncomment `CUSTOM_FOOTER` to add a custom footer.
 * Uncomment and leave `CUSTOM_FOOTER` empty to remove the footer.
 * You can now add one or more links in the CUSTOM_FOOTER value using the following format: `[Anchor text](URL)`. Each link should be delineated with a pipe (`|`).

 > **Markdown example:** `CUSTOM_FOOTER=[Link 1](http://example1.com) | [Link 2](http://example2.com)`

#### Birthday Hat

<OptionTable
  options={[
    ['SHOW_BIRTHDAY_ICON', 'boolean', 'Show the birthday hat icon.','# SHOW_BIRTHDAY_ICON=true'],
  ]}
/>

**Behaviour:**

* The birthday hat icon will show automatically on February 11th (LibreChat's birthday).
* Set `SHOW_BIRTHDAY_ICON` to `false` to disable the birthday hat.
* Set `SHOW_BIRTHDAY_ICON` to `true` to enable the birthday hat all the time.


### Analytics

#### Google Tag Manager

LibreChat supports Google Tag Manager for analytics. You will need a Google Tag Manager ID to enable it in LibreChat. Follow [this guide](https://support.google.com/tagmanager/answer/9442095?sjid=10155093630524971297-EU) to generate a Google Tag Manager ID and configure Google Analytics. Then set the `ANALYTICS_GTM_ID` environment variable to your Google Tag Manager ID.

**Note:** If `ANALYTICS_GTM_ID` is not set, Google Tag Manager will not be enabled. If it is set incorrectly, you will see failing requests to `gtm.js`

<OptionTable
  options={[
    ['ANALYTICS_GTM_ID', 'string', 'Google Tag Manager ID.','ANALYTICS_GTM_ID='],
  ]}
/>

### Other

#### Redis

**Note:** Redis support is experimental, and you may encounter some problems when using it.

**Important:** If using Redis, you should flush the cache after changing any LibreChat settings.

If you are using Redis, you will need to set the following variables:

* `REDIS_URI`: The URI for your Redis instance.
* `USE_REDIS`: Set to `true` to enable Redis.
* `USE_REDIS_CLUSTER`: Set to `true` to enable Redis Cluster mode.
* `REDIS_CA`: The path to the PEM-encoded certificate authority file for Redis TLS connections.
* `REDIS_KEY_PREFIX`: A prefix to be added to all keys in the Redis database. Defaults to empty string if not specified.
* `REDIS_MAX_LISTENERS`: The maximum number of event listeners allowed for the Redis client instance. It helps prevent memory leaks by limiting event listeners. If set to 0 (zero), it will be considered limitless. Defaults to 10 if not specified.

<OptionTable
  options={[
    ['REDIS_URI', 'string', 'Redis URI.', '# REDIS_URI="10.11.12.13:6379"'],
    ['USE_REDIS', 'boolean', 'Use Redis.', '# USE_REDIS="true"'],
    ['USE_REDIS_CLUSTER', 'boolean', 'User Redis Cluster mode', '# USE_REDIS_CLUSTER="true"'],
    ['REDIS_CA', 'string', 'Path to certificate', '# REDIS_CA="/path/to/file.crt"'],
    ['REDIS_KEY_PREFIX', 'string', 'Prefix for Redis keys', '# REDIS_KEY_PREFIX="librechat-staging:"'],
    ['REDIS_MAX_LISTENERS', 'number', 'Maximum number of event listeners allowed for the Redis client instance', '# REDIS_MAX_LISTENERS=20'],
  ]}
/>



================================================
FILE: pages/docs/configuration/index.mdx
================================================
---
title: Intro
description: Consult our advanced configuration guides for LibreChat
---

# Configuration

Whether you are setting up a basic AI chat application or scaling an enterprise solution, these guides provide step-by-step instructions and in-depth details on every aspect of LibreChat.

In this documentation, you will discover how to deploy LibreChat using Docker for a seamless and scalable installation, configure endpoints, and integrate social logins to enhance user accessibility and engagement.

We also cover the essentials of setting up the LibreChat config file, which is the heart of customizing your AI chat solution to fit your operational needs. The moderation system configuration will ensure you maintain a safe and productive environment for your users.

Additionally, you will find resources on integrating the RAG API, enabling you to extend LibreChat's functionality and interactivity.

And a lot more!

Whether you are a beginner or a seasoned developer, these guides are designed to provide you with all the tools you need to succeed. Let's make your LibreChat experience smooth, efficient, and tailored to your goals. Thank you for choosing LibreChat, and happy configuring!


================================================
FILE: pages/docs/configuration/logging.mdx
================================================
---
title: Logging System
description: This doc explains how to use the logging feature of LibreChat, which saves error and debug logs in the `/api/logs` folder. You can use these logs to troubleshoot issues, monitor your server, and report bugs. You can also disable debug logs if you want to save space.
---

### General

LibreChat has central logging built into its backend (api).

- With the **docker** install, log files are saved in `/logs`

<FileTree>
  <FileTree.Folder name="librechat" defaultOpen>
    <FileTree.Folder name="logs" defaultOpen>
      <FileTree.File name="debug-2024-01-01.log" active/>
      <FileTree.File name="error-2024-01-01.log" active/>
      <FileTree.File name="meiliSync-2024-01-01.log" active/>
    </FileTree.Folder>
  </FileTree.Folder>
</FileTree>

- With the **npm** install, log files are saved in `/api/logs`

<FileTree>
  <FileTree.Folder name="librechat" defaultOpen>
      <FileTree.Folder name="api" defaultOpen>
        <FileTree.Folder name="logs" defaultOpen>
        <FileTree.File name="debug-2024-01-01.log" active/>
        <FileTree.File name="error-2024-01-01.log" active/>
        <FileTree.File name="meiliSync-2024-01-01.log" active/>
        </FileTree.Folder>
      </FileTree.Folder>  
  </FileTree.Folder>
</FileTree>

Error logs are saved by default. Debug logs are enabled by default but can be turned off if not desired.

This allows you to monitor your server through external tools that inspect log files, such as **[the ELK stack](https://aws.amazon.com/what-is/elk-stack/)**.

Debug logs are essential for developer work and fixing issues. If you encounter any problems running LibreChat, reproduce as close as possible, and **[report the issue](https://github.com/danny-avila/LibreChat/issues)** with your logs found in `./api/logs/debug-%DATE%.log`. 

Errors logs are also saved in the same location: `./api/logs/error-%DATE%.log`. If you have meilisearch configured, there is a separate log file for this as well.

<Callout type="note" title="Note:">
Note: Logs are rotated on a 14-day basis, so you will generate one error log file, one debug log file, and one meiliSync log file per 14 days.
Errors will also be present in debug log files as well, but provide stack traces and more detail in the error log files.
</Callout>

### Setup

- Toggle debug logs with the following environment variable. By default, even if you never set this variable, debug logs will be generated, but you have the option to disable them by setting it to `FALSE`.

<OptionTable
  options={[
    ['DEBUG_LOGGING', 'boolean', 'Keep debug logs active.','DEBUG_LOGGING=true'],
  ]}
/>

> Note: it's recommended to disable debug logs in a production environment.

- For verbose server output in the console/terminal, you can set the following to `TRUE`:

<OptionTable
  options={[
    ['DEBUG_CONSOLE', 'boolean', 'Enable verbose console/stdout logs in the same format as file debug logs.', 'DEBUG_CONSOLE=false'],
  ]}
/>

This is not recommended however, as the outputs can be quite verbose. It's disabled by default and should be enabled sparingly.

- When handling console logs in cloud deployments (such as GCP or AWS), enabling this will dump the logs with a UTC timestamp and format them as JSON.

<OptionTable
  options={[
    ['CONSOLE_JSON', 'boolean', 'Enable verbose JSON console/stdout logs suitable for cloud deployments like GCP/AWS.', 'CONSOLE_JSON=false'],
  ]}
/>

By default, the JSON string length is truncated to 255 characters. You can configure this with the following environment variable:

<OptionTable
  options={[
    ['CONSOLE_JSON_STRING_LENGTH', 'number', 'Configure the truncation size for console/stdout logs, defaults to 255', 'CONSOLE_JSON_STRING_LENGTH=1000'],
  ]}
/>



================================================
FILE: pages/docs/configuration/meilisearch.mdx
================================================
---
title: Meilisearch
description: Setting up MeiliSearch for LibreChat
---

# Setting up MeiliSearch for LibreChat
- See: [Search feature in LibreChat](/docs/features/search) for a quick overview.

MeiliSearch is a powerful, open-source search engine that enhances LibreChat's functionality by enabling full-text search, typo tolerance, and instant search results for past conversations. 

Follow these steps to set up MeiliSearch for LibreChat:

## 1. Download MeiliSearch
    - Go to the MeiliSearch GitHub releases page: https://github.com/meilisearch/meilisearch/releases
    - Download the latest version of MeiliSearch for your operating system (e.g., `meilisearch-linux-amd64.tar.gz` for Linux, `meilisearch-macos-amd64` for macOS, or `meilisearch-windows-amd64.zip` for Windows).

## 2. Extract/Install MeiliSearch
    - Linux/macOS: Extract the downloaded archive to a directory of your choice.
    - Windows: Extract the ZIP file to a directory of your choice.

## 3. Make the MeiliSearch Binary Executable (Linux/macOS)
    - Open a terminal and navigate to the directory where you extracted MeiliSearch.
    - Run the following command to make the binary executable:
        ```sh filename="Make the MeiliSearch Binary Executable"
        chmod +x meilisearch
        ```

## 4. Generate a Master Key
    - Open a terminal (or Command Prompt on Windows) and navigate to the MeiliSearch directory.
    - Run the following command to generate a Master Key:
        ```sh filename="Generate a Master Key"
        ./meilisearch --generate-master-key
        ```
    - Copy the generated Master Key as you'll need it later.

## 5. Start MeiliSearch
    - In the same terminal, run the following command to start MeiliSearch, replacing `<your_master_key>` with the Master Key you generated in the previous step:
        ```sh filename="Start MeiliSearch"
        ./meilisearch --master-key=<your_master_key>
        ```
    - MeiliSearch will now start running on the default port (`7700`).

## 6. Update LibreChat's Environment Variables
    - Open the `.env` file in the root directory of your LibreChat project.
    - Add or update the following lines with your MeiliSearch configuration:

        ```sh filename=".env"
        SEARCH=true
        MEILI_NO_ANALYTICS=true
        MEILI_HOST=http://localhost:7700
        MEILI_MASTER_KEY=<your_master_key>
        ```

    - Replace `<your_master_key>` with the Master Key you generated earlier.

## 7. Start/Restart LibreChat
    - Start or restart your LibreChat application.

That's it! With MeiliSearch set up and configured, LibreChat should now have the Conversation search feature enabled, allowing users to perform full-text searches, benefit from typo tolerance, and experience instant search results for their past conversations.

Note: Make sure to keep the MeiliSearch process running in the background for the search functionality to work correctly. You may want to set up a script or service to keep MeiliSearch running persistently.

## 8. Optional: Disable Meilisearch/Database Sync in a Multi-node Setup

If you're running LibreChat in a node cluster, or multi-node setup, and want to disable the MeiliSearch indexing sync, you can set the `MEILI_NO_SYNC` environment variable to `true` in your `.env` file. This will prevent your database documents from syncing redundantly across multiple LibreChat instances, which can also lead to unnecessary resource consumption, as only one instance should be responsible for indexing.

        ```sh filename=".env"
        MEILI_NO_SYNC=true
        ```


================================================
FILE: pages/docs/configuration/metrics.mdx
================================================
---
title: Metrics
description: This document explains how to add a metrics exporter for Prometheus to LibreChat.
---

### General

![Active users in LibreChat](/images/metrics/librechat-metrics-active-users.png)

You can use Prometheus or any other OpenMetrics compatible monitoring tool to get metrics about active usage of LibreChat.
This includes technical information like the usage of tokens,
but also information about end users like the number of total or active users.

The metrics exporter is available at [virtUOS/librechat_exporter](https://github.com/virtUOS/librechat_exporter).
It is a separate tool you deploy alongside LibreChat.

### Setup

To deploy the exporter, just add the necessary container to your compose configuration like this:

```yaml
  metrics:
    image: ghcr.io/virtuos/librechat_exporter:main
    depends_on:
      - mongodb
    ports:
      - "8000:8000"
    restart: unless-stopped
```

You can optionally also configure the exporter.
But usually, the defaults should be just fine.
```yaml
    environment:
      - MONGODB_URI=mongodb://mongodb:27017/
      - LOGGING_LEVEL=info
```

### Usage

You can now add the exporter to your Prometheus scrape configuration:
```yaml
- job_name: librechat
  static_configs:
    - targets:
        - 'librechat.example.com:8000'
```

Once scraping the metrics has started, look for `librechat_*` metrics (e.g., `librechat_registered_users`).
The exporter provides several metrics.

Have fun building your Grafana dashboard!



================================================
FILE: pages/docs/configuration/mod_system.mdx
================================================
---
title: Automated Moderation
description: Configuration of the Automated Moderation System. This uses a scoring mechanism to track user violations. As users commit actions like excessive logins, registrations, or messaging, they accumulate violation scores. Upon reaching a set threshold, the user and their IP are temporarily banned. This system ensures platform security by monitoring and penalizing rapid or suspicious activities.
---

# Automated Moderation System (optional)
The Automated Moderation System uses a scoring mechanism to track user violations. As users commit actions like excessive logins, registrations, or messaging, they accumulate violation scores. Upon reaching a set threshold, the user and their IP are temporarily banned. This system ensures platform security by monitoring and penalizing rapid or suspicious activities.

In production, you should have Cloudflare or some other DDoS protection in place to really protect the server from excessive requests, but these changes will largely protect you from the single or several bad actors targeting your deployed instance for proxying.

**For further details, refer to the user guide provided here: [Automated Moderation](/docs/features/mod_system)**

## Setup

The following are all of the related env variables to make use of and configure the mod system. Note this is also found in the [/.env.example](https://github.com/danny-avila/LibreChat/blob/main/.env.example) file, to be set in your own `.env` file.

**Note:** currently, most of these values are configured through the .env file, but they may soon migrate to be exclusively configured from the [`librechat.yaml` config file](/docs/configuration/librechat_yaml/object_structure/config#ratelimits).

### Violation, Interval, Duration

<OptionTable
  options={[
    ['BAN_VIOLATIONS', 'boolean', 'Whether or not to enable banning users for violations (they will still be logged).','BAN_VIOLATIONS=true'],
    ['BAN_DURATION', 'integer', 'How long the user and associated IP are banned for (in milliseconds).','BAN_DURATION=1000 * 60 * 60 * 2'],
    ['BAN_INTERVAL', 'integer', 'The user will be banned every time their score reaches/crosses over the interval threshold.','BAN_INTERVAL=20'],
  ]}
/>

### The score for each violation

<OptionTable
  options={[    
    ['LOGIN_VIOLATION_SCORE', 'integer', 'Score for login violations.','LOGIN_VIOLATION_SCORE=1'],
    ['REGISTRATION_VIOLATION_SCORE', 'integer', 'Score for registration violations.','REGISTRATION_VIOLATION_SCORE=1'],
    ['CONCURRENT_VIOLATION_SCORE', 'integer', 'Score for concurrent violations.','CONCURRENT_VIOLATION_SCORE=1'],
    ['MESSAGE_VIOLATION_SCORE', 'integer', 'Score for message violations.','MESSAGE_VIOLATION_SCORE=1'],
    ['NON_BROWSER_VIOLATION_SCORE', 'integer', 'Score for non-browser violations.','NON_BROWSER_VIOLATION_SCORE=20'],
  ]}
/>

### Login and registration rate limiting.

<OptionTable
  options={[    
    ['LOGIN_MAX', 'number', 'The max amount of logins allowed per IP per LOGIN_WINDOW. Defaults to `7`.'],
    ['LOGIN_WINDOW', 'number', 'In minutes, determines the window of time for LOGIN_MAX logins. Defaults to `5`.'],
    ['REGISTER_MAX', 'number', 'The max amount of registrations allowed per IP per REGISTER_WINDOW. Defaults to `5`.'],
    ['REGISTER_WINDOW', 'number', 'In minutes, determines the window of time for REGISTER_MAX registrations. Defaults to `60`.'],
  ]}
/>

### Message rate limiting

<OptionTable
  options={[    
    ['LIMIT_CONCURRENT_MESSAGES', 'boolean', 'Whether to limit the amount of messages a user can send per request.','LIMIT_CONCURRENT_MESSAGES=true'],
    ['CONCURRENT_MESSAGE_MAX', 'integer', 'The max amount of messages a user can send per request.','CONCURRENT_MESSAGE_MAX=2'],
  ]}
/>

> Note: You can utilize both limiters, but default is to limit by IP only.

#### Message rate limiting (per IP)

<OptionTable
  options={[    
    ['LIMIT_MESSAGE_IP', 'boolean', 'Whether to limit the amount of messages an IP can send per `MESSAGE_IP_WINDOW`.','LIMIT_MESSAGE_IP=true'],
    ['MESSAGE_IP_MAX', 'integer', 'The max amount of messages an IP can send per `MESSAGE_IP_WINDOW`.','MESSAGE_IP_MAX=40'],
    ['MESSAGE_IP_WINDOW', 'integer', 'In minutes, determines the window of time for `MESSAGE_IP_MAX` messages.','MESSAGE_IP_WINDOW=1'],
  ]}
/>

#### Message rate limiting (per User)
<OptionTable
  options={[    
    ['LIMIT_MESSAGE_USER', 'boolean', 'Whether to limit the amount of messages an user can send per `MESSAGE_USER_WINDOW`.','LIMIT_MESSAGE_USER=false'],
    ['MESSAGE_USER_MAX', 'integer', 'The max amount of messages an user can send per `MESSAGE_USER_WINDOW`.','MESSAGE_USER_MAX=40'],
    ['MESSAGE_USER_WINDOW', 'integer', 'In minutes, determines the window of time for `MESSAGE_USER_MAX` messages.','MESSAGE_USER_WINDOW=1'],
  ]}
/>


#### Illegal model requests

> Note: Illegal model requests are almost always nefarious as it means a 3rd party is attempting to access the server through an automated script. For this, I recommend a relatively high score, no less than 5.

<OptionTable
  options={[    
    ['ILLEGAL_MODEL_REQ_SCORE', 'integer', 'Score for illegal model requests.','ILLEGAL_MODEL_REQ_SCORE=5'],
  ]}
/>

## OpenAI text moderation

<OptionTable
  options={[    
    ['OPENAI_MODERATION', 'boolean', 'Whether or not to enable OpenAI moderation on the **OpenAI** and **Plugins** endpoints.','OPENAI_MODERATION=false'],
    ['OPENAI_MODERATION_API_KEY', 'string', 'Your OpenAI API key.','OPENAI_MODERATION_API_KEY='],
  ]}
/>


Note that this might not work with all reverse proxies:

<OptionTable
  options={[    
    ['OPENAI_MODERATION_REVERSE_PROXY', 'string', 'Note: Commented out by default, this is not working with all reverse proxys.','# OPENAI_MODERATION_REVERSE_PROXY='],
  ]}
/>



================================================
FILE: pages/docs/configuration/rag_api.mdx
================================================
---
title: RAG API
description: Configure Retrieval-Augmented Generation (RAG) API for document indexing and retrieval using Langchain and FastAPI. This API integrates with LibreChat to provide context-aware responses based on user-uploaded files.
---

# RAG API Configuration

**For further details about RAG, refer to the user guide provided here: [RAG API Presentation](/docs/features/rag_api)**

---

**Currently, this feature is available to all Custom Endpoints, OpenAI, Azure OpenAi, Anthropic, and Google.**

OpenAI Assistants have their own implementation of RAG through the "Retrieval" capability. Learn more about it [here.](https://platform.openai.com/docs/assistants/tools/knowledge-retrieval) 

It will still be useful to implement usage of the RAG API with the Assistants API since OpenAI charges for both file storage, and use of "Retrieval," and will be introduced in a future update.

Plugins support is not enabled as the whole "plugin/tool" framework will get a complete rework soon, making tools available to most endpoints (ETA Summer 2024).

**Still confused about RAG?** [Read the RAG API Presentation](/docs/features/rag_api#what-is-rag) explaining the general concept in more detail with a link to a helpful video.

## Setup

To set up the RAG API with LibreChat, follow these steps:

### Docker Setup - Quick Start

#### Default Configuration

**Use RAG with OpenAI Embedding (default)**

1. Add the following to your `.env` file:

    ```sh
    RAG_API_URL=http://host.docker.internal:8000
    ```

2. If your OpenAI API key is set to "user_provided," also add this to your `.env` file to provide an OpenAI API key:
    - Note: You can ignore this step if you are already providing the OpenAI API key in the .env file
    ```sh
    RAG_OPENAI_API_KEY=sk-your-openai-api-key-example
    ```

3. Run the command to start the Docker containers:

    ```sh
    docker compose up -d
    ```

That's it!

---

#### Custom Configuration - Hugging Face

**Use RAG with Hugging Face Embedding**

1. Add the following to your `.env` file:

    ```sh
    RAG_API_URL=http://host.docker.internal:8000
    EMBEDDINGS_PROVIDER=huggingface
    HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxx
    ```

2. Update your `docker-compose.override.yml` file with:

    ```yaml
    version: '3.4'

    services:
      rag_api:
        image: ghcr.io/danny-avila/librechat-rag-api-dev:latest
    ```

3. Run the command to start the Docker containers:

    ```sh
    docker compose up -d
    ```

That's it!

---

#### Custom Configuration - Ollama

**Use RAG with Ollama Local Embedding**

**Prerequisite:** You need Ollama and the `nomic-embed-text` embedding model:

- `ollama pull nomic-embed-text`

1. Add the following to your `.env` file:

    ```sh
    RAG_API_URL=http://host.docker.internal:8000
    EMBEDDINGS_PROVIDER=ollama
    OLLAMA_BASE_URL=http://host.docker.internal:11434
    EMBEDDINGS_MODEL=nomic-embed-text
    ```

2. Update your `docker-compose.override.yml` file with:

    ```yaml
    version: '3.4'

    services:
      rag_api:
        image: ghcr.io/danny-avila/librechat-rag-api-dev:latest
        # If running on Linux
        # extra_hosts:
        #   - "host.docker.internal:host-gateway"
    ```

3. Run the command to start the Docker containers:

    ```sh
    docker compose up -d
    ```

That's it!

---

### Docker Setup

For Docker, the setup is configured for you in both the default `docker-compose.yml` and `deploy-compose.yml` files, and you will just need to make sure you are using the latest docker image and compose files. Make sure to read the [Updating LibreChat guide for Docker](/docs/local/docker#update-librechat) if you are unsure how to update your Docker instance.

Docker uses the "lite" image of the RAG API by default, which only supports remote embeddings, leveraging embeddings proccesses from OpenAI or a remote service you have configured for HuggingFace/Ollama.

Local embeddings are supported by changing the image used by the default compose file, from `ghcr.io/danny-avila/librechat-rag-api-dev-lite:latest` to `ghcr.io/danny-avila/librechat-rag-api-dev:latest`.

As always, make these changes in your update URLs [Docker Compose Override File](/docs/configuration/docker_override). You can find an example for exactly how to change the image in `docker-compose.override.yml.example` at the root of the project.

If you wish to see an example of a compose file that only includes the PostgresQL + PGVector database and the Python API, see `rag.yml` file at the root of the project.

**Important:** When using the default docker setup, the .env file, where configuration options can be set for the RAG API, is shared between LibreChat and the RAG API.

### Local Setup

Local, non-container setup is more hands-on, and for this you can refer to the [RAG API repo.](https://github.com/danny-avila/rag_api/)

In a local setup, you will need to manually set the `RAG_API_URL` in your LibreChat `.env` file to where it's available from your setup.

This contrasts Docker, where is already set in the default `docker-compose.yml` file.

## Configuration

The RAG API provides several configuration options that can be set using environment variables from an `.env` file accessible to the API. Most of them are optional, asides from the credentials/paths necessary for the provider you configured. In the default setup, only `RAG_OPENAI_API_KEY` is required.

<Callout type="warning" title="Docker" emoji="🐳">
When using the default docker setup, the .env file is shared between LibreChat and the RAG API. For this reason, it's important to define the needed variables shown in the [RAG API readme.md](https://github.com/danny-avila/rag_api/blob/main/README.md)
</Callout>

Here are some notable configurations:

- `RAG_OPENAI_API_KEY`: The API key for OpenAI API Embeddings (if using default settings).
    - Note: `OPENAI_API_KEY` will work but `RAG_OPENAI_API_KEY` will override it in order to not conflict with the LibreChat credential.
- `RAG_PORT`: The port number where the API server will run. Defaults to port 8000.
- `RAG_HOST`: The hostname or IP address where the API server will run. Defaults to "0.0.0.0"
- `COLLECTION_NAME`: The name of the collection in the vector store. Default is "testcollection".
- `RAG_USE_FULL_CONTEXT`: (Optional) Set to "True" to fetch entire context of the file(s) uploaded/referenced into the conversation. Default value is "false" which means it fetches only the top 4 results (top_k=4) of the file based on the user's message.
- `CHUNK_SIZE`: The size of the chunks for text processing. Default is "1500".
- `CHUNK_OVERLAP`: The overlap between chunks during text processing. Default is "100".
- `EMBEDDINGS_PROVIDER`: The embeddings provider to use. Options are "openai", "azure", "huggingface", "huggingfacetei", or "ollama". Default is "openai".
- `EMBEDDINGS_MODEL`: The specific embeddings model to use from the configured provider. Default is dependent on the provider; for "openai", the model is "text-embedding-3-small".
- `OLLAMA_BASE_URL`: It should be provided if RAG API runs in docker and usually is `http://host.docker.internal:11434`. 

There are several more configuration options.

For a complete list and their descriptions, please refer to the [RAG API repo.](https://github.com/danny-avila/rag_api/)

## Usage

Once the RAG API is set up and running, it seamlessly integrates with LibreChat. When a user uploads files to a conversation, the RAG API indexes those files and uses them to provide context-aware responses.

**To utilize the RAG API effectively:**

1. Ensure that the necessary files are uploaded to the conversation in LibreChat. If `RAG_API_URL` is not configured, or is not reachable, the file upload will fail.
2. As the user interacts with the chatbot, the RAG API will automatically retrieve relevant information from the indexed files based on the user's input.
3. The retrieved information will be used to augment the user's prompt, enabling LibreChat to generate more accurate and contextually relevant responses.
4. Craft your prompts carefully when you attach files as the default behavior is to query the vector store upon every new message to a conversation with a file attached. 
    - You can disable the default behavior by toggling the "Resend Files" option to an "off" state, found in the conversation settings.
    - Doing so allows for targeted file queries, making it so that the "retrieval" will only be done when files are explicitly attached to a message.
    - ![image](https://github.com/danny-avila/LibreChat/assets/110412045/29a2468d-85ac-40d7-90be-a945301c5729)
5. You only have to upload a file once to use it multiple times for RAG.
    - You can attach uploaded/indexed files to any new message or conversation using the Side Panel:
    - ![image](https://github.com/danny-avila/LibreChat/assets/110412045/b40cb3d3-e6e7-46ec-bc74-65d194f55a1e)
    - Note: The files must be in the "Host" storage, as "OpenAI" files are treated differently and exclusive to Assistants. In other words, they must not have been uploaded when the Assistants endpoint was selected and active. You can view and manage your files by clicking here from the Side Panel.
    - ![image](https://github.com/danny-avila/LibreChat/assets/110412045/1f27e974-4124-4ee3-8091-13514cb4cbca)


## Troubleshooting

If you encounter any issues while setting up or using the RAG API, consider the following:

- Double-check that all the required environment variables are correctly set in your `.env` file.
- Ensure that the vector database is properly configured and accessible.
- Verify that the OpenAI API key or other necessary credentials are valid.
- Check both the LibreChat and RAG API logs for any error messages or warnings.

If the problem persists, please refer to the RAG API documentation or seek assistance from the LibreChat community on GitHub Discussions or Discord.



================================================
FILE: pages/docs/configuration/stt_tts.mdx
================================================
---
title: Speech Settings
description: Configuration of the Speech to Text (STT) and Text to Speech (TTS) features
---

# Speech to Text (STT) and Text to Speech (TTS)

<Callout type="info" title="Upcoming STT/TTS Enhancements" collapsible>
The Google Cloud STT/TTS and Deepgram services are being planned for future integration.
</Callout>

## Speech Introduction

The Speech Configuration includes settings for both Speech-to-Text (STT) and Text-to-Speech (TTS) under a unified `speech:` section. Additionally, there is a new `speechTab` menu for user-specific settings

## Speech Tab (optional)

The `speechTab` menu provides customizable options for conversation and advanced modes, as well as detailed settings for STT and TTS. This will set the default settings for users

example:

```yaml
speech:
  speechTab:
    conversationMode: true
    advancedMode: false
    speechToText:
      engineSTT: "external"
      languageSTT: "English (US)"
      autoTranscribeAudio: true
      decibelValue: -45
      autoSendText: 0
    textToSpeech:
      engineTTS: "external"
      voice: "alloy"
      languageTTS: "en"
      automaticPlayback: true
      playbackRate: 1.0
      cacheTTS: true
```

## STT (Speech-to-Text)

The Speech-to-Text (STT) feature converts spoken words into written text. To enable STT, click on the STT button (near the send button) or use the key combination ++Ctrl+Alt+L++ to start the transcription.

### Available STT Services

- **Local STT**
  - Browser-based
  - Whisper (tested on LocalAI)
- **Cloud STT**
  - OpenAI Whisper
  - Azure Whisper
  - Other OpenAI-compatible STT services

### Configuring Local STT

- #### Browser-based
  No setup required. Ensure the "Speech To Text" switch in the speech settings tab is enabled and "Browser" is selected in the engine dropdown.

- #### Whisper Local
  Requires a local Whisper instance.

```yaml
speech:
  stt:
    openai:
      url: 'http://host.docker.internal:8080/v1/audio/transcriptions'
      model: 'whisper'
```

### Configuring Cloud STT

- #### OpenAI Whisper

```yaml
speech:
  stt:
    openai:
      apiKey: '${STT_API_KEY}'
      model: 'whisper-1'
```

- #### Azure Whisper

```yaml
speech:
  stt:
    azureOpenAI:
      instanceName: 'instanceName'
      apiKey: '${STT_API_KEY}'
      deploymentName: 'deploymentName'
      apiVersion: 'apiVersion'
```

- #### OpenAI compatible

Refer to the OpenAI Whisper section, adjusting the `url` and `model` as needed.

example
  
```yaml
speech:
  stt:
    openai:
      url: 'http://host.docker.internal:8080/v1/audio/transcriptions'
      model: 'whisper'
  ```


## TTS (Text-to-Speech)

The Text-to-Speech (TTS) feature converts written text into spoken words. Various TTS services are available:

### Available TTS Services

- **Local TTS**
  - Browser-based
  - Piper (tested on LocalAI)
  - Coqui (tested on LocalAI)
- **Cloud TTS**
  - OpenAI TTS
  - Azure OpenAI
  - ElevenLabs
  - Other OpenAI/ElevenLabs-compatible TTS services

### Configuring Local TTS

- #### Browser-based

No setup required. Ensure the "Text To Speech" switcg in the speech settings tab is enabled and "Browser" is selected in the engine dropdown.

- #### Piper

Requires a local Piper instance.

```yaml
speech:
  tts:
    localai:
      url: "http://host.docker.internal:8080/tts"
      apiKey: "EMPTY"
      voices: [
        "en-us-amy-low.onnx",
        "en-us-danny-low.onnx",
        "en-us-libritts-high.onnx",
        "en-us-ryan-high.onnx",
      ]
      backend: "piper"
```

- #### Coqui

Requires a local Coqui instance.

```yaml
speech:
  tts:
    localai:
      url: 'http://localhost:8080/v1/audio/synthesize'
      voices: ['tts_models/en/ljspeech/glow-tts', 'tts_models/en/ljspeech/tacotron2', 'tts_models/en/ljspeech/waveglow']
      backend: 'coqui'
```

### Configuring Cloud TTS

- #### OpenAI TTS

```yaml
speech:
  tts:
    openai:
      apiKey: '${TTS_API_KEY}'
      model: 'tts-1'
      voices: ['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']
```

- #### Azure OpenAI

```yaml
speech:
  tts:
    azureOpenAI:
      instanceName: ''
      apiKey: '${TTS_API_KEY}'
      deploymentName: ''
      apiVersion: ''
      model: 'tts-1'
      voices: ['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']
```

- #### ElevenLabs

```yaml
speech:
  tts:
    elevenlabs:
      apiKey: '${TTS_API_KEY}'
      model: 'eleven_multilingual_v2'
      voices: ['202898wioas09d2', 'addwqr324tesfsf', '3asdasr3qrq44w', 'adsadsa']
```

Additional ElevenLabs-specific parameters can be added as follows:

```yaml
      voice_settings:
        similarity_boost: '' # number
        stability: '' # number
        style: '' # number
        use_speaker_boost: # boolean
      pronunciation_dictionary_locators: [''] # list of strings (array)
```

- #### OpenAI compatible

Refer to the OpenAI TTS section, adjusting the `url` variable as needed

example:

```yaml
speech:
  tts:
    openai:
      url: 'http://host.docker.internal:8080/v1/audio/synthesize'
      apiKey: '${TTS_API_KEY}'
      model: 'tts-1'
      voices: ['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']
```

- #### ElevenLabs compatible

Refer to the ElevenLabs section, adjusting the `url` variable as needed

example:

```yaml
speech:
  tts:
    elevenlabs:
      url: 'http://host.docker.internal:8080/v1/audio/synthesize'
      apiKey: '${TTS_API_KEY}'
      model: 'eleven_multilingual_v2'
      voices: ['202898wioas09d2', 'addwqr324tesfsf', '3asdasr3qrq44w', 'adsadsa']
```



================================================
FILE: pages/docs/configuration/token_usage.mdx
================================================
---
title: Token Usage
description: This covers how to track and control your token usage for the OpenAI/Plugins endpoints in LibreChat. You will learn how to view your transactions, enable user balances, and add credits to your account.
---
# Token Usage

## Intro

As of `v0.6.0`, LibreChat accurately tracks token usage for the OpenAI/Plugins endpoints. All token transactions are stored in the "Transactions" collection in your database. In future releases, you'll be able to toggle viewing how much a conversation costs.

Currently, you can limit user token usage by enabling user balances. Instead of configuring token credit limits via environment variables, you now set these options in your `librechat.yaml` file under the **balance** section.

## Balance Configuration

The balance system in LibreChat allows administrators to configure how token credit balances are managed for users. All balance settings are now managed in your YAML configuration under the `balance` object.

**Note:** This replaces the previous environment variables (`CHECK_BALANCE` and `START_BALANCE`) and provides a more structured way to manage user balances.

### Complete Balance Settings

```yaml filename="librechat.yaml"
version: 1.2.1

# Balance settings
balance:
  enabled: true                # Enable token credit balances for users
  startBalance: 20000          # Initial tokens credited upon registration
  autoRefillEnabled: false     # Enable automatic token refills
  refillIntervalValue: 30      # Numerical value for refill interval
  refillIntervalUnit: "days"   # Time unit for refill interval (days, hours, etc.)
  refillAmount: 10000          # Tokens added during each refill
```

### Balance Settings Explained

- **enabled**: Activates token credit tracking and balance management for users. When set to `true`, the system will track token usage and enforce balance limits.

- **startBalance**: Specifies the initial number of tokens credited to a user upon registration. This is the starting balance for all new users.

- **autoRefillEnabled**: Determines whether automatic refilling of token credits is enabled. When set to `true`, the system will automatically add credits to user balances based on the refill interval.

- **refillIntervalValue**: Specifies the numerical value for the interval at which token credits are automatically refilled. Works in conjunction with `refillIntervalUnit`.

- **refillIntervalUnit**: Specifies the time unit for the refill interval. Supported values include "seconds", "minutes", "hours", "days", "weeks", and "months".

- **refillAmount**: Specifies the number of tokens to be added to the user's balance during each automatic refill.

Check out the [Balance Configuration](/docs/configuration/librechat_yaml/object_structure/balance) page for more details.

## How Auto-Refill Works

When a user's balance is tracked and **autoRefill** is enabled, the system will automatically add credits to the balance only when the specified time interval has passed since the last refill. This is achieved by comparing the current date with the `lastRefill` date plus the specified interval.

### Auto-Refill Process

1. When a user attempts to spend tokens, the system checks if the current balance is sufficient
2. If the balance would drop to zero or below after the transaction, the system checks if auto-refill is enabled
3. If auto-refill is enabled, the system checks if the time interval since the last refill has passed:
   - The system compares the current date with `lastRefill + refillInterval`
   - If the interval has passed, tokens are added to the user's balance
   - The `lastRefill` date is updated to the current date
4. The transaction proceeds if the balance is sufficient (either originally or after refill)

### Supported Time Units

The `refillIntervalUnit` can be set to any of the following values:
- "seconds"
- "minutes"
- "hours"
- "days"
- "weeks"
- "months"

For example, if `refillIntervalValue` is set to 30 and `refillIntervalUnit` is `days`, the system will add `refillAmount` tokens to the user's balance only if 30 days have passed since the last refill.

### Balance Synchronization

When a user logs in, the system automatically synchronizes their balance settings with the current global balance configuration. This ensures that any changes to the balance configuration are applied to all users.

The synchronization process:
1. Checks if the user has a balance record
2. If no record exists, creates one with the current `startBalance`
3. Updates the user's auto-refill settings to match the global configuration
4. Ensures the user's refill interval and amount match the global settings

## Managing Token Balances

You can manually add or set user balances. This is especially useful during development or if you plan to build out a full balance-accruing system in the future (for example, via an admin dashboard).

### Adding Balances

```bash filename="To manually add balances, run the following command:"
# Local Development
npm run add-balance

# Docker (default setup)
docker-compose exec api npm run add-balance

# Docker (deployment setup)
docker exec -it LibreChat-API /bin/sh -c "cd .. && npm run add-balance"
```

```bash filename="You can also specify the email and token credit amount to add, e.g.:"
# Local Development
npm run add-balance danny@librechat.ai 1000

# Docker (default setup)
docker-compose exec api npm run add-balance danny@librechat.ai 1000

# Docker (deployment setup)
docker exec -it LibreChat-API /bin/sh -c "cd .. && npm run add-balance danny@librechat.ai 1000"
```

### Setting Balances

Additionally, you can set a balance for a user. An existing balance will be overwritten by the new balance.

```bash filename="To manually set balances, run the following command:"
# Local Development
npm run set-balance

# Docker (default setup)
docker-compose exec api npm run set-balance

# Docker (deployment setup)
docker exec -it LibreChat-API /bin/sh -c "cd .. && npm run set-balance"
```

```bash filename="You can also specify the email and token credit amount to set, e.g.:"
# Local Development
npm run set-balance danny@librechat.ai 1000

# Docker (default setup)
docker-compose exec api npm run set-balance danny@librechat.ai 1000

# Docker (deployment setup)
docker exec -it LibreChat-API /bin/sh -c "cd .. && npm run set-balance danny@librechat.ai 1000"
```

### Listing of balances

```bash filename="To see the balances of your users, you can run:"
# Local Development
npm run list-balances

# Docker (default setup)
docker-compose exec api npm run list-balances

# Docker (deployment setup)
docker exec -it LibreChat-API /bin/sh -c "cd .. && npm run list-balances"
```

This works well to track your own usage for personal use; 1000 credits = $0.001 (1 mill USD)

## Notes on Token Usage and Balance

- With summarization enabled, you will be blocked from making an API request if the cost of the content that you need to summarize + your messages payload exceeds the current balance
- Counting Prompt tokens is really accurate for OpenAI calls, but not 100% for plugins (due to function calling). It is really close and conservative, meaning its count may be higher by 2-5 tokens.
- The system allows deficits incurred by the completion tokens. It only checks if you have enough for the prompt Tokens, and is pretty lenient with the completion. The graph below details the logic
- The above said, plugins are checked at each generation step, since the process works with multiple API calls. Anything the LLM has generated since the initial user prompt is shared to the user in the error message as seen below.
- There is a 150 token buffer for titling since this is a 2 step process, that averages around 200 total tokens. In the case of insufficient funds, the titling is cancelled before any spend happens and no error is thrown.

![image](https://github.com/danny-avila/LibreChat/assets/110412045/78175053-9c38-44c8-9b56-4b81df61049e)

## More details
source: [LibreChat/discussions/1640](https://github.com/danny-avila/LibreChat/discussions/1640#discussioncomment-8251970)

> "rawAmount": -000, // what's this?

Raw amount of tokens as counted per the tokenizer algorithm.

>    "tokenValue": -00000, // what's this?

Token credits value. 1000 credits = $0.001 (1 mill USD)

> "rate": 00, // what's this?

The rate at which tokens are charged as credits.

For example, gpt-3.5-turbo-1106 has a rate of 1 for user prompt (input) and 2 for completion (output)

| Model                 | Input                | Output               |
|-----------------------|----------------------|----------------------|
| gpt-3.5-turbo-1106    | $0.0010 / 1K tokens  | $0.0020 / 1K tokens  |


Given the provided example:
```sh
    "rawAmount": -137
    "tokenValue": -205.5
    "rate": 1.5
```

```math
\text{Token Value} = (\text{Raw Amount of Tokens}) \times (\text{Rate})
```

```math
137 \times 1.5 = 205.5
```

And to get the real amount of USD spend based on **Token Value**:

```math
\frac{\text{Token Value}}{1,000,000} = \left(\frac{\text{Raw Amount of Tokens} \times \text{Rate}}{1,000,000}\right)
```

```math
\frac{205.5}{1,000,000} = \$0.0002055 \text{ USD}
```

The relevant file for editing rates is found in `api/models/tx.js`

There will be more customization for this soon from the `librechat.yaml` file.

## Preview

![image](https://github.com/danny-avila/LibreChat/assets/110412045/39a1aa5d-f8fc-43bf-81f2-299e57d944bb)

![image](https://github.com/danny-avila/LibreChat/assets/110412045/e1b1cc3f-8981-4c7c-a5f8-e7badbc6f675)

## Additional Notes

- With summarization enabled, API requests are blocked if the cost of the content plus the messages payload exceeds the current balance.
- The system is lenient with completion tokens, focusing primarily on prompt tokens for balance checks.
- A buffer is added for titling (approximately 150 tokens) to account for the two-step process.
- Token credits translate to monetary value (e.g., 1000 credits = $0.001 USD).

For more details and customizations, please refer to the [LibreChat Documentation](https://www.librechat.ai/docs/configuration/librechat_yaml).



================================================
FILE: pages/docs/configuration/authentication/_meta.ts
================================================
export default {
  index: 'Intro',
}



================================================
FILE: pages/docs/configuration/authentication/email.mdx
================================================
---
title: Email setup
description: This guide explains how to configure the secure email verification/password reset. You can configure it to work with various email services, including Gmail and custom mail servers.
---

# Email verification and Password Reset

For a quick overview, refer to the user guide provided here: [Password Reset](/docs/features/password_reset)

## General setup

**Basic Configuration**

If you want to use one of the predefined services, configure only these variables:
For more info about supported email services: https://community.nodemailer.com/2-0-0-beta/setup-smtp/well-known-services/

<OptionTable
  options={[
    ['EMAIL_SERVICE', 'string', 'Email service (e.g., Gmail, Outlook).','EMAIL_SERVICE='],
    ['EMAIL_USERNAME', 'string', 'Username for authentication.','EMAIL_USERNAME='],
    ['EMAIL_PASSWORD', 'string', 'Password for authentication.','EMAIL_PASSWORD='],
    ['EMAIL_FROM_NAME', 'string', 'From name.','EMAIL_FROM_NAME='],
    ['EMAIL_FROM', 'string', 'From email address. Required.','EMAIL_FROM=noreply@librechat.ai'],
  ]}
/>

**Advanced Configuration**

If you want to use a generic SMTP service or need advanced configuration for one of the predefined providers, configure these variables as well:

<OptionTable
  options={[
    ['EMAIL_HOST', 'string', 'Mail server host.','EMAIL_HOST='],
    ['EMAIL_PORT', 'number', 'Mail server port.','EMAIL_PORT=25'],
    ['EMAIL_ENCRYPTION', 'string', 'Encryption method (starttls, tls, etc.).','EMAIL_ENCRYPTION='],
    ['EMAIL_ENCRYPTION_HOSTNAME', 'string', 'Hostname for encryption.','EMAIL_ENCRYPTION_HOSTNAME='],
    ['EMAIL_ALLOW_SELFSIGNED', 'boolean', 'Allow self-signed certificates.','EMAIL_ALLOW_SELFSIGNED='],
  ]}
/>

<Callout type="warning" title="Warning">
**Failing to perform either of the below setups will result in LibreChat using the unsecured password reset! This allows anyone to reset any password on your server immediately, without mail being sent at all!**
</Callout>

## Setup with Gmail

To set up Gmail, follow these steps:

1. Create a Google Account and enable 2-step verification.
2. In the **[Google Account settings](https://myaccount.google.com/)**, click on the "Security" tab and open "2-step verification."
3. Scroll down and open "App passwords." Choose "Mail" for the app and select "Other" for the device, then give it a random name.
4. Click on "Generate" to create a password, and copy the generated password.
5. In the `.env` file, modify the variables as follows:

<OptionTable
  options={[    
    ['EMAIL_SERVICE', 'string', 'gmail', 'EMAIL_SERVICE=gmail'],
    ['EMAIL_USERNAME', 'string', 'your-email', 'EMAIL_USERNAME=your-email'],
    ['EMAIL_PASSWORD', 'string', 'your-email-password', 'EMAIL_PASSWORD=your-email-password'],
    ['EMAIL_FROM', 'string', 'email address for the from field, e.g., noreply@librechat.ai', 'EMAIL_FROM=noreply@librechat.ai'],
    ['EMAIL_FROM_NAME', 'string', 'My LibreChat Server', 'EMAIL_FROM_NAME=LibreChat'],
  ]}
/>

## Setup with custom mail server

To set up a custom mail server, follow these steps:

1. Gather your SMTP login data from your provider. The steps are different for each, but they will usually list values for all variables.
2. In the `.env` file, modify the variables as follows, assuming some sensible example values:

<OptionTable
  options={[    
    ['EMAIL_HOST', 'string', 'Hostname to connect to', 'EMAIL_HOST=mail.example.com'],
    ['EMAIL_PORT', 'integer', 'Port to connect to','EMAIL_PORT=25'],
    ['EMAIL_ENCRYPTION', 'string', 'Encryption type','EMAIL_ENCRYPTION=starttls'],
    ['EMAIL_USERNAME', 'string', 'Your email username','EMAIL_USERNAME=usernale@example.com'],
    ['EMAIL_PASSWORD', 'string', 'Your app password','EMAIL_PASSWORD=password'],
    ['EMAIL_FROM', 'string', 'Email address for the from field','EMAIL_FROM=noreply@librechat.ai'],
    ['EMAIL_FROM_NAME', 'string', 'Name that will appear in the "from" field','EMAIL_FROM_NAME=LibreChat'],
  ]}
/>



================================================
FILE: pages/docs/configuration/authentication/index.mdx
================================================
---
title: Authentication System
description: This guide explains how to use the user authentication system of LibreChat, which offers secure and easy email and social logins. You will learn how to set up sign up, log in, password reset, and more.
--- 

# Basic Configuration:

## General

For a quick overview, refer to the user guide provided here: [Authentication](/docs/features/authentication)

Here's an overview of the general configuration.

<OptionTable
  options={[
    ['ALLOW_EMAIL_LOGIN', 'boolean', 'Enable or disable ONLY email login.','ALLOW_EMAIL_LOGIN=true'],
    ['ALLOW_REGISTRATION', 'boolean', 'Enable or disable Email registration of new users.','ALLOW_REGISTRATION=true'],
    ['ALLOW_SOCIAL_LOGIN', 'boolean', 'Allow users to connect to LibreChat with various social networks.','ALLOW_SOCIAL_LOGIN=false'],
    ['ALLOW_SOCIAL_REGISTRATION', 'boolean', 'Enable or disable registration of new users using various social networks.','ALLOW_SOCIAL_REGISTRATION=false'],
  ]}
/>

> **Note:** OpenID does not support the ability to disable only registration.

Quick Tips:

- Even with registration disabled, you can add users directly to the database using [the create-user script](#create-user-script) detailed below.
- To delete a user, you can use [the delete-user script](#delete-user-script) also detailed below.



<div style={{display: "flex", justifyContent: "center", alignItems: "center", flexDirection: "column"}}>
  <div className="image-light-theme">
    ![register-light](https://github.com/danny-avila/LibreChat/assets/32828263/4c51dc25-31d3-4c51-8c2a-0cdfb5a25033)
  </div>

  <div className="image-dark-theme">
    ![register](https://github.com/danny-avila/LibreChat/assets/32828263/3bc5371d-e51d-4e91-ac68-56db6e85bb2c)
  </div>
</div>


## Session Expiry and Refresh Token

- Default values: session expiry: 15 minutes, refresh token expiry: 7 days
  - For more information: **[GitHub PR #927 - Refresh Token](https://github.com/danny-avila/LibreChat/pull/927)**

<OptionTable
  options={[
    ['SESSION_EXPIRY', 'integer (milliseconds)', 'Session expiry time.','SESSION_EXPIRY=1000 * 60 * 15'],
    ['REFRESH_TOKEN_EXPIRY', 'integer (milliseconds)', 'Refresh token expiry time.','REFRESH_TOKEN_EXPIRY=(1000 * 60 * 60 * 24) * 7'],
  ]}
/>

``` mermaid
sequenceDiagram
    Client->>Server: Login request with credentials
    Server->>Passport: Use authentication strategy (e.g., 'local', 'google', etc.)
    Passport-->>Server: User object or false/error
    Note over Server: If valid user...
    Server->>Server: Generate access and refresh tokens
    Server->>Database: Store hashed refresh token
    Server-->>Client: Access token and refresh token
    Client->>Client: Store access token in HTTP Header and refresh token in HttpOnly cookie
    Client->>Server: Request with access token from HTTP Header
    Server-->>Client: Requested data
    Note over Client,Server: Access token expires
    Client->>Server: Request with expired access token
    Server-->>Client: Unauthorized
    Client->>Server: Request with refresh token from HttpOnly cookie
    Server->>Database: Retrieve hashed refresh token
    Server->>Server: Compare hash of provided refresh token with stored hash
    Note over Server: If hashes match...
    Server-->>Client: New access token and refresh token
    Client->>Server: Retry request with new access token
    Server-->>Client: Requested data
```

## JWT Secret and Refresh Secret

- You should use new secure values. The examples given are 32-byte keys (64 characters in hex). 
  - Use this tool to generate some quickly: **[JWT Keys](/toolkit/creds_generator)**

<OptionTable
  options={[
    ['JWT_SECRET', 'string (hex)', 'JWT secret key.','JWT_SECRET=16f8c0ef4a5d391b26034086c628469d3f9f497f08163ab9b40137092f2909ef'],
    ['JWT_REFRESH_SECRET', 'string (hex)', 'JWT refresh secret key.','JWT_REFRESH_SECRET=eaa5191f2914e30b9387fd84e254e4ba6fc51b4654968a9b0803b456a54b8418'],
  ]}
/>

---

## Automated Moderation System (optional)

The Automated Moderation System is enabled by default. It uses a scoring mechanism to track user violations. As users commit actions like excessive logins, registrations, or messaging, they accumulate violation scores. Upon reaching a set threshold, the user and their IP are temporarily banned. This system ensures platform security by monitoring and penalizing rapid or suspicious activities.

To set up the mod system, review [the setup guide](/docs/configuration/mod_system).

> *Please Note: If you want this to work in development mode, you will need to create a file called `.env.development` in the root directory and set `DOMAIN_CLIENT` to `http://localhost:3090` or whatever port  is provided by vite when runnning `npm run frontend-dev`*

## User Management Scripts

### Create User Script

The create-user script allows you to add users directly to the database, even when registration is disabled. Here's how to use it:

1. For the default `docker-compose.yml` (if you use `docker compose up` to start the app):
   ```
   docker-compose exec api npm run create-user
   ```

2. For the `deploy-compose.yml` (if you followed the [Ubuntu Docker Guide](/docs/remote/docker_linux)):
   ```
   docker exec -it LibreChat-API /bin/sh -c "cd .. && npm run create-user"
   ```

3. For local development (from project root):
   ```
   npm run create-user
   ```

Follow the prompts to enter the new user's email and password.

### Delete User Script

To delete a user, you can use the delete-user script:

1. For the default `docker-compose.yml` (if you use `docker compose up` to start the app):
   ```
   docker-compose exec api npm run delete-user email@domain.com
   ```

2. For the `deploy-compose.yml` (if you followed the [Ubuntu Docker Guide](/docs/remote/docker_linux)):
   ```
   docker exec -it LibreChat-API /bin/sh -c "cd .. && npm run delete-user email@domain.com"
   ```

3. For local development (from project root):
   ```
   npm run delete-user email@domain.com
   ```

Replace `email@domain.com` with the email of the user you want to delete.




================================================
FILE: pages/docs/configuration/authentication/ldap.mdx
================================================
---
title: LDAP/AD
description: Learn how to configure LibreChat to use LDAP for user authentication.
---

# LDAP/AD Server Authentication

You can use a Lightweight Directory Access Protocol (LDAP) authentication server to authenticate users.

## LDAP/AD Server Configuration

**Basic Configuration**

- `LDAP_URL` and `LDAP_USER_SEARCH_BASE` are required.
- `LDAP_SEARCH_FILTER` is optional; if not specified, the `mail` attribute is used by default. If specified, use the literal `{{username}}` to use the given username for the search.

<OptionTable
  options={[
    ['LDAP_URL', 'string', 'LDAP server URL.', 'LDAP_URL=ldap://localhost:389'],
    ['LDAP_BIND_DN', 'string', 'Bind DN', 'LDAP_BIND_DN=cn=root'],
    ['LDAP_BIND_CREDENTIALS', 'string', 'Password for bindDN', 'LDAP_BIND_CREDENTIALS=password'],
    [
      'LDAP_USER_SEARCH_BASE',
      'string',
      'LDAP user search base',
      'LDAP_USER_SEARCH_BASE=o=users,o=example.com',
    ],
    ['LDAP_SEARCH_FILTER', 'string', 'LDAP search filter', 'LDAP_SEARCH_FILTER=mail={{username}}'],
  ]}
/>

**Field Mappings**

You can specify a mapping between the attributes of LibreChat users and those of LDAP users. Use these settings if the default mappings do not work properly.

<OptionTable
  options={[
    [
      'LDAP_ID',
      'string',
      'Specify a unique user ID. By default, uid or sAMAccountName, mail is used.',
      'LDAP_ID=uid',
    ],
    [
      'LDAP_USERNAME',
      'string',
      'By default, it uses givenName or mail.',
      'LDAP_USERNAME=givenName',
    ],
    [
      'LDAP_EMAIL',
      'string',
      'By default, it uses mail.',
      'LDAP_EMAIL=userPrincipalName',
    ],
    [
      'LDAP_FULL_NAME',
      'string',
      'By default, it uses a combination of givenName and surname.',
      'LDAP_FULL_NAME=givenName,surname',
    ],
  ]}
/>

**Username or Email**

By default, LibreChat uses an email address and password for authentication.
This may sometimes cause problem with LDAP and you may want to use a username instead.
Set the `LDAP_SEARCH_FILTER` to filter for the username instead (e.g. `LDAP_SEARCH_FILTER=uid={{username}}`
and configure LibreChat to request login via username:

<OptionTable
  options={[
    [
      'LDAP_LOGIN_USES_USERNAME',
      'string',
      'Use username instead of email.',
      'LDAP_LOGIN_USES_USERNAME=true',
    ],
  ]}
/>

**Active Directory over SSL**

To connect via SSL (ldaps://), such as a company using Windows AD, specify the path to the internal CA certificate.
`LDAP_TLS_REJECT_UNAUTHORIZED` is optional;if not specified LibreChat will reject TLS/SSL connections if the LDAP server's certificate cannot be verified.
set `LDAP_TLS_REJECT_UNAUTHORIZED` to false (not recommended for production environments)
to allow Librechat to accept TLS/SSL connections even if the LDAP server's certificate cannot be verified,

<OptionTable
  options={[
    [
      'LDAP_CA_CERT_PATH',
      'string',
      'CA certificate path.',
      'LDAP_CA_CERT_PATH=/path/to/root_ca_cert.crt',
    ],
    [
      'LDAP_TLS_REJECT_UNAUTHORIZED',
      'string',
      'Disable TLS verification',
      'LDAP_TLS_REJECT_UNAUTHORIZED=true',
    ],
  ]}
/>

**LDAP StartTLS**

Enabling LDAP StartTLS allows LibreChat to upgrade an insecure connection to a secure TLS connection. This is useful if you want to secure the connection without switching to ldaps://.

<OptionTable
    options={[
        [
            'LDAP_STARTTLS',
            'string',
            'Enable LDAP StartTLS for upgrading the connection to TLS. Set to true to enable this feature.',
            'LDAP_STARTTLS=true',
        ],
    ]}
/>



================================================
FILE: pages/docs/configuration/authentication/OAuth2-OIDC/_meta.ts
================================================
export default {
  index: 'Intro',
  OAuth2: {
    type: 'separator',
  },
  apple: 'Apple',
  discord: 'Discord',
  facebook: 'Facebook',
  github: 'GitHub',
  google: 'Google',
  OIDC: {
    type: 'separator',
  },
  aws: 'AWS Cognito',
  azure: 'Azure Entra/AD',
  keycloak: 'Keycloak',
}



================================================
FILE: pages/docs/configuration/authentication/OAuth2-OIDC/apple.mdx
================================================
---
title: Apple
description: Learn how to configure LibreChat to use Apple for user authentication.
---

# Apple


## Prerequisites

Before you begin, ensure you have the following:

- **Apple Developer Account:** If you don't have one, enroll [here](https://developer.apple.com/programs/enroll/).

---

## Creating a New App ID

### 1. Log in to the Apple Developer Console

- **Action:**
- Visit [Apple Developer](https://developer.apple.com/) and sign in with your Apple ID.


### 2. Navigate to Identifiers

- Go to **Certificates, Identifiers & Profiles**.
- Click on **Identifiers** in the sidebar.


### 3. Create a New App ID

1. Click the **"+"** button to add a new identifier.
2. Select **App IDs** and click **Continue**.
3. Choose **App** and click **Continue**.
4. Enter a **Description** for your App ID (e.g., `LibreChat App ID`).
5. Set the **Bundle ID** (e.g., `com.yourdomain.librechat`).
6. Click **Continue** and then **Register**.

- **Image References:**
- ![Create App ID](https://user-images.githubusercontent.com/5569219/59017558-6d643600-8861-11e9-927b-a4952b56f34e.png)
*Figure 1: Creating a New App ID*

- ![Select App](https://github.com/user-attachments/assets/7b67c1bb-dea0-4475-ad45-e3c13ad514d5)
*Figure 2: Selecting App Identifier*

### 4. Enable "Sign in with Apple"

1. After creating the App ID, click on it to edit.
2. Under **Capabilities**, find and check **Sign in with Apple**.
3. Click **Save**.

- **Image Reference:**
- ![Enable Sign in with Apple](https://user-images.githubusercontent.com/5569219/59017720-dea3e900-8861-11e9-898e-f486c093edd8.png)
*Figure 3: Enabling "Sign in with Apple"*

---

## Creating a Services ID

### 1. Navigate to Identifiers

- In the **Certificates, Identifiers & Profiles** section, click on **Identifiers**.

### 2. Create a New Services ID

1. Click the **"+"** button.
2. Select **Services IDs** and click **Continue**.
3. Enter a **Description** (e.g., `LibreChat Services ID`).
4. Enter an **Identifier** (e.g., `com.yourdomain.librechat.services`).
5. Click **Continue** and then **Register**.

- **Image References:**
- ![Select Services ID](https://user-images.githubusercontent.com/5569219/59017808-16ab2c00-8862-11e9-8beb-4da7bb509b0c.png)
*Figure 4: Selecting Services ID*

- ![Create Services ID](https://github.com/user-attachments/assets/cac99e43-a6d7-4fb8-890d-eabd87a60e7d)
*Figure 5: Creating Services ID*

### 3. Configure "Sign in with Apple"

1. Click on the newly created Services ID.
2. Under **Capabilities**, click **Configure** next to **Sign in with Apple**.
3. Enter your **Domains** (e.g., `your-domain.com`) and **Return URLs** (e.g., `https://your-domain.com/oauth/apple/callback`).
4. Click **Next** and then **Register**.

- **Image Reference:**
- ![Configure Sign in with Apple](https://github.com/user-attachments/assets/9309e1c1-6f98-49fc-a87d-bb5f46e200f7)
*Figure 6: Configuring "Sign in with Apple" for Services ID*

- ![Web Authentication Configuration](https://github.com/user-attachments/assets/d1ca9ad2-e555-4083-a974-b26239c9694f)
*Figure 7: Web Authentication Configuration*

- ![Web Authentication Configuration](https://github.com/user-attachments/assets/8facac9e-002f-458b-8049-63c91afc30de)
*Figure 8: Save edit Services ID Configuration*


---

## Creating a Key

### 1. Navigate to Keys

- In the **Certificates, Identifiers & Profiles** section, click on **Keys**.

### 2. Create a New Key

1. Click the **"+"** button to add a new key.
2. Enter a **Key Name** (e.g., `LibreChatSignInWithApple`).
3. Select **Sign in with Apple** under **Capabilities**.
4. Click **Configure** and select the created App ID (e.g., `com.yourdomain.librechat`), then click **Save**.
5. Click **Continue** and then **Register**.

- **Image References:**
- ![Create Key](https://github.com/user-attachments/assets/6db095dd-79dd-485d-a57a-8b1ea523b67f)
*Figure 8: Creating a New Key*

- ![Configure Key](https://github.com/user-attachments/assets/30b593ab-b8f3-4d94-b56d-1eeac1900a1f)
*Figure 9: Configuring the Key with App ID*

- ![Register a New Key](https://github.com/user-attachments/assets/91aa4d6e-9b5c-4f7c-bb1a-5980015ee15d)
*Figure 10: Registering the Key*

### 3. Download the Private Key

1. After creating the key, click **Download**.
2. **Important:** Save the `.p8` file securely. You will not be able to download it again.
3. Note the **Key ID**; you'll need it for the `.env` file.

- **Image Reference:**
- ![Download Your Key](https://github.com/user-attachments/assets/79e5aefa-9797-4fa7-bdf9-cb4b526ab3dd)
*Figure 11: Downloading the Private Key*

---

## Configuring LibreChat

### 1. Update `.env` Configuration

Add the following Apple OAuth2 configuration to your `.env` file:

```env filename=".env"
DOMAIN_CLIENT=https://your-domain.com # use http://localhost:3080 if not using a custom domain
DOMAIN_SERVER=https://your-domain.com # use http://localhost:3080 if not using a custom domain

# Apple
APPLE_CLIENT_ID=com.yourdomain.librechat.services
APPLE_TEAM_ID=YOUR_TEAM_ID
APPLE_KEY_ID=YOUR_KEY_ID
APPLE_PRIVATE_KEY_PATH=/path/to/AuthKey.p8 # Absolute path to your downloaded .p8 file
APPLE_CALLBACK_URL=/oauth/apple/callback
```

> **Note:** 
> - Replace `com.yourdomain.librechat.services` with your actual Services ID.
> - Replace `YOUR_TEAM_ID` and `YOUR_KEY_ID` with the respective values from your Apple Developer account.
> - If using Docker, ensure the `.p8` file is accessible within your Docker container and update the `APPLE_PRIVATE_KEY_PATH` accordingly.

### 2. Restart LibreChat

After updating the `.env` file, restart LibreChat to apply the changes.

- **If using Docker:**

```bash
docker compose up -d
  ```

---

## Troubleshooting

If you encounter issues during the setup, consider the following solutions:

- **Invalid Redirect URI:**
    - Ensure that the redirect URI in your Apple Developer Console (`https://your-domain.com/oauth/apple/callback`) matches exactly with the one specified in your `.env` file (`APPLE_CALLBACK_URL`).

- **Private Key Issues:**
    - Verify that the path to your `.p8` file (`APPLE_PRIVATE_KEY_PATH`) is correct.
    - Ensure that LibreChat has read permissions for the `.p8` file.

- **Team ID and Key ID Errors:**
    - Double-check that the `APPLE_TEAM_ID` and `APPLE_KEY_ID` in your `.env` file match those in your Apple Developer Account.

- **Domain Verification Failed:**
    - Ensure that the verification file is correctly uploaded to the root of your domain.
    - Verify that there are no typos in the domain name entered during configuration.

- **Docker Configuration Issues:**
    - If using Docker, confirm that the `.p8` file is properly mounted and the path in `APPLE_PRIVATE_KEY_PATH` is accessible within the container.

- **Check Logs:**
    - Review LibreChat logs for any error messages related to Apple authentication. This can provide specific insights into what might be going wrong.



================================================
FILE: pages/docs/configuration/authentication/OAuth2-OIDC/authelia.mdx
================================================
---
title: Authelia
description: Learn how to configure LibreChat to use Authelia for user authentication.
---

# Authelia

- Generate a client secret using:
  ```
  docker run --rm authelia/authelia:latest authelia crypto hash generate pbkdf2 --variant sha512 --random --random.length 72 --random.charset rfc3986
  ```
- Then in your `configuration.yml` add the following in the oidc section:
  ```bash filename="configuration.yml"
    - client_id: 'librechat'
      client_name: 'LibreChat'
      client_secret: '$pbkdf2-GENERATED_SECRET_KEY_HERE'
      public: false
      authorization_policy: 'two_factor'
      redirect_uris:
        - 'https://LIBRECHAT.URL/oauth/openid/callback'
      scopes:
        - 'openid'
        - 'profile'
        - 'email'
      userinfo_signing_algorithm: 'none'
  ```
- Then restart Authelia

# LibreChat

- Open the `.env` file in your project folder and add the following variables:
  ```bash filename=".env"
  ALLOW_SOCIAL_LOGIN=true
  OPENID_BUTTON_LABEL='Log in with Authelia'
  OPENID_ISSUER=https://auth.example.com/.well-known/openid-configuration
  OPENID_CLIENT_ID=librechat
  OPENID_CLIENT_SECRET=ACTUAL_GENERATED_SECRET_HERE
  OPENID_SESSION_SECRET=ANY_RANDOM_STRING
  OPENID_CALLBACK_URL=/oauth/openid/callback
  OPENID_SCOPE="openid profile email"
  OPENID_IMAGE_URL=https://www.authelia.com/images/branding/logo-cropped.png
  # Optional: redirects the user to the end session endpoint after logging out
  OPENID_USE_END_SESSION_ENDPOINT=true 
  ```



================================================
FILE: pages/docs/configuration/authentication/OAuth2-OIDC/authentik.mdx
================================================
---
title: Authentik
description: Learn how to configure LibreChat to use Authentik for user authentication.
---

# Authentik

1. **Access Authentik Admin Interface:**
- Open the Authentik Admin Interface in your browser. Can be found at a URL such as: `https://authentik.example.com/if/admin/#/administration/overview`.
> We will use `https://authentik.example.com` as an example URL. Replace this with the URL of your Authentik instance.

2. **Create a new Application and Provider using the wizard:**
- Click on the Applications tab in the left sidebar and click on Applications again.
- At the top of the page you should see a button that says `Create with Wizard`. Click on it.
> Note: You can also create an application and provider manually just be sure to link them afterwards.
- You can name the application whatever you want. For this example, we will name it `LibreChat` and click next.
- Choose the `OAuth2/OIDC` provider and click next.
- Choose your authentication and authorization flows.
- Scroll down and take note of the `Client ID` and `Client Secret`. You will need these later.
- Under Advanced protocol settings change Subject mode to `Based on the User's Email`.
- Click Submit.
- Add the new application you created to an Outpost.
> Note: You should also apply any policies for access control that you want to apply to LibreChat at this point.

3. **Gather Information for .env:**
- You will need the following information from Authentik:
  - `Client ID`
  - `Client Secret`
  - `OpenID Configuration URL`
  > All of these can be found by clicking on the provider you just created.

3. **Configure LibreChat:**
- Open the `.env` file and add the following variables:
```bash filename=".env"
OPENID_ISSUER=https://authentik.example.com/application/o/librechat/.well-known/openid-configuration
OPENID_CLIENT_ID=[YourClientID]
OPENID_CLIENT_SECRET=[YourClientSecret]
OPENID_SESSION_SECRET=[JustGenerateARandomSessionSecret]
OPENID_CALLBACK_URL=/oauth/openid/callback
OPENID_SCOPE=openid profile email
# Optional customization below
OPENID_BUTTON_LABEL=Login with Authentik
OPENID_IMAGE_URL=https://cdn.jsdelivr.net/gh/selfhst/icons/png/authentik.png
# Redirects the user to the end session endpoint after logging out
OPENID_USE_END_SESSION_ENDPOINT=true 
```
> Note: Make sure nothing is wrapped in quotes in your .env and you have allowed social login.

4. **Check Configuration:**
- Restart LibreChat to apply the changes.
- Open an Icognito window and navigate to your LibreChat instance.
- Underneath the form login there should be a new button that says `Login with Authentik`.
- You should be redirected to Authentik to login.
- After logging in you should be redirected back to LibreChat and be logged in.
    - If you are not redirected back to LibreChat, check Authentik logs for any errors.


================================================
FILE: pages/docs/configuration/authentication/OAuth2-OIDC/aws.mdx
================================================
---
title: AWS Cognito
description: Learn how to configure LibreChat to use AWS Cognito for user authentication.
---

# AWS Cognito

## Create a new User Pool in Cognito

- Visit: **[https://console.aws.amazon.com/cognito/](https://console.aws.amazon.com/cognito/)**
- Sign in as Root User
- Click on `Create user pool`

![image](https://github.com/danny-avila/LibreChat/assets/32828263/e9b412c3-2cf1-4f54-998c-d1d6c12581a5)

## Configure sign-in experience

Your Cognito user pool sign-in options should include `User Name` and `Email`.

![image](https://github.com/danny-avila/LibreChat/assets/32828263/d2cf362d-469e-4993-8466-10282da114c2)

## Configure Security Requirements

You can configure the password requirements now if you desire

![image](https://github.com/danny-avila/LibreChat/assets/32828263/e125e8f1-961b-4a38-a6b7-ed1faf29c4a3)

## Configure sign-up experience

Choose the attributes required at signup. The minimum required is `name`. If you want to require users to use their full name at sign up use: `given_name` and `family_name` as required attributes.

![image](https://github.com/danny-avila/LibreChat/assets/32828263/558b8e2c-afbd-4dd1-87f3-c409463b5f7c)

## Configure message delivery

Send email with Cognito can be used for free for up to 50 emails a day

![image](https://github.com/danny-avila/LibreChat/assets/32828263/fcb2323b-708e-488c-9420-7eb482974648)

## Integrate your app

Select `Use Cognitio Hosted UI` and chose a domain name

![image](https://github.com/danny-avila/LibreChat/assets/32828263/111b3dd4-3b20-4e3e-80e1-7167d2ad0f62)

Set the app type to `Confidential client`
Make sure `Generate a client secret` is set.
Set the `Allowed callback URLs` to `https://YOUR_DOMAIN/oauth/openid/callback`

![image](https://github.com/danny-avila/LibreChat/assets/32828263/1f92a532-7c4d-4632-a55d-9d00bf77fc4d)

Under `Advanced app client settings` make sure `Profile` is included in the `OpenID Connect scopes` (in the bottom)

![image](https://github.com/danny-avila/LibreChat/assets/32828263/5b035eae-4a8e-482c-abd5-29cee6502eeb)

## Review and create
You can now make last minute changes, click on `Create user pool` when you're done reviewing the configuration

![image](https://github.com/danny-avila/LibreChat/assets/32828263/dc8b2374-9adb-4065-85dc-a087d625372d)

![image](https://github.com/danny-avila/LibreChat/assets/32828263/67efb1e9-dfe3-4ebd-9ebb-92186c514b5c)

![image](https://github.com/danny-avila/LibreChat/assets/32828263/9f819175-ace1-44b1-ba68-af21ac9f6735)

![image](https://github.com/danny-avila/LibreChat/assets/32828263/3e7b8b17-4e12-49af-99cf-78981d6331df)

## Get your environment variables

1. Open your User Pool

![image](https://github.com/danny-avila/LibreChat/assets/32828263/b658ff2a-d252-4f3d-90a7-9fbde42c01db)

2. The `User Pool ID` and your AWS region will be used to construct the `OPENID_ISSUER` (see below)

![image](https://github.com/danny-avila/LibreChat/assets/32828263/dc8ae403-cbff-4aae-9eee-42d7cf3485e7)
![image](https://github.com/danny-avila/LibreChat/assets/32828263/d606f5c8-c60b-4d20-bdb2-d0d69e49ea1e)

3.  Go to the `App Integrations` tab

![image](https://github.com/danny-avila/LibreChat/assets/32828263/58713bdc-24bc-47de-bdca-020dc321e997)

4.  Open the app client

![image](https://github.com/danny-avila/LibreChat/assets/32828263/271bf7d2-3df2-43a7-87fc-e50294e49b2e)

5. Toggle `Show Client Secret`

![image](https://github.com/danny-avila/LibreChat/assets/32828263/a844fe65-313d-4754-81b4-380336e0e336)

- Use the `Client ID` for `OPENID_CLIENT_ID`

- Use the `Client secret` for `OPENID_CLIENT_SECRET`

- Generate a random string for the `OPENID_SESSION_SECRET`

> The `OPENID_SCOPE` and `OPENID_CALLBACK_URL` are pre-configured with the correct values

6. Open the `.env` file at the root of your LibreChat folder and add the following variables with the values you copied:

```bash filename=".env"
DOMAIN_CLIENT=https://your-domain.com # use http://localhost:3080 if not using a custom domain
DOMAIN_SERVER=https://your-domain.com # use http://localhost:3080 if not using a custom domain

OPENID_CLIENT_ID=Your client ID
OPENID_CLIENT_SECRET=Your client secret
OPENID_ISSUER=https://cognito-idp.[AWS REGION].amazonaws.com/[USER POOL ID]/.well-known/openid-configuration
OPENID_SESSION_SECRET=Any random string
OPENID_SCOPE=openid profile email
OPENID_CALLBACK_URL=/oauth/openid/callback

# Optional: redirects the user to the end session endpoint after logging out
OPENID_USE_END_SESSION_ENDPOINT=true 
```
7. Save the .env file

> Note: If using docker, run `docker compose up -d` to apply the .env configuration changes



================================================
FILE: pages/docs/configuration/authentication/OAuth2-OIDC/azure.mdx
================================================
---
title: Azure Entra
description: Learn how to configure LibreChat to use Azure Entra for user authentication.
---

# OpenID with Azure Entra

1. Go to the [Azure Portal](https://portal.azure.com/) and sign in with your account.
2. In the search box, type "Azure Entra" and click on it.
3. On the left menu, click on App registrations and then on New registration.
4. Give your app a name and select Web as the platform type.
5. In the Redirect URI field, enter `http://localhost:3080/oauth/openid/callback` and click on Register.

![image](https://github.com/danny-avila/LibreChat/assets/6623884/2b1aabce-850e-4165-bf76-3c1984f10b6c)

6. You will see an Overview page with some information about your app. Copy the Application (client) ID and the 
Directory (tenant) ID and save them somewhere.

![image](https://github.com/danny-avila/LibreChat/assets/6623884/e67d5e97-e26d-48a5-aa6e-50de4450b1fd)

7. On the left menu, click on Authentication and check the boxes for Access tokens and ID tokens under Implicit 
grant and hybrid flows.

![image](https://github.com/danny-avila/LibreChat/assets/6623884/88a16cbc-ff68-4b3a-ba7b-b380cc3d2366)

8. On the left menu, click on Certificates & Secrets and then on New client secret. Give your secret a 
name and an expiration date and click on Add. You will see a Value column with your secret. Copy it and 
save it somewhere. Don't share it with anyone!

![image](https://github.com/danny-avila/LibreChat/assets/6623884/31aa6cee-5402-4ce0-a950-1b7e147aafc8)

9. If you want to restrict access by groups you should add the groups claim to the token. To do this, go to
Token configuration and click on Add group claim. Select the groups you want to include in the token and click on Add.

![image](https://github.com/danny-avila/LibreChat/assets/6623884/c9d353f5-2cb2-4f00-b4f0-493cfec8fe9a)

10. Open the .env file in your project folder and add the following variables with the values you copied:

```bash filename=".env"
DOMAIN_CLIENT=https://your-domain.com # use http://localhost:3080 if not using a custom domain
DOMAIN_SERVER=https://your-domain.com # use http://localhost:3080 if not using a custom domain

# enable social login or else OpenID button will not appear on login page
ALLOW_SOCIAL_LOGIN=true

OPENID_CLIENT_ID=Your Application (client) ID
OPENID_CLIENT_SECRET=Your client secret
OPENID_ISSUER=https://login.microsoftonline.com/Your Directory (tenant ID)/v2.0/
OPENID_SESSION_SECRET=Any random string
OPENID_SCOPE=openid profile email #DO NOT CHANGE THIS
OPENID_CALLBACK_URL=/oauth/openid/callback # this should be the same for everyone

OPENID_REQUIRED_ROLE_TOKEN_KIND=id

# If you want to restrict access by groups
OPENID_REQUIRED_ROLE_PARAMETER_PATH="roles"
OPENID_REQUIRED_ROLE="Your Group Name"

# Optional: redirects the user to the end session endpoint after logging out
OPENID_USE_END_SESSION_ENDPOINT=true 
```
11. Save the .env file

> Note: If using docker, run `docker compose up -d` to apply the .env configuration changes




================================================
FILE: pages/docs/configuration/authentication/OAuth2-OIDC/discord.mdx
================================================
---
title: Discord
description: Learn how to configure LibreChat to use Discord for user authentication.
---

# Discord

## Create a new Discord Application

- Go to **[Discord Developer Portal](https://discord.com/developers)**

- Create a new Application and give it a name

![image](https://github.com/danny-avila/LibreChat/assets/32828263/7e7cdfa0-d1d6-4b6b-a8a9-905aaa40d135)

## Discord Application Configuration

- In the OAuth2 general settings add a valid redirect URL:
    - Example for localhost: `http://localhost:3080/oauth/discord/callback`
    - Example for a domain: `https://example.com/oauth/discord/callback`

![image](https://github.com/danny-avila/LibreChat/assets/32828263/6c56fb92-f4ab-43b9-981b-f98babeeb19d)

- In `Default Authorization Link`, select `In-app Authorization` and set the scopes to `applications.commands`

![image](https://github.com/danny-avila/LibreChat/assets/32828263/2ce94670-9422-48d2-97e9-ec40bd331573)

- Save changes and reset the Client Secret

![image](https://github.com/danny-avila/LibreChat/assets/32828263/3af164fc-66ed-4e5e-9f5a-9bcab3df37b4)
![image](https://github.com/danny-avila/LibreChat/assets/32828263/2ece3935-68e6-4f2e-8656-9721cba5388a)

## .env Configuration

- Paste your `Client ID` and `Client Secret` in the `.env` file:

```bash filename=".env"
DOMAIN_CLIENT=https://your-domain.com # use http://localhost:3080 if not using a custom domain
DOMAIN_SERVER=https://your-domain.com # use http://localhost:3080 if not using a custom domain

DISCORD_CLIENT_ID=your_client_id
DISCORD_CLIENT_SECRET=your_client_secret
DISCORD_CALLBACK_URL=/oauth/discord/callback
```

- Save the `.env` file

> Note: If using docker, run `docker compose up -d` to apply the .env configuration changes



================================================
FILE: pages/docs/configuration/authentication/OAuth2-OIDC/facebook.mdx
================================================
---
title: Facebook
description: Learn how to configure LibreChat to use Facebook for user authentication.
---

# Facebook - WIP

> ⚠️ **Warning: Work in progress, not currently functional**

> ❗ Note: Facebook Authentication will not work from `localhost`

## Create a Facebook Application

- Go to the **[Facebook Developer Portal](https://developers.facebook.com/)**

- Click on "My Apps" in the header menu

![image](https://github.com/danny-avila/LibreChat/assets/32828263/b75ccb8b-d56b-41b7-8b0d-a32c2e762962)

- Create a new application

![image](https://github.com/danny-avila/LibreChat/assets/32828263/706f050d-5423-44cc-80f0-120913695d8f)

- Select "Authenticate and request data from users with Facebook Login"

![image](https://github.com/danny-avila/LibreChat/assets/32828263/2ebbb571-afe8-429e-ab39-be6e83d12c01)

- Choose "No, I'm not creating a game"

![image](https://github.com/danny-avila/LibreChat/assets/32828263/88b5160a-9c72-414a-bbcc-7717b81106f3)

- Provide an `app name` and `App contact email` and click `Create app`

![image](https://github.com/danny-avila/LibreChat/assets/32828263/e1282c9e-4e7d-4cbe-82c9-cc76967f83e1)

## Facebook Application Configuration

- In the side menu, select "Use cases" and click "Customize" under "Authentication and account creation."

![image](https://github.com/danny-avila/LibreChat/assets/32828263/39f4bb70-d9dc-4d1c-8443-2666fe56499b)

-  Add the `email permission`

![image](https://github.com/danny-avila/LibreChat/assets/32828263/dfa20879-2cb8-4daf-883d-3790854afca0)

- Now click `Go to settings`

![image](https://github.com/danny-avila/LibreChat/assets/32828263/512213a2-bd8b-4fd3-96c7-0de6d3222ddd)

- Ensure that `Client OAuth login`, `Web OAuth login` and `Enforce HTTPS` are **enabled**.

![image](https://github.com/danny-avila/LibreChat/assets/32828263/3a7d935b-97bf-493b-b909-39ecf9b3432b)

- Add a `Valid OAuth Redirect URIs` and "Save changes"
    - Example for a domain: `https://example.com/oauth/facebook/callback`

![image](https://github.com/danny-avila/LibreChat/assets/32828263/ef8e54ee-a766-4871-9719-d4eff7a770b6)

- Click `Go back` and select `Basic` in the `App settings` tab

![image](https://github.com/danny-avila/LibreChat/assets/32828263/0d14f702-5183-422e-a12c-5d1b6031581b)

- Click "Show" next to the App secret.

![image](https://github.com/danny-avila/LibreChat/assets/32828263/9a009e37-2bb6-4da6-b5c7-9139c3db6185)

## .env Configuration

- Copy the `App ID` and `App Secret` and paste them into the `.env` file as follows:

```bash filename=".env"
DOMAIN_CLIENT=https://your-domain.com # use http://localhost:3080 if not using a custom domain
DOMAIN_SERVER=https://your-domain.com # use http://localhost:3080 if not using a custom domain

FACEBOOK_CLIENT_ID=your_app_id
FACEBOOK_CLIENT_SECRET=your_app_secret
FACEBOOK_CALLBACK_URL=/oauth/facebook/callback
```

- Save the `.env` file.

> Note: If using docker, run `docker compose up -d` to apply the .env configuration changes



================================================
FILE: pages/docs/configuration/authentication/OAuth2-OIDC/github.mdx
================================================
---
title: GitHub
description: Learn how to configure LibreChat to use GitHub for user authentication.
---

# GitHub

## Create a GitHub Application

- Go to your **[Github Developer settings](https://github.com/settings/apps)**
- Create a new GitHub app

![image](https://github.com/danny-avila/LibreChat/assets/138638445/3a8b88e7-78f8-426e-bfc2-c5e3f8b21ccb)

## GitHub Application Configuration

-  Give it a `GitHub App name` and set your `Homepage URL`
    - Example for localhost: `http://localhost:3080`
    - Example for a domain: `https://example.com`

![image](https://github.com/danny-avila/LibreChat/assets/138638445/f10d497d-460b-410f-9504-08735662648b)

- Add a valid `Callback URL`:
    - Example for localhost: `http://localhost:3080/oauth/github/callback`
    - Example for a domain: `https://example.com/oauth/github/callback`

![image](https://github.com/danny-avila/LibreChat/assets/138638445/4e7e6dba-0afb-4ed8-94bf-4c61b0f29240)

- Uncheck the box labeled `Active` in the `Webhook` section

![image](https://github.com/danny-avila/LibreChat/assets/138638445/aaeb3ecb-2e76-4ea5-8264-edfbdd53de1a)

- Scroll down to `Account permissions` and set `Email addresses` to `Access: Read-only`

![image](https://github.com/danny-avila/LibreChat/assets/138638445/3e561aa4-1f9e-4cb7-ace8-dbba8f0c0d55)

![image](https://github.com/danny-avila/LibreChat/assets/138638445/7b5f99af-7bde-43ee-9b43-6d3ce79ee00a)

- Click on `Create GitHub App`

![image](https://github.com/danny-avila/LibreChat/assets/138638445/4cc48550-eac3-4970-939b-81a23fa9c7cf)

## .env Configuration

- Click `Generate a new client secret`

![image](https://github.com/danny-avila/LibreChat/assets/138638445/484c7851-71dd-4167-a59e-9a56c4e08c36)

- Copy the `Client ID` and `Client Secret` in the `.env` file

![image](https://github.com/danny-avila/LibreChat/assets/138638445/aaf78840-48a9-44e1-9625-4109ed91d965)

```bash filename=".env"
DOMAIN_CLIENT=https://your-domain.com # use http://localhost:3080 if not using a custom domain
DOMAIN_SERVER=https://your-domain.com # use http://localhost:3080 if not using a custom domain

GITHUB_CLIENT_ID=your_client_id
GITHUB_CLIENT_SECRET=your_client_secret
GITHUB_CALLBACK_URL=/oauth/github/callback

# GitHub Enterprise (optional)
# Uncomment and configure the following if you are using GitHub Enterprise for authentication
# GITHUB_ENTERPRISE_BASE_URL=https://your-ghe-instance.com
# GITHUB_ENTERPRISE_USER_AGENT=YourEnterpriseAppName
```

- Save the `.env` file

> **Note:** If using Docker, run `docker compose up -d` to apply the .env configuration changes


================================================
FILE: pages/docs/configuration/authentication/OAuth2-OIDC/google.mdx
================================================
---
title: Google
description: Learn how to configure LibreChat to use Google for user authentication.
---

# Google

## Create a Google Application

- Visit: **[Google Cloud Console](https://cloud.google.com)** and open the `Console`

![image](https://github.com/danny-avila/LibreChat/assets/138638445/a7d290ea-6031-43b3-b367-36ce00e46f20)

- Create a New Project and give it a name

![image](https://github.com/danny-avila/LibreChat/assets/138638445/ce71c9ca-7ddd-4021-9133-a872c64c20c4)

![image](https://github.com/danny-avila/LibreChat/assets/138638445/8abbd41e-8332-4851-898d-9cddb373c527)

## Google Application Configuration

- Select the project you just created and go to `APIs and Services`

![image](https://github.com/danny-avila/LibreChat/assets/138638445/c6265582-2cf6-430f-ae51-1edbdd9f2c48)

![image](https://github.com/danny-avila/LibreChat/assets/138638445/006e16ba-56b8-452d-b324-5f2d202637ab)

- Select `Credentials` and click `CONFIGURE CONSENT SCREEN`

![image](https://github.com/danny-avila/LibreChat/assets/138638445/e4285cbb-833f-4366-820d-addf04a2ad77)

- Select `External` then click `CREATE`

![image](https://github.com/danny-avila/LibreChat/assets/138638445/232d46c0-dd00-4637-b538-3ba3bdbdc0b2)

- Fill in your App information

> Note: You can get a logo from your LibreChat folder here: `docs\assets\favicon_package\android-chrome-192x192.png`

![image](https://github.com/danny-avila/LibreChat/assets/138638445/e6c4c8ec-2f02-4af5-9458-c72394d0b7c5)

- Configure your `App domain` and add your `Developer contact information` then click `SAVE AND CONTINUE`

![image](https://github.com/danny-avila/LibreChat/assets/138638445/6c2aa557-9b9b-412d-bc2b-76a0dc11f394)

- Configure the `Sopes`
    - Add `email`,`profile` and `openid`
    - Click `UPDATE` and `SAVE AND CONTINUE`

![image](https://github.com/danny-avila/LibreChat/assets/138638445/46af2fb9-8cfd-41c5-a763-814b308e45c3)

![image](https://github.com/danny-avila/LibreChat/assets/138638445/4e832970-d392-4c67-bb38-908a5c51660a)

- Click `SAVE AND CONTINUE`
- Review your app and go back to dashboard

- Go back to the `Credentials` tab, click on `+ CREATE CREDENTIALS` and select `OAuth client ID`

![image](https://github.com/danny-avila/LibreChat/assets/138638445/beef1982-55a3-4837-8e8c-20bad8d846ba)

- Select `Web application` and give it a name

![image](https://github.com/danny-avila/LibreChat/assets/138638445/badde864-f6b5-468f-a72f-bac93326ffa5)

- Configure the `Authorized JavaScript origins`, you can add both your domain and localhost if you desire
    - Example for localhost: `http://localhost:3080`
    - Example for a domain: `https://example.com`

![image](https://github.com/danny-avila/LibreChat/assets/138638445/f7e3763a-5f74-4850-8638-44f81693b9ac)

- Add a valid `Authorized redirect URIs`
    - Example for localhost: `http://localhost:3080/oauth/google/callback`
    - Example for a domain: `https://example.com/oauth/google/callback`

![image](https://github.com/danny-avila/LibreChat/assets/138638445/0db34b19-d780-4651-9c2f-d33e24a74d55)

## .env Configuration

- Click `CREATE` and copy your `Client ID` and `Client secret`

![image](https://github.com/danny-avila/LibreChat/assets/138638445/fa8572bf-f482-457a-a285-aec7d41af76b)

- Add them to your `.env` file:

```bash filename=".env"
DOMAIN_CLIENT=https://your-domain.com # use http://localhost:3080 if not using a custom domain
DOMAIN_SERVER=https://your-domain.com # use http://localhost:3080 if not using a custom domain

GOOGLE_CLIENT_ID=your_client_id
GOOGLE_CLIENT_SECRET=your_client_secret
GOOGLE_CALLBACK_URL=/oauth/google/callback
```

- Save the `.env` file

> Note: If using docker, run `docker compose up -d` to apply the .env configuration changes



================================================
FILE: pages/docs/configuration/authentication/OAuth2-OIDC/index.mdx
================================================
---
title: Intro
description: How to configure Social Authentication for LibreChat
---

# Social Authentication

This section will cover how to configure OAuth2 and OpenID Connect with LibreChat

<div style={{ padding: "20px", display: "flex", justifyContent: "center", alignItems: "center", flexDirection: "column" }}>
  <div className="image-light-theme">
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/786fa525-73c4-4640-b4cf-91925ad8802e" style={{ width: "75%", height: "75%" }} alt="Image for Light Theme" />
  </div>

  <div className="image-dark-theme">
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/dddc34c6-9602-4177-89e8-4c0db01b0eac" style={{ width: "75%", height: "75%" }} alt="Image for Dark Theme" />
  </div>
</div>

## OAuth2
  - [Apple](/docs/configuration/authentication/OAuth2-OIDC/apple)
  - [Discord](/docs/configuration/authentication/OAuth2-OIDC/discord)
  - [Facebook](/docs/configuration/authentication/OAuth2-OIDC/facebook)
  - [GitHub](/docs/configuration/authentication/OAuth2-OIDC/github)
  - [Google](/docs/configuration/authentication/OAuth2-OIDC/google)
## OpenID Connect
  - [AWS Cognito](/docs/configuration/authentication/OAuth2-OIDC/aws)
  - [Azure Entra/AD](/docs/configuration/authentication/OAuth2-OIDC/azure)
  - [Keycloak](/docs/configuration/authentication/OAuth2-OIDC/keycloak)


================================================
FILE: pages/docs/configuration/authentication/OAuth2-OIDC/keycloak.mdx
================================================
---
title: Keycloak
description: Learn how to configure LibreChat to use Keycloak for user authentication.
---

# Keycloak

1. **Access Keycloak Admin Console:**
- Open the Keycloak Admin Console in your web browser. This is usually 
found at a URL like `http://localhost:8080/auth/admin/`.

2. **Create a Realm (if necessary):**
- If you don't already have a realm for your application, create one. Click on 'Add Realm' and give it a name.

3. **Create a Client:**
- Within your realm, click on 'Clients' and then 'Create'.
- Enter a client ID and select 'openid-connect' as the Client Protocol.
- Set 'Client Authentication' to 'On'.
- In 'Valid Redirect URIs', enter `http://localhost:3080/oauth/openid/callback` or the appropriate URI for 
your application.

![image](https://github.com/danny-avila/LibreChat/assets/6623884/d956de3d-e1f7-4327-818a-f146eb86a949)

![image](https://github.com/danny-avila/LibreChat/assets/6623884/fbefbc05-b4ec-4122-8229-54a0a5876d76)

![image](https://github.com/danny-avila/LibreChat/assets/6623884/f75c7b0f-030e-4182-bf87-ccf3aeae17d4)


4. **Configure Client:**
- After creating the client, you will be redirected to its settings page.
- Note the 'Client ID' and 'Secret' from the 'Credentials' tab – you'll need these for your application.

![image](https://github.com/danny-avila/LibreChat/assets/6623884/b1c1f0b6-641b-4cf7-a7f1-a9a32026d51b)


5. **Add Roles (Optional):**
If you want to restrict access to users with specific roles, you can define roles in Keycloak and assign them to users.
- Go to the 'Roles' tab in your client or realm (depending on where you want to define the roles).
- Create a new role that matches the value you have in `OPENID_REQUIRED_ROLE`.

![image](https://github.com/danny-avila/LibreChat/assets/6623884/67ca635f-5082-4dcc-97ac-019029a81d7c)

6. **Assign Roles to Users (Optional):**
- Go to 'Users', select a user, and go to the 'Role Mappings' tab.
- Assign the appropriate role (that matches `OPENID_REQUIRED_ROLE`) to the user.

![image](https://github.com/danny-avila/LibreChat/assets/6623884/f2ea70ed-e16c-4ec8-b84f-79fbfca627be)

7. **Get path of roles list inside token (Optional):**
- Decode your jwtToken from OpenID provider and determine path for roles list inside access token. For example, if you are 
    using Keycloak, the path is `realm_access.roles`.
- Put this path in `OPENID_REQUIRED_ROLE_PARAMETER_PATH` variable in `.env` file.
- By parameter `OPENID_REQUIRED_ROLE_TOKEN_KIND` you can specify which token kind you want to use. 
 Possible values are `access` and `id`.

8. **Update Your Project's Configuration:**
- Open the `.env` file in your project folder and add the following variables:
  ```bash filename=".env"
  OPENID_ISSUER=http://localhost:8080/realms/[YourRealmName]
  OPENID_CLIENT_ID=[YourClientID]
  OPENID_CLIENT_SECRET=[YourClientSecret]
  OPENID_SESSION_SECRET=[JustGenerateARandomSessionSecret]
  OPENID_CALLBACK_URL=/oauth/openid/callback
  OPENID_SCOPE="openid profile email"
  OPENID_REQUIRED_ROLE=[YourRequiredRole]
  OPENID_REQUIRED_ROLE_TOKEN_KIND=(access|id) # that means, `access` or `id`
  OPENID_REQUIRED_ROLE_PARAMETER_PATH="realm_access.roles"

  # Optional: redirects the user to the end session endpoint after logging out
  OPENID_USE_END_SESSION_ENDPOINT=true 
  ```



================================================
FILE: pages/docs/configuration/cdn/_meta.ts
================================================
export default {
  index: 'Intro',
  s3: 'Amazon S3 CDN',
  azure: 'Azure Blob Storage CDN',
  firebase: 'Firebase CDN',
}



================================================
FILE: pages/docs/configuration/cdn/azure.mdx
================================================
---
title: Azure Blob Storage CDN
description: This document provides instructions for setting up Azure Blob Storage CDN for LibreChat
---

# Azure Blob Storage CDN Setup

Azure Blob Storage offers scalable, secure object storage that can be used as a CDN for your static assets such as images, CSS, and JavaScript. Follow these steps to configure your Azure Blob Storage for LibreChat.

## 1. Create an Azure Storage Account

1. **Sign in to Azure:**
   - Open the [Azure Portal](https://portal.azure.com/) and sign in with your Microsoft account.

2. **Create a Storage Account:**
   - Click on **"Create a resource"** and search for **"Storage account"**.
   - Click **"Create"** and fill in the required details:
     - **Subscription & Resource Group:** Choose your subscription and either select an existing resource group or create a new one.
     - **Storage Account Name:** Enter a unique name (e.g., `mylibrechatstorage`).
     - **Region:** Select the region closest to your users.
     - **Performance & Redundancy:** Choose the performance tier and redundancy level that best suit your needs.
   - Click **"Review + Create"** and then **"Create"**. Wait until the deployment completes.

## 2. Set Up Authentication

You have two options for authenticating with your Azure Storage Account:

### Option A: Using a Connection String

1. **Navigate to Access Keys:**
   - In your newly created storage account, go to **"Access keys"** in the sidebar.

2. **Copy Connection String:**
   - Copy one of the connection strings provided. This string includes the credentials required to connect to your Blob Storage account.

### Option B: Using Managed Identity

If your LibreChat application is running on an Azure service that supports Managed Identity (such as an Azure VM, App Service, or AKS), you can use that instead of a connection string.

1. **Assign Managed Identity:**
   - Ensure your Azure resource (VM, App Service, or AKS) has a system-assigned or user-assigned Managed Identity enabled.

2. **Grant Storage Permissions:**
   - In your storage account, assign the **Storage Blob Data Contributor** (or a similarly scoped role) to your Managed Identity. This allows your application to access Blob Storage without a connection string.

## 3. Update Your Environment Variables

Create or update your `.env` file in your project’s root with the following configuration:

```bash filename=".env"
# Option A: Using a Connection String
AZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=https;AccountName=yourAccountName;AccountKey=yourAccountKey;EndpointSuffix=core.windows.net

# Option B: Using Managed Identity (do not set the connection string if using Managed Identity)
AZURE_STORAGE_ACCOUNT_NAME=yourAccountName

AZURE_STORAGE_PUBLIC_ACCESS=false
AZURE_CONTAINER_NAME=files
```

- **AZURE_STORAGE_CONNECTION_STRING:** Set this if you are using Option A.
- **AZURE_STORAGE_ACCOUNT_NAME:** Set this if you are using Option B (Managed Identity). Do not set both.
- **AZURE_STORAGE_PUBLIC_ACCESS:** Set to `false` if you do not want your blobs to be publicly accessible by default. Set to `true` if you need public access (for example, for publicly viewable images).
- **AZURE_CONTAINER_NAME:** This is the container name your application will use (e.g., `files`). The application will automatically create this container if it doesn’t exist.

## 4. Configure LibreChat to Use Azure Blob Storage

Update your LibreChat configuration file (`librechat.yaml`) to specify that the application should use Azure Blob Storage for file handling:

```yaml filename="librechat.yaml"
version: 1.0.8
cache: true
fileStrategy: "azure"
```

This setting tells LibreChat to use the Azure Blob Storage implementation provided in your code.

---

## Summary

1. **Create a Storage Account:**  
Sign in to the Azure Portal, create a storage account, and wait for deployment to finish.

2. **Set Up Authentication:**
- **Option A:** Retrieve the connection string from **"Access keys"** in your storage account.
- **Option B:** Use Managed Identity by enabling it on your Azure resource and granting it appropriate storage permissions.

3. **Update Environment Variables:**  
In your `.env` file, set either:
- `AZURE_STORAGE_CONNECTION_STRING` (for Option A), or
- `AZURE_STORAGE_ACCOUNT_NAME` (for Option B), along with:
- `AZURE_STORAGE_PUBLIC_ACCESS` and
- `AZURE_CONTAINER_NAME`.

4. **Configure LibreChat:**  
Set `fileStrategy` to `"azure"` in your `librechat.yaml` configuration file.

With these steps, your LibreChat application will automatically create the container (if it doesn’t exist) and manage file uploads, downloads, and deletions using Azure Blob Storage as your CDN. Managed Identity provides a secure alternative by eliminating the need for long-term credentials.

---

<Callout type="info" title="Note">
  Always ensure that your connection string remains secure and never commit it to a public repository. Adjust the public access setting as needed based on your application’s security requirements.
</Callout>


================================================
FILE: pages/docs/configuration/cdn/firebase.mdx
================================================
---
title: Firebase CDN
description: This document provides instructions for setting up Firebase CDN for LibreChat
---

# Firebase CDN Setup

Firebase CDN (Content Delivery Network) is a feature of the Firebase platform that allows you to host and serve static assets, such as HTML, CSS, JavaScript, images, and videos, from a network of edge locations around the world.

## Steps to Set Up Firebase

1. Open the [Firebase website](https://firebase.google.com/).
2. Click on "Get started."
3. Sign in with your Google account.

### Create a New Project

- Name your project (you can use the same project as Google OAuth).

![Project Name](https://github.com/danny-avila/LibreChat/assets/81851188/dccce3e0-b639-41ef-8142-19d24911c65c)

- Optionally, you can disable Google Analytics.

![Google Analytics](https://github.com/danny-avila/LibreChat/assets/81851188/5d4d58c5-451c-498b-97c0-f123fda79514)

- Wait for 20/30 seconds for the project to be ready, then click on "Continue."

![Continue](https://github.com/danny-avila/LibreChat/assets/81851188/6929802e-a30b-4b1e-b124-1d4b281d0403)

- Click on "All Products."

![All Products](https://github.com/danny-avila/LibreChat/assets/81851188/92866c82-2b03-4ebe-807e-73a0ccce695e)

- Select "Storage."

![Storage](https://github.com/danny-avila/LibreChat/assets/81851188/b22dcda1-256b-494b-a835-a05aeea02e89)

- Click on "Get Started."

![Get Started](https://github.com/danny-avila/LibreChat/assets/81851188/c3f0550f-8184-4c79-bb84-fa79655b7978)

- Click on "Next."

![Next](https://github.com/danny-avila/LibreChat/assets/81851188/2a65632d-fe22-4c71-b8f1-aac53ee74fb6)

- Select your "Cloud Storage location."

![Cloud Storage Location](https://github.com/danny-avila/LibreChat/assets/81851188/c094d4bc-8e5b-43c7-96d9-a05bcf4e2af6)

- Return to the Project Overview.

![Project Overview](https://github.com/danny-avila/LibreChat/assets/81851188/c425f4bb-a494-42f2-9fdc-ff2c8ce005e1)

- Click on "+ Add app" under your project name, then click on "Web."

![Web](https://github.com/danny-avila/LibreChat/assets/81851188/22dab877-93cb-4828-9436-10e14374e57e)

- Register the app.

![Register App](https://github.com/danny-avila/LibreChat/assets/81851188/0a1b0a75-7285-4f03-95cf-bf971bd7d874)

- Save all this information in a text file.

![Save Information](https://github.com/danny-avila/LibreChat/assets/81851188/056754ad-9d36-4662-888e-f189ddb38fd3)

- Fill all the `firebaseConfig` variables in the `.env` file.

```bash
FIREBASE_API_KEY=api_key #apiKey
FIREBASE_AUTH_DOMAIN=auth_domain #authDomain
FIREBASE_PROJECT_ID=project_id #projectId
FIREBASE_STORAGE_BUCKET=storage_bucket #storageBucket
FIREBASE_MESSAGING_SENDER_ID=messaging_sender_id #messagingSenderId
FIREBASE_APP_ID=1:your_app_id #appId
```

- Return one last time to the Project Overview.

![Project Overview](https://github.com/danny-avila/LibreChat/assets/81851188/c425f4bb-a494-42f2-9fdc-ff2c8ce005e1)

- Select `Storage`

![image](https://github.com/danny-avila/LibreChat/assets/32828263/16a0f850-cdd4-4875-8342-ab67bfb59804)

- Select `Rules` and delete `: if false;` on this line: `allow read, write: if false;`

    - your updated rules should look like this:

    ```bash
    rules_version = '2';
    
    service firebase.storage {
      match /b/{bucket}/o {
        match /images/{userId}/{fileName} {
          allow read, write: if true;
        }
      }
    }
    ```

![image](https://github.com/danny-avila/LibreChat/assets/32828263/c190011f-c1a6-47c7-986e-8d309b5f8704)

- Publish your updated rules

![image](https://github.com/danny-avila/LibreChat/assets/32828263/5e6a17c3-5aba-419a-a18f-be910b1f25d5)

### Configure `fileStrategy` in `librechat.yaml`

Finally, to enable the app use Firebase, you must set the following in your `librechat.yaml` config file.

```yaml
  version: 1.0.8
  cache: true
  fileStrategy: "firebase"
```

For more information about the `librechat.yaml` config file, see the guide here: [Custom Endpoints & Configuration](/docs/configuration/librechat_yaml).

<Callout type="warning" title="Export convos as png when using Firebase">

### Step-by-Step Guide to Set Up CORS for Firebase Storage

---

#### **Step 1: Create the CORS Configuration File**

- Open a text editor of your choice.
- Create a new file and name it `cors.json`.
- Add the following configuration to allow access from "https://ai.example.com":

```json
[
  {
    "origin": ["https://ai.example.com"],
    "method": ["GET", "POST", "DELETE", "PUT"],
    "maxAgeSeconds": 3600
  }
]
```
- Save the file.

---

#### **Step 2: Apply the CORS Configuration**

- Open your terminal or command prompt.
- Navigate to the directory where you saved the `cors.json` file.
- Execute the following command, replacing `<your-cloud-storage-bucket>` with the name of your Firebase Storage bucket:

```shell
gsutil cors set cors.json gs://<your-cloud-storage-bucket>
```

---

#### **Step 3: Verify the CORS Settings**

- To confirm that the CORS settings have been applied correctly, you can retrieve the current CORS configuration with the following command:

```shell
gsutil cors get gs://<your-cloud-storage-bucket>
```

- The output should reflect the settings you specified in the `cors.json` file.

---

#### **Step 4: Test the Configuration**

- Try exporting a convo as png from the allowed origin ("https://ai.example.com").
- If everything is set up correctly, you should not encounter any CORS issues.

---

> **Note:** Always ensure that you're applying CORS settings only for trusted origins to maintain the security of your application. Adjust the allowed methods and headers according to your specific needs.

---

That's it! You've successfully configured CORS for your Firebase Storage bucket to allow requests from a specific origin. Remember to replace `<your-cloud-storage-bucket>` with your actual bucket name and `https://ai.example.com` with your own domain when applying the configuration.

</Callout>



================================================
FILE: pages/docs/configuration/cdn/index.mdx
================================================
---
title: CDN
description: CDN setup instructions
---

# CDN

<Callout type="info" title="Note">
  This guide covers the setup instructions for three CDN options. Choose the one that best meets your deployment requirements.
</Callout>

## Setup Instructions:

### Amazon S3 CDN
- [Amazon S3 CDN](/docs/configuration/cdn/s3)

### Azure Blob Storage CDN
- [Azure Blob Storage CDN](/docs/configuration/cdn/azure)

### Firebase CDN
- [Firebase CDN](/docs/configuration/cdn/firebase)



================================================
FILE: pages/docs/configuration/cdn/s3.mdx
================================================
---
title: Amazon S3 CDN
description: This document provides instructions for setting up Amazon S3 as a CDN for LibreChat.
---

# Amazon S3 CDN Setup

Amazon S3 is a scalable, secure object storage service that can be used as a CDN for your static assets (such as images, CSS, and JavaScript) in LibreChat. Follow these steps to configure your S3 bucket.

## 1. Create an AWS Account and Configure an IAM User (or Use IRSA)

### Option A: Using an IAM User with Explicit Credentials

1. **Sign in to AWS:**
   - Open the [AWS Management Console](https://aws.amazon.com/console/) and sign in with your account.

2. **Create or Use an Existing IAM User:**
   - Navigate to the **IAM (Identity and Access Management)** section.
   - Create a new IAM user with **Programmatic Access** or select an existing one.
   - Attach an appropriate policy (for example, `AmazonS3FullAccess` or a custom policy with limited S3 permissions).
   - After creating the user, you will receive an **AWS_ACCESS_KEY_ID** and **AWS_SECRET_ACCESS_KEY**. Store these securely.

### Option B: Using IRSA (IAM Roles for Service Accounts) in Kubernetes

If you are deploying LibreChat on Kubernetes (e.g. on EKS), you can use IRSA to assign AWS permissions to your pods without having to provide explicit credentials. To use IRSA:

1. **Create a Trust Policy** for your EKS service account (example below):
   ```json
   {
     "Version": "2012-10-17",
     "Statement": [
       {
         "Effect": "Allow",
         "Principal": {
           "Federated": "arn:aws:iam::{AWS_ACCOUNT}:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/{EKS_OIDC}"
         },
         "Action": "sts:AssumeRoleWithWebIdentity",
         "Condition": {
           "StringEquals": {
             "oidc.eks.us-east-1.amazonaws.com/id/{EKS_OIDC}:sub": "system:serviceaccount:librechat:librechat",
             "oidc.eks.us-east-1.amazonaws.com/id/{EKS_OIDC}:aud": "sts.amazonaws.com"
           }
         }
       }
     ]
   }
   ```
2. **Create a Policy** that grants necessary S3 permissions (example below):
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "VisualEditor0",
      "Effect": "Allow",
      "Action": [
        "s3:PutObject",
        "s3:GetObjectAcl",
        "s3:GetObject",
        "s3:ListBucket",
        "s3:DeleteObject"
      ],
      "Resource": [
        "arn:aws:s3:::my-example-librechat-bucket/*",
        "arn:aws:s3:::my-example-librechat-bucket"
      ]
    }
  ]
}
   ```
3. **Annotate your Kubernetes ServiceAccount:**  
Ensure your LibreChat pods use a service account annotated for IRSA. This way, the AWS SDK in your application (using our updated S3 initialization code) will automatically use the temporary credentials provided by IRSA without needing the environment variables for AWS credentials.

## 2. Create an S3 Bucket

1. **Open the S3 Console:**
- Go to the [Amazon S3 console](https://s3.console.aws.amazon.com/s3/).

2. **Create a New Bucket:**
- Click **"Create bucket"**.
- **Bucket Name:** Enter a unique name (e.g., `mylibrechatbucket`).
- **Region:** Select the AWS region closest to your users (for example, `us-east-1` or `eu-west-1`).
- **Configure Options:** Set other options as needed, then click **"Create bucket"**.

## 3. Update Your Environment Variables

If you are **not** using IRSA, create or update your `.env` file in your project’s root directory with the following configuration:

```bash filename=".env"
AWS_ACCESS_KEY_ID=your_access_key_id
AWS_SECRET_ACCESS_KEY=your_secret_access_key
AWS_REGION=your_selected_region
AWS_BUCKET_NAME=your_bucket_name
```

- **AWS_ACCESS_KEY_ID:** Your IAM user's access key.
- **AWS_SECRET_ACCESS_KEY:** Your IAM user's secret key.
- **AWS_REGION:** The AWS region where your S3 bucket is located.
- **AWS_BUCKET_NAME:** The name of the S3 bucket you created.

If you are using **IRSA** on Kubernetes, you do **not** need to set `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` in your environment. The AWS SDK will automatically obtain temporary credentials via the service account assigned to your pod. Ensure that `AWS_REGION` and `AWS_BUCKET_NAME` are still provided.

## 4. Configure LibreChat to Use Amazon S3

Update your LibreChat configuration file (`librechat.yaml`) to specify that the application should use Amazon S3 for file handling:

```yaml filename="librechat.yaml"
version: 1.0.8
cache: true
fileStrategy: "s3"
```

This setting tells LibreChat to use the S3 implementation provided in your code.

## Summary

1. **Create an AWS Account & IAM User (or configure IRSA):**
- For traditional deployments, create an IAM user with programmatic access and obtain your access keys.
- For Kubernetes deployments (e.g., on EKS), set up IRSA so that your pods automatically obtain temporary credentials.

2. **Create an S3 Bucket:**
- Use the Amazon S3 console to create a bucket, choosing a unique name and region.

3. **Update Environment Variables:**
- For non-IRSA: set `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`, and `AWS_BUCKET_NAME` in your `.env` file.
- For IRSA: set only `AWS_REGION` and `AWS_BUCKET_NAME`; ensure your pod’s service account is correctly annotated.

4. **Configure LibreChat:**
- Set `fileStrategy` to `"s3"` in your `librechat.yaml` configuration file.

With these steps, your LibreChat application will use Amazon S3 to handle file uploads, downloads, and deletions, leveraging S3 as your CDN for static assets. Additionally, with IRSA support, your application can run securely on Kubernetes without embedding long-term AWS credentials.

<Callout type="info" title="Note">
  Always ensure your AWS credentials remain secure. Do not commit them to a public repository. Adjust IAM policies to follow the principle of least privilege as needed.
</Callout>



================================================
FILE: pages/docs/configuration/librechat_yaml/_meta.ts
================================================
export default {
  index: 'Intro',
  ai_endpoints: 'Custom AI Endpoints',
  setup: 'Setup',
  example: 'Example Configs',
  object_structure: 'Settings',
}



================================================
FILE: pages/docs/configuration/librechat_yaml/example.mdx
================================================
# Example

## Clean Example

<Callout type="example" title="Example" collapsible>

This example config includes all documented endpoints (Except Azure, LiteLLM, MLX, and Ollama, which all require additional configurations)

```yaml filename="librechat.yaml"
version: 1.0.8

cache: true

interface:
  # Privacy policy settings
  privacyPolicy:
    externalUrl: 'https://librechat.ai/privacy-policy'
    openNewTab: true

  # Terms of service
  termsOfService:
    externalUrl: 'https://librechat.ai/tos'
    openNewTab: true

registration:
  socialLogins: ["discord", "facebook", "github", "google", "openid"]

endpoints:
  custom:

    # Anyscale
    - name: "Anyscale"
      apiKey: "${ANYSCALE_API_KEY}"
      baseURL: "https://api.endpoints.anyscale.com/v1"
      models:
        default: [
          "meta-llama/Llama-2-7b-chat-hf",
          ]
        fetch: true
      titleConvo: true
      titleModel: "meta-llama/Llama-2-7b-chat-hf"
      summarize: false
      summaryModel: "meta-llama/Llama-2-7b-chat-hf"
      forcePrompt: false
      modelDisplayLabel: "Anyscale"

    # APIpie
    - name: "APIpie"
      apiKey: "${APIPIE_API_KEY}"
      baseURL: "https://apipie.ai/v1/"
      models:
        default: [
          "gpt-4",
          "gpt-4-turbo",
          "gpt-3.5-turbo",
          "claude-3-opus",
          "claude-3-sonnet",
          "claude-3-haiku",
          "llama-3-70b-instruct",
          "llama-3-8b-instruct",
          "gemini-pro-1.5",
          "gemini-pro",
          "mistral-large",
          "mistral-medium",
          "mistral-small",
          "mistral-tiny",
          "mixtral-8x22b",
          ]
        fetch: false
      titleConvo: true
      titleModel: "gpt-3.5-turbo"
      dropParams: ["stream"]

    #cohere
    - name: "cohere"
      apiKey: "${COHERE_API_KEY}"
      baseURL: "https://api.cohere.ai/v1"
      models:
        default: ["command-r","command-r-plus","command-light","command-light-nightly","command","command-nightly"]
        fetch: false
      modelDisplayLabel: "cohere"
      titleModel: "command"
      dropParams: ["stop", "user", "frequency_penalty", "presence_penalty", "temperature", "top_p"]

    # Fireworks
    - name: "Fireworks"
      apiKey: "${FIREWORKS_API_KEY}"
      baseURL: "https://api.fireworks.ai/inference/v1"
      models:
        default: [
          "accounts/fireworks/models/mixtral-8x7b-instruct",
          ]
        fetch: true
      titleConvo: true
      titleModel: "accounts/fireworks/models/llama-v2-7b-chat"
      summarize: false
      summaryModel: "accounts/fireworks/models/llama-v2-7b-chat"
      forcePrompt: false
      modelDisplayLabel: "Fireworks"
      dropParams: ["user"]
  
    # groq
    - name: "groq"
      apiKey: "${GROQ_API_KEY}"
      baseURL: "https://api.groq.com/openai/v1/"
      models:
        default: [
          "llama2-70b-4096",
          "llama3-70b-8192",
          "llama3-8b-8192",
          "mixtral-8x7b-32768",
          "gemma-7b-it",
          ]
        fetch: false
      titleConvo: true
      titleModel: "mixtral-8x7b-32768"
      modelDisplayLabel: "groq"

    # Mistral AI API
    - name: "Mistral"
      apiKey: "${MISTRAL_API_KEY}"
      baseURL: "https://api.mistral.ai/v1"
      models:
        default: [
          "mistral-tiny",
          "mistral-small",
          "mistral-medium",
          "mistral-large-latest"
          ]
        fetch: true
      titleConvo: true
      titleModel: "mistral-tiny"
      modelDisplayLabel: "Mistral"
      dropParams: ["stop", "user", "frequency_penalty", "presence_penalty"]

    # OpenRouter.ai
    - name: "OpenRouter"
      apiKey: "${OPENROUTER_KEY}"
      baseURL: "https://openrouter.ai/api/v1"
      models:
        default: ["openai/gpt-3.5-turbo"]
        fetch: true
      titleConvo: true
      titleModel: "gpt-3.5-turbo"
      summarize: false
      summaryModel: "gpt-3.5-turbo"
      forcePrompt: false
      modelDisplayLabel: "OpenRouter"

    # Perplexity
    - name: "Perplexity"
      apiKey: "${PERPLEXITY_API_KEY}"
      baseURL: "https://api.perplexity.ai/"
      models:
        default: [
          "mistral-7b-instruct",
          "sonar-small-chat",
          "sonar-small-online",
          "sonar-medium-chat",
          "sonar-medium-online"
          ]
        fetch: false # fetching list of models is not supported
      titleConvo: true
      titleModel: "sonar-medium-chat"
      summarize: false
      summaryModel: "sonar-medium-chat"
      forcePrompt: false
      dropParams: ["stop", "frequency_penalty"]
      modelDisplayLabel: "Perplexity"

    # ShuttleAI API
    - name: "ShuttleAI"
      apiKey: "${SHUTTLEAI_API_KEY}"
      baseURL: "https://api.shuttleai.app/v1"
      models:
        default: [
          "shuttle-1", "shuttle-turbo"
          ]
        fetch: true
      titleConvo: true
      titleModel: "gemini-pro"
      summarize: false
      summaryModel: "llama-summarize"
      forcePrompt: false
      modelDisplayLabel: "ShuttleAI"
      dropParams: ["user"]

    # together.ai
    - name: "together.ai"
      apiKey: "${TOGETHERAI_API_KEY}"
      baseURL: "https://api.together.xyz"
      models:
        default: [
          "zero-one-ai/Yi-34B-Chat",
          "Austism/chronos-hermes-13b",
          "DiscoResearch/DiscoLM-mixtral-8x7b-v2",
          "Gryphe/MythoMax-L2-13b",
          "lmsys/vicuna-13b-v1.5",
          "lmsys/vicuna-7b-v1.5",
          "lmsys/vicuna-13b-v1.5-16k",
          "codellama/CodeLlama-13b-Instruct-hf",
          "codellama/CodeLlama-34b-Instruct-hf",
          "codellama/CodeLlama-70b-Instruct-hf",
          "codellama/CodeLlama-7b-Instruct-hf",
          "togethercomputer/llama-2-13b-chat",
          "togethercomputer/llama-2-70b-chat",
          "togethercomputer/llama-2-7b-chat",
          "NousResearch/Nous-Capybara-7B-V1p9",
          "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
          "NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT",
          "NousResearch/Nous-Hermes-Llama2-70b",
          "NousResearch/Nous-Hermes-llama-2-7b",
          "NousResearch/Nous-Hermes-Llama2-13b",
          "NousResearch/Nous-Hermes-2-Yi-34B",
          "openchat/openchat-3.5-1210",
          "Open-Orca/Mistral-7B-OpenOrca",
          "togethercomputer/Qwen-7B-Chat",
          "snorkelai/Snorkel-Mistral-PairRM-DPO",
          "togethercomputer/alpaca-7b",
          "togethercomputer/falcon-40b-instruct",
          "togethercomputer/falcon-7b-instruct",
          "togethercomputer/GPT-NeoXT-Chat-Base-20B",
          "togethercomputer/Llama-2-7B-32K-Instruct",
          "togethercomputer/Pythia-Chat-Base-7B-v0.16",
          "togethercomputer/RedPajama-INCITE-Chat-3B-v1",
          "togethercomputer/RedPajama-INCITE-7B-Chat",
          "togethercomputer/StripedHyena-Nous-7B",
          "Undi95/ReMM-SLERP-L2-13B",
          "Undi95/Toppy-M-7B",
          "WizardLM/WizardLM-13B-V1.2",
          "garage-bAInd/Platypus2-70B-instruct",
          "mistralai/Mistral-7B-Instruct-v0.1",
          "mistralai/Mistral-7B-Instruct-v0.2",
          "mistralai/Mixtral-8x7B-Instruct-v0.1",
          "teknium/OpenHermes-2-Mistral-7B",
          "teknium/OpenHermes-2p5-Mistral-7B",
          "upstage/SOLAR-10.7B-Instruct-v1.0"
          ]
        fetch: false # fetching list of models is not supported
      titleConvo: true
      titleModel: "togethercomputer/llama-2-7b-chat"
      summarize: false
      summaryModel: "togethercomputer/llama-2-7b-chat"
      forcePrompt: false
      modelDisplayLabel: "together.ai"
```

</Callout>

## Example with Comments

This example configuration file sets up LibreChat with detailed options across several key areas:

- **Caching**: Enabled to improve performance.
- **File Handling**:
    - **File Strategy**: Commented out but hints at possible integration with Firebase for file storage.
    - **File Configurations**: Customizes file upload limits and allowed MIME types for different endpoints, including a global server file size limit and a specific limit for user avatar images.
- **Rate Limiting**: Defines thresholds for the maximum number of file uploads allowed per IP and user within a specified time window, aiming to prevent abuse.
- **Registration**:
    - Allows registration from specified social login providers and email domains, enhancing security and user management.
- **Endpoints**:
    - **Assistants**: Configures the assistants' endpoint with a polling interval and a timeout for operations, and provides an option to disable the builder interface.
    - **Custom Endpoints**:
        - Configures two external AI service endpoints, Mistral and OpenRouter, including API keys, base URLs, model handling, and specific feature toggles like conversation titles, summarization, and parameter adjustments.
        - For Mistral, it enables dynamic model fetching, applies additional parameters for safe prompts, and explicitly drops unsupported parameters.
        - For OpenRouter, it sets up a basic configuration without dynamic model fetching and specifies a model for conversation titles.

<Callout type="example" title="Commented Example" collapsible>

```yaml filename="librechat.yaml"
# For more information, see the Configuration Guide:
# https://docs.librechat.ai/install/configuration/custom_config.html

# Configuration version (required)
version: 1.0.9

# Cache settings: Set to true to enable caching
cache: true

# Custom interface configuration
interface:
  # Privacy policy settings
  privacyPolicy:
    externalUrl: 'https://librechat.ai/privacy-policy'
    openNewTab: true

  # Terms of service
  termsOfService:
    externalUrl: 'https://librechat.ai/tos'
    openNewTab: true

# Example Registration Object Structure (optional)
registration:
  socialLogins: ['github', 'google', 'discord', 'openid', 'facebook']
  # allowedDomains:
  # - "gmail.com"

# rateLimits:
#   fileUploads:
#     ipMax: 100
#     ipWindowInMinutes: 60  # Rate limit window for file uploads per IP
#     userMax: 50
#     userWindowInMinutes: 60  # Rate limit window for file uploads per user
#   conversationsImport:
#     ipMax: 100
#     ipWindowInMinutes: 60  # Rate limit window for conversation imports per IP
#     userMax: 50
#     userWindowInMinutes: 60  # Rate limit window for conversation imports per user

# Definition of custom endpoints
endpoints:
  # assistants:
  #   disableBuilder: false # Disable Assistants Builder Interface by setting to `true`
  #   pollIntervalMs: 750  # Polling interval for checking assistant updates
  #   timeoutMs: 180000  # Timeout for assistant operations
  #   # Should only be one or the other, either `supportedIds` or `excludedIds`
  #   supportedIds: ["asst_supportedAssistantId1", "asst_supportedAssistantId2"]
  #   # excludedIds: ["asst_excludedAssistantId"]
  #   Only show assistants that the user created or that were created externally (e.g. in Assistants playground).
  #   # privateAssistants: false # Does not work with `supportedIds` or `excludedIds`
  #   # (optional) Models that support retrieval, will default to latest known OpenAI models that support the feature
  #   retrievalModels: ["gpt-4-turbo-preview"]
  #   # (optional) Assistant Capabilities available to all users. Omit the ones you wish to exclude. Defaults to list below.
  #   capabilities: ["code_interpreter", "retrieval", "actions", "tools", "image_vision"]
  custom:
    # Groq Example
    - name: 'groq'
      apiKey: '${GROQ_API_KEY}'
      baseURL: 'https://api.groq.com/openai/v1/'
      models:
        default: [
          "llama3-70b-8192",
          "llama3-8b-8192",
          "llama2-70b-4096",
          "mixtral-8x7b-32768",
          "gemma-7b-it",
          ]
        fetch: false
      titleConvo: true
      titleModel: 'mixtral-8x7b-32768'
      modelDisplayLabel: 'groq'

    # Mistral AI Example
    - name: 'Mistral' # Unique name for the endpoint
      # For `apiKey` and `baseURL`, you can use environment variables that you define.
      # recommended environment variables:
      apiKey: '${MISTRAL_API_KEY}'
      baseURL: 'https://api.mistral.ai/v1'

      # Models configuration
      models:
        # List of default models to use. At least one value is required.
        default: ['mistral-tiny', 'mistral-small', 'mistral-medium']
        # Fetch option: Set to true to fetch models from API.
        fetch: true # Defaults to false.

      # Optional configurations

      # Title Conversation setting
      titleConvo: true # Set to true to enable title conversation

      # Title Method: Choose between "completion" or "functions".
      # titleMethod: "completion"  # Defaults to "completion" if omitted.

      # Title Model: Specify the model to use for titles.
      titleModel: 'mistral-tiny' # Defaults to "gpt-3.5-turbo" if omitted.

      # Summarize setting: Set to true to enable summarization.
      # summarize: false

      # Summary Model: Specify the model to use if summarization is enabled.
      # summaryModel: "mistral-tiny"  # Defaults to "gpt-3.5-turbo" if omitted.

      # Force Prompt setting: If true, sends a `prompt` parameter instead of `messages`.
      # forcePrompt: false

      # The label displayed for the AI model in messages.
      modelDisplayLabel: 'Mistral' # Default is "AI" when not set.

      # Add additional parameters to the request. Default params will be overwritten.
      # addParams:
      # safe_prompt: true # This field is specific to Mistral AI: https://docs.mistral.ai/api/

      # Drop Default params parameters from the request. See default params in guide linked below.
      # NOTE: For Mistral, it is necessary to drop the following parameters or you will encounter a 422 Error:
      dropParams: ['stop', 'user', 'frequency_penalty', 'presence_penalty']

    # OpenRouter Example
    - name: 'OpenRouter'
      # For `apiKey` and `baseURL`, you can use environment variables that you define.
      # recommended environment variables:
      # Known issue: you should not use `OPENROUTER_API_KEY` as it will then override the `openAI` endpoint to use OpenRouter as well.
      apiKey: '${OPENROUTER_KEY}'
      baseURL: 'https://openrouter.ai/api/v1'
      models:
        default: ['meta-llama/llama-3-70b-instruct']
        fetch: true
      titleConvo: true
      titleModel: 'meta-llama/llama-3-70b-instruct'
      # Recommended: Drop the stop parameter from the request as Openrouter models use a variety of stop tokens.
      dropParams: ['stop']
      modelDisplayLabel: 'OpenRouter'

# fileConfig:
#   endpoints:
#     assistants:
#       fileLimit: 5
#       fileSizeLimit: 10  # Maximum size for an individual file in MB
#       totalSizeLimit: 50  # Maximum total size for all files in a single request in MB
#       supportedMimeTypes:
#         - "image/.*"
#         - "application/pdf"
#     openAI:
#       disabled: true  # Disables file uploading to the OpenAI endpoint
#     default:
#       totalSizeLimit: 20
#     YourCustomEndpointName:
#       fileLimit: 2
#       fileSizeLimit: 5
#   serverFileSizeLimit: 100  # Global server file size limit in MB
#   avatarSizeLimit: 2  # Limit for user avatar image size in MB
# See the Custom Configuration Guide for more information:
# https://docs.librechat.ai/install/configuration/custom_config.html
```

</Callout>


================================================
FILE: pages/docs/configuration/librechat_yaml/index.mdx
================================================
---
title: Custom Config
description: Comprehensive guides for configuring the `librechat.yaml` file AKA the LibreChat Config file. This section is your one-stop resource for understanding and customizing endpoints & other integrations.
weight: -11
---

# Intro

Welcome to the guide for configuring the **librechat.yaml** file in LibreChat.

This file enables the integration of custom AI endpoints, enabling you to connect with any AI provider compliant with OpenAI API standards.

## Key Features

- **Endpoint Integration**: Seamlessly integrate with a variety of AI providers compliant with OpenAI API standards, including Mistral AI, reverse proxies, and more.
- **Advanced Customization**: Configure file handling, rate limiting, user registration, and interface elements to align with your preferences and requirements.
- **Model Specifications**: Define detailed model configurations, presets, and behaviors to deliver a tailored AI experience.
- **Agents**: Use Provider-agnostic, no-code assistants, with options to customize capabilities.
- **MCP Servers**: Integrate with the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) for tool integrations.
- **Assistants Integration**: Leverage the power of OpenAI's Assistants API, with options to customize capabilities, polling intervals, and timeouts.
- **Azure OpenAI Support**: Integrate with Azure OpenAI Service, enabling access to multiple deployments, region models, and serverless inference endpoints.

<div style={{ display: "flex", justifyContent: "center", alignItems: "center", flexDirection: "column" }}>
  <div className="image-light-theme">
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/26336fa4-db61-438b-929c-0004aa4db56c" alt="modelmenu-light" style={{ width: "75%", height: "75%" }} />
  </div>

  <div className="image-dark-theme">
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/1e2fe33b-7073-4b4e-9ee1-7b7a99704bad" alt="modelmenu-dark" style={{ width: "75%", height: "75%" }} />
  </div>
</div>


Future updates will streamline configuration further by migrating some settings from your [.env file](/docs/configuration/dotenv) to `librechat.yaml`.

Stay tuned for ongoing enhancements to customize your LibreChat instance!

**Note:** To verify your YAML config, you can use the [YAML Validator](/toolkit/yaml_checker) or other online tools like [yamlchecker.com](https://yamlchecker.com/)



================================================
FILE: pages/docs/configuration/librechat_yaml/setup.mdx
================================================
# Setup

**The `librechat.yaml` file should be placed in the root of the project where the .env file is located.**

You can copy the [example config file](/docs/configuration/librechat_yaml/example) as a good starting point while reading the rest of the guide.

The example config file has some options ready to go for Mistral AI, Groq and Openrouter.

**Note:** You can set an alternate filepath for the `librechat.yaml` file through an environment variable:

```bash filename=".env"
CONFIG_PATH="/alternative/path/to/librechat.yaml"
```

## Docker Setup

For Docker, you need to make use of an [override file](/docs/configuration/docker_override), named `docker-compose.override.yml`, to ensure the config file works for you.

- First, make sure your containers stop running with `docker compose down`
- Create or edit existing `docker-compose.override.yml` at the root of the project:



```yaml filename="docker-compose.override.yml"
version: '3.4'

services:
  api:
    volumes:
      - type: bind
        source: ./librechat.yaml
        target: /app/librechat.yaml
```

- **Note:** If you are using `CONFIG_PATH` for an alternative filepath for this file, make sure to specify it accordingly.

- Start docker again, and you should see your config file settings apply

```bash filename="Restart the containers"
docker compose up
```



================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/_meta.ts
================================================
export default {
  index: 'Intro',
}



================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/anyscale.mdx
================================================
---
title: Anyscale
description: Example configuration for Anyscale
---

# [Anyscale](https://app.endpoints.anyscale.com/)

> **Anyscale API key:** [anyscale.com/credentials](https://app.endpoints.anyscale.com/credentials)

**Notes:**

- **Known:** icon provided, fetching list of models is recommended.

```yaml filename="librechat.yaml"
    - name: "Anyscale"
      apiKey: "${ANYSCALE_API_KEY}"
      baseURL: "https://api.endpoints.anyscale.com/v1"
      models:
        default: [
          "meta-llama/Llama-2-7b-chat-hf",
          ]
        fetch: true
      titleConvo: true
      titleModel: "meta-llama/Llama-2-7b-chat-hf"
      summarize: false
      summaryModel: "meta-llama/Llama-2-7b-chat-hf"
      forcePrompt: false
      modelDisplayLabel: "Anyscale"
```

![image](https://github.com/danny-avila/LibreChat/assets/32828263/9f2d8ad9-3f49-4fe3-a3ed-c85994c1c85f)


================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/apipie.mdx
================================================
---
title: APIpie
description: Example configuration for APIpie
---

# [APIpie](https://apipie.ai/)

> **APIpie API key:** [apipie.ai/dashboard/profile/api-keys](https://apipie.ai/dashboard/profile/api-keys)

**Notes:**

- **Known:** icon provided, fetching list of models is recommended as API token rates and pricing used for token credit balances when models are fetched.

- **Known issue:** 
  - Fetching list of models is not supported.
  - Your success may vary with conversation titling
  - Stream isn't currently supported (but is planned as of April 24, 2024)

<Callout type="tip" title="Fetch and order the models" collapsible>
This python script can fetch and order the llm models for you. The output will be saved in models.txt, formated in a way that should make it easier for you to include in the yaml config.

```py filename="fetch.py"
import json
import requests

def fetch_and_order_models():
    # API endpoint
    url = "https://apipie.ai/models"

    # headers as per request example
    headers = {"Accept": "application/json"}

    # request parameters
    params = {"type": "llm"}

    # make request
    response = requests.get(url, headers=headers, params=params)

    # parse JSON response
    data = response.json()

    # extract an ordered list of unique model IDs
    model_ids = sorted(set([model["id"] for model in data]))

    # write result to a text file
    with open("models.txt", "w") as file:
        json.dump(model_ids, file, indent=2)

# execute the function
if __name__ == "__main__":
    fetch_and_order_models()
```
</Callout>

```yaml filename="librechat.yaml"
    # APIpie
    - name: "APIpie"
      apiKey: "${APIPIE_API_KEY}"
      baseURL: "https://apipie.ai/v1/"
      models:
        default: [
          "gpt-4",
          "gpt-4-turbo",
          "gpt-3.5-turbo",
          "claude-3-opus",
          "claude-3-sonnet",
          "claude-3-haiku",
          "llama-3-70b-instruct",
          "llama-3-8b-instruct",
          "gemini-pro-1.5",
          "gemini-pro",
          "mistral-large",
          "mistral-medium",
          "mistral-small",
          "mistral-tiny",
          "mixtral-8x22b",
          ]
        fetch: false
      titleConvo: true
      titleModel: "claude-3-haiku"
      summarize: false
      summaryModel: "claude-3-haiku"
      dropParams: ["stream"]
      modelDisplayLabel: "APIpie"
```

![image](https://github.com/danny-avila/LibreChat/assets/32828263/b6a21524-b309-4a51-8b88-c280fb330af4)


================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/cohere.mdx
================================================
---
title: Cohere
description: Example configuration for Cohere
---

# [Cohere](https://cohere.com/)

> Cohere API key: [dashboard.cohere.com](https://dashboard.cohere.com/)

**Notes:**

- **Known:** icon provided.
- Experimental: does not follow OpenAI-spec, uses a new method for endpoint compatibility, shares some similarities and parameters.
- For a full list of Cohere-specific parameters, see the [Cohere API documentation](https://docs.cohere.com/reference/chat).
- Note: The following parameters are recognized between OpenAI and Cohere. Most are removed in the example config below to prefer Cohere's default settings:
    - `stop`: mapped to `stopSequences`
    - `top_p`: mapped to `p`, different min/max values
    - `frequency_penalty`: mapped to `frequencyPenalty`, different min/max values
    - `presence_penalty`: mapped to `presencePenalty`, different min/max values
    - `model`: shared, included by default.
    - `stream`: shared, included by default.
    - `max_tokens`: shared, mapped to `maxTokens`, not included by default.


```yaml filename="librechat.yaml"
    - name: "cohere"
      apiKey: "${COHERE_API_KEY}"
      baseURL: "https://api.cohere.ai/v1"
      models:
        default: ["command-r","command-r-plus","command-light","command-light-nightly","command","command-nightly"]
        fetch: false
      modelDisplayLabel: "cohere"
      titleModel: "command"
      dropParams: ["stop", "user", "frequency_penalty", "presence_penalty", "temperature", "top_p"]
```

![image](https://github.com/danny-avila/LibreChat/assets/110412045/03549e00-243c-4539-ac9a-0d782af7cd6c)


================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/databricks.mdx
================================================
---
title: Databricks
description: Example configuration for Databricks
---

# [Databricks](https://www.databricks.com/)

> **[Sign up for Databricks]**(https://www.databricks.com/try-databricks#account)

**Notes:**

- Since Databricks provides a full completions endpoint, ending with "invocations" for "serving-endpoints", use of [directEndpoint](/docs/configuration/librechat_yaml/object_structure/custom_endpoint#directendpoint) setting is required.
- [titleMessageRole](/docs/configuration/librechat_yaml/object_structure/custom_endpoint#titlemessagerole) set to "user" is also required for title generation, as singular "system" messages are not supported.

```yaml filename="librechat.yaml"
    - name: 'Databricks'
      apiKey: '${DATABRICKS_API_KEY}'
      baseURL: 'https://your_databricks_serving_endpoint_url_here_ending_with/invocations'
      models:
        default: [
          "databricks-meta-llama-3-70b-instruct",
        ]
        fetch: false
      titleConvo: true
      titleModel: 'current_model'
      directEndpoint: true # required
      titleMessageRole: 'user' # required
```


================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/deepseek.mdx
================================================
---
title: Deepseek
description: Example configuration for Deepseek API
---

import Image from 'next/image'

# [Deepseek](https://www.deepseek.com/)

> Deepseek API key: [platform.deepseek.com](https://platform.deepseek.com/usage)

**Notes:**

- **Known:** icon provided.
- `deepseek-chat` and `deepseek-coder` are compatible with [Agents + tools](/docs/features/agents).
- `deepseek-reasoner`, codenamed "R1," is supported and will stream its "thought process"; however, certain OpenAI API parameters may not be supported by this specific model.
- "R1" may also be compatible with Agents when not using tools.
- `deepseek-chat` is preferred for title generation.

```yaml filename="librechat.yaml"
    - name: "Deepseek"
      apiKey: "${DEEPSEEK_API_KEY}"
      baseURL: "https://api.deepseek.com/v1"
      models:
        default: ["deepseek-chat", "deepseek-coder", "deepseek-reasoner"]
        fetch: false
      titleConvo: true
      titleModel: "deepseek-chat"
      modelDisplayLabel: "Deepseek"
```

<Image src="https://firebasestorage.googleapis.com/v0/b/superb-reporter-407417.appspot.com/o/chrome_GsVGKQ8aF3.png?alt=media&token=30cde5cd-3b62-428a-bd24-b58afff0e4bb" alt="Deepseek Generation" width={943} height={747}/>




================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/fireworks.mdx
================================================
---
title: Fireworks
description: Example configuration for Fireworks
---

# [Fireworks](https://fireworks.ai/)

> Fireworks API key: [fireworks.ai/api-keys](https://fireworks.ai/api-keys)

**Notes:**

- **Known:** icon provided, fetching list of models is recommended.
- - API may be strict for some models, and may not allow fields like `user`, in which case, you should use [`dropParams`.](/docs/configuration/librechat_yaml/object_structure/custom_endpoint#dropparams)

```yaml filename="librechat.yaml"
    - name: "Fireworks"
      apiKey: "${FIREWORKS_API_KEY}"
      baseURL: "https://api.fireworks.ai/inference/v1"
      models:
        default: [
          "accounts/fireworks/models/mixtral-8x7b-instruct",
          ]
        fetch: true
      titleConvo: true
      titleModel: "accounts/fireworks/models/llama-v2-7b-chat"
      summarize: false 
      summaryModel: "accounts/fireworks/models/llama-v2-7b-chat"
      forcePrompt: false
      modelDisplayLabel: "Fireworks"
      dropParams: ["user"]
```

![image](https://github.com/danny-avila/LibreChat/assets/32828263/e9254681-d4d8-43c7-a3c5-043c32a625a0)


================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/groq.mdx
================================================
---
title: Groq
description: Example configuration for Groq
---

# [Groq](https://wow.groq.com/)

> groq API key: [wow.groq.com](https://console.groq.com/keys)

**Notes:**

- **Known:** icon provided.

- **Temperature:** If you set a temperature value of 0, it will be converted to 1e-8. If you run into any issues, please try setting the value to a float32 greater than 0 and less than or equal to 2.

- Groq is currently free but rate limited: 10 queries/minute, 100/hour.

```yaml filename="librechat.yaml"
    - name: "groq"
      apiKey: "${GROQ_API_KEY}"
      baseURL: "https://api.groq.com/openai/v1/"
      models:
        default: [
          "llama3-70b-8192",
          "llama3-8b-8192",
          "llama2-70b-4096",
          "mixtral-8x7b-32768",
          "gemma-7b-it",
          ]
        fetch: false
      titleConvo: true
      titleModel: "mixtral-8x7b-32768"
      modelDisplayLabel: "groq"
```

![image](https://github.com/danny-avila/LibreChat/assets/110412045/cc4f0710-7e27-4f82-8b4f-81f788a6cb13)


================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/huggingface.mdx
================================================
---
title: Huggingface
description: Example configuration for Huggingface
---

# [Huggingface](https://huggingface.co)

> HuggingFace Token: [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)

**Notes:**

- **Known:** icon provided.

- The provided models are free but rate limited

  - The use of [`dropParams`](/docs/configuration/librechat_yaml/object_structure/custom_endpoint#dropparams) to drop "top_p" params is required.
  - Fetching models isn't supported
  - Note: Some models currently work better than others, answers are very short (at least when using the free tier).

- The example includes a model list, which was last updated on May 09, 2024, for your convenience.

```yaml
   - name: 'HuggingFace'
      apiKey: '${HUGGINGFACE_TOKEN}'
      baseURL: 'https://api-inference.huggingface.co/v1'
      models:
        default: [
          "codellama/CodeLlama-34b-Instruct-hf",
          "google/gemma-1.1-2b-it",
          "google/gemma-1.1-7b-it",
          "HuggingFaceH4/starchat2-15b-v0.1",
          "HuggingFaceH4/zephyr-7b-beta",
          "meta-llama/Meta-Llama-3-8B-Instruct",
          "microsoft/Phi-3-mini-4k-instruct",
          "mistralai/Mistral-7B-Instruct-v0.1",
          "mistralai/Mistral-7B-Instruct-v0.2",
          "mistralai/Mixtral-8x7B-Instruct-v0.1",
          "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
        ]
        fetch: true
      titleConvo: true
      titleModel: "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO"
      dropParams: ["top_p"]
      modelDisplayLabel: "HuggingFace"
```

<Callout type="warning" title="Other Model Errors" collapsible>
    Here’s a list of the other models that were tested along with their corresponding errors
    
    ```yaml
      models:
        default: [
          "CohereForAI/c4ai-command-r-plus", # Model requires a Pro subscription
          "HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1", # Model requires a Pro subscription
          "meta-llama/Llama-2-7b-hf", # Model requires a Pro subscription
          "meta-llama/Meta-Llama-3-70B-Instruct", # Model requires a Pro subscription
          "meta-llama/Llama-2-13b-chat-hf", # Model requires a Pro subscription
          "meta-llama/Llama-2-13b-hf", # Model requires a Pro subscription
          "meta-llama/Llama-2-70b-chat-hf", # Model requires a Pro subscription
          "meta-llama/Llama-2-7b-chat-hf", # Model requires a Pro subscription
          "------",
          "bigcode/octocoder", # template not found
          "bigcode/santacoder", # template not found
          "bigcode/starcoder2-15b", # template not found
          "bigcode/starcoder2-3b", # template not found 
          "codellama/CodeLlama-13b-hf", # template not found
          "codellama/CodeLlama-7b-hf", # template not found
          "google/gemma-2b", # template not found
          "google/gemma-7b", # template not found
          "HuggingFaceH4/starchat-beta", # template not found
          "HuggingFaceM4/idefics-80b-instruct", # template not found
          "HuggingFaceM4/idefics-9b-instruct", # template not found
          "HuggingFaceM4/idefics2-8b", # template not found
          "kashif/stack-llama-2", # template not found
          "lvwerra/starcoderbase-gsm8k", # template not found
          "tiiuae/falcon-7b", # template not found
          "timdettmers/guanaco-33b-merged", # template not found
          "------",
          "bigscience/bloom", # 404 status code (no body)
          "------",
          "google/gemma-2b-it", # stream` is not supported for this model / unknown error
          "------",
          "google/gemma-7b-it", # AI Response error likely caused by Google censor/filter
          "------",
          "bigcode/starcoder", # Service Unavailable
          "google/flan-t5-xxl", # Service Unavailable
          "HuggingFaceH4/zephyr-7b-alpha", # Service Unavailable
          "mistralai/Mistral-7B-v0.1", # Service Unavailable
          "OpenAssistant/oasst-sft-1-pythia-12b", # Service Unavailable
          "OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5", # Service Unavailable
        ]
    ```
</Callout>

![image](https://github.com/danny-avila/LibreChat/assets/32828263/191a3735-3acb-4ba7-917d-b930a933fc67)


================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/index.mdx
================================================
---
title: AI Endpoints
description: This section lists known, compatible AI Endpoints with example setups for the `librechat.yaml` AKA the LibreChat Custom Config file.
---

# Custom AI Endpoints

## Intro

This section lists known, compatible AI Endpoints, also known as "Custom Endpoints," with example setups for the `librechat.yaml` file, also known as the [Custom Config](/docs/configuration/librechat_yaml) file.

In all of the examples, arbitrary environment variable names are defined but you can use any name you wish, as well as changing the value to `user_provided` to allow users to submit their own API key from the web UI.

Some of the endpoints are marked as **Known,** which means they might have special handling and/or an icon already provided in the app for you.

⚠️ Important: make sure you setup the `librechat.yaml` file correctly: **[setup documentation](/docs/configuration/librechat_yaml/setup)**.



================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/litellm.mdx
================================================
---
title: LiteLLM
description: Example configuration for LiteLLM
---

# [LiteLLM](https://docs.litellm.ai/docs/)

**Notes:**

- Reference [Using LibreChat with LiteLLM Proxy](/blog/2023-11-30_litellm) for configuration.

```yaml filename="librechat.yaml"
    - name: "LiteLLM"
      apiKey: "sk-from-config-file"
      baseURL: "http://localhost:8000/v1"
      # if using LiteLLM example in docker-compose.override.yml.example, use "http://litellm:8000/v1"
      models:
        default: ["gpt-3.5-turbo"]
        fetch: true
      titleConvo: true
      titleModel: "gpt-3.5-turbo"
      summarize: false
      summaryModel: "gpt-3.5-turbo"
      forcePrompt: false
      modelDisplayLabel: "LiteLLM"
```

![image](https://github.com/danny-avila/LibreChat/assets/110412045/ddb4b2f3-608e-4034-9a27-3e94fc512034)


================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/mistral.mdx
================================================
---
title: Mistral
description: Example configuration for Mistral
---

# [Mistral AI](https://mistral.ai/)

> Mistral API key: [console.mistral.ai](https://console.mistral.ai/)

**Notes:**

- **Known:** icon provided, special handling of message roles: system message is only allowed at the top of the messages payload.

- API is strict with unrecognized parameters and errors are not descriptive (usually "no body")

    - The use of [`dropParams`](/docs/configuration/librechat_yaml/object_structure/custom_endpoint#dropparams) to drop "user", "frequency_penalty", "presence_penalty" params is required.
    - `stop` is no longer included as a default parameter, so there is no longer a need to include it in [`dropParams`](/docs/configuration/librechat_yaml/object_structure/custom_endpoint#dropparams), unless you would like to completely prevent users from configuring this field.
    
- Allows fetching the models list, but be careful not to use embedding models for chat.

```yaml filename="librechat.yaml"
    - name: "Mistral"
      apiKey: "${MISTRAL_API_KEY}"
      baseURL: "https://api.mistral.ai/v1"
      models:
        default: ["mistral-tiny", "mistral-small", "mistral-medium", "mistral -large-latest"]
        fetch: true
      titleConvo: true
      titleModel: "mistral-tiny"
      modelDisplayLabel: "Mistral"
      dropParams: ["stop", "user", "frequency_penalty", "presence_penalty"]
```

![image](https://github.com/danny-avila/LibreChat/assets/110412045/ddb4b2f3-608e-4034-9a27-3e94fc512034)


================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/mlx.mdx
================================================
---
title: Apple MLX
description: Example configuration for Apple MLX
---

# [Apple MLX](https://github.com/ml-explore/mlx)

> [MLX OpenAI Compatibility](https://github.com/ml-explore/mlx-examples/blob/main/llms/mlx_lm/SERVER.md)

**Notes:**

- **Known:** icon provided.

- API is mostly strict with unrecognized parameters.
- Support only one model at a time, otherwise you'll need to run a different endpoint with a different `baseURL`.

```yaml filename="librechat.yaml"
    - name: "MLX"
      apiKey: "mlx"
      baseURL: "http://localhost:8080/v1/" 
      models:
        default: [
          "Meta-Llama-3-8B-Instruct-4bit"
          ]
        fetch: false # fetching list of models is not supported
      titleConvo: true
      titleModel: "current_model"
      summarize: false
      summaryModel: "current_model"
      forcePrompt: false
      modelDisplayLabel: "Apple MLX"
      addParams:
            max_tokens: 2000
            "stop": [
              "<|eot_id|>"
            ]
```

![MLX](https://github.com/LibreChat-AI/librechat.ai/assets/32828263/e5765729-5ee4-4dbc-b553-df1684486a23)



================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/neurochain.mdx
================================================
---
title: NeurochainAI
description: Example configuration for NeurochainAI
---

# [NeurochainAI](https://neurochain.ai)

> NeurochainAI API key: Required - [NeurochainAI REST API Documentation](https://app.neurochain.ai/network-integrations/rest-api)

**Notes:**

- Api is based on the OpenAI API.
- Model list is constantly growing and may not be up-to-date in this example. Keep an eye on the [NeurochainAI](https://neurochain.ai) for the latest models.
- The example includes the NeurochainAI model: "Mistral-7B-OpenOrca-GPTQ".

```yaml filename="neurochainai.yaml"
    - name: "NeurochainAI"
      apiKey: "<=generated api key=>"
      baseURL: "https://ncmb.neurochain.io/v1/"
      models:
        default: [
          "Mistral-7B-OpenOrca-GPTQ"
        ]
        fetch: true
      titleConvo: true
      titleModel: "current_model"
      summarize: false
      summaryModel: "current_model"
      forcePrompt: false
      modelDisplayLabel: "NeurochainAI"
      iconURL: "https://raw.githubusercontent.com/LibreChat-AI/librechat-config-yaml/refs/heads/main/icons/NeurochainAI.png"
```



================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/ollama.mdx
================================================
---
title: Ollama
description: Example configuration for Ollama
---

# [Ollama](https://ollama.com/)

> Ollama API key: Required but ignored - [Ollama OpenAI Compatibility](https://github.com/ollama/ollama/blob/main/docs/openai.md)

**Notes:**

- **Known:** icon provided.
- Download models with ollama run command. See [Ollama Library](https://ollama.com/library)
- It's recommend to use the value "current_model" for the `titleModel` to avoid loading more than 1 model per conversation.
    - Doing so will dynamically use the current conversation model for the title generation.
- The example includes a top 5 popular model list from the Ollama Library, which was last updated on March 1, 2024, for your convenience.

```yaml filename="librechat.yaml"
  custom:
    - name: "Ollama"
      apiKey: "ollama"
      # use 'host.docker.internal' instead of localhost if running LibreChat in a docker container
      baseURL: "http://localhost:11434/v1/chat/completions" 
      models:
        default: [
          "llama2",
          "mistral",
          "codellama",
          "dolphin-mixtral",
          "mistral-openorca"
          ]
        # fetching list of models is supported but the `name` field must start
        # with `ollama` (case-insensitive), as it does in this example.
        fetch: true
      titleConvo: true
      titleModel: "current_model"
      summarize: false
      summaryModel: "current_model"
      forcePrompt: false
      modelDisplayLabel: "Ollama"
```

<Callout type="tip" title="Ollama -> llama3">
    
Note: Once `stop` was removed from the [default parameters](/docs/configuration/librechat_yaml/object_structure/default_params), the issue highlighted below should no longer exist.

However, in case you experience the behavior where `llama3` does not stop generating, add this `addParams` block to the config:

```yaml filename="librechat.yaml"
  custom:
    - name: "Ollama"
      apiKey: "ollama"
      baseURL: "http://host.docker.internal:11434/v1/"
      models:
        default: [
          "llama3"
        ]
        fetch: false # fetching list of models is not supported
      titleConvo: true
      titleModel: "current_model"
      summarize: false
      summaryModel: "current_model"
      forcePrompt: false
      modelDisplayLabel: "Ollama"
      addParams:
          "stop": [
              "<|start_header_id|>",
              "<|end_header_id|>",
              "<|eot_id|>",
              "<|reserved_special_token"
          ]
```

If you are only using `llama3` with **Ollama**, it's fine to set the `stop` parameter at the config level via `addParams`.

However, if you are using multiple models, it's now recommended to add stop sequences from the frontend via conversation parameters and presets.

For example, we can omit `addParams`:

```yaml filename="librechat.yaml"
- name: "Ollama"
    apiKey: "ollama"
    baseURL: "http://host.docker.internal:11434/v1/" 
    models:
    default: [
        "llama3:latest",
        "mistral"
        ]
    fetch: false # fetching list of models is not supported
    titleConvo: true
    titleModel: "current_model"
    modelDisplayLabel: "Ollama"
```

And use these settings (best to also save it):

![image](https://github.com/danny-avila/LibreChat/assets/110412045/57460b8c-308a-4d21-9dfe-f48a2ac85099)

</Callout>



================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/openrouter.mdx
================================================
---
title: Openrouter
description: Example configuration for Openrouter
---

# [Openrouter](https://openrouter.ai/)

> OpenRouter API key: [openrouter.ai/keys](https://openrouter.ai/keys)

**Notes:**

- **Known:** icon provided, fetching list of models is recommended as API token rates and pricing used for token credit balances when models are fetched.

- `stop` is no longer included as a default parameter, so there is no longer a need to include it in [`dropParams`](/docs/configuration/librechat_yaml/object_structure/custom_endpoint#dropparams), unless you would like to completely prevent users from configuring this field.

- **Known issue:** you should not use `OPENROUTER_API_KEY` as it will then override the `openAI` endpoint to use OpenRouter as well.

```yaml
    - name: "OpenRouter"
      # For `apiKey` and `baseURL`, you can use environment variables that you define.
      # recommended environment variables:
      apiKey: "${OPENROUTER_KEY}" # NOT OPENROUTER_API_KEY
      baseURL: "https://openrouter.ai/api/v1"
      models:
        default: ["meta-llama/llama-3-70b-instruct"]
        fetch: true
      titleConvo: true
      titleModel: "meta-llama/llama-3-70b-instruct"
      # Recommended: Drop the stop parameter from the request as Openrouter models use a variety of stop tokens.
      dropParams: ["stop"]
      modelDisplayLabel: "OpenRouter"
```

![image](https://github.com/danny-avila/LibreChat/assets/110412045/c4a0415e-732c-46af-82a6-3598663b7f42)




================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/perplexity.mdx
================================================
---
title: Perplexity
description: Example configuration for Perplexity
---

# [Perplexity](https://www.perplexity.ai)

> Perplexity API key: [perplexity.ai/settings/api](https://www.perplexity.ai/settings/api)

**Notes:**

- **Known:** icon provided.
- **Known issue:** fetching list of models is not supported.
- API may be strict for some models, and may not allow fields like `stop` and `frequency_penalty` may cause an error when set to 0, in which case, you should use [`dropParams`.](/docs/configuration/librechat_yaml/object_structure/custom_endpoint#dropparams)
- The example includes a model list, which was last updated on 3 July 2024, for your convenience.

```yaml
    - name: "Perplexity"
      apiKey: "${PERPLEXITY_API_KEY}"
      baseURL: "https://api.perplexity.ai/"
      models:
        default: [
          "sonar-deep-research",
          "sonar-reasoning-pro",
          "sonar-reasoning",
          "sonar-pro",
          "sonar",
          "r1-1776"
          ]
        fetch: false # fetching list of models is not supported
      titleConvo: true
      titleModel: "llama-3-sonar-small-32k-chat"
      summarize: false
      summaryModel: "llama-3-sonar-small-32k-chat"
      forcePrompt: false
      dropParams: ["stop", "frequency_penalty"]
      modelDisplayLabel: "Perplexity"
```

![image](https://github.com/danny-avila/LibreChat/assets/32828263/6bf6c121-0895-4210-a1dd-e5e957992fd4)



================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/portkey.mdx
================================================
---
title: Portkey AI
description: Example configuration for Portkey AI
---

# [Portkey Docs](https://docs.portkey.ai/docs/integrations/libraries/librechat)

> Portkey API key: [app.portkey.ai](https://app.portkey.ai/)



**Notes:**
- LibreChat requires that the `API key` field is present. We don’t need it for the Portkey integration, we can pass a `dummy` string for it.
- Portkey integrates with LibreChat, offering observability, 50+ guardrails, caching, and conditional routing with fallbacks and retries for reliable, production-grade deployments.
- Portkey Supports 250+ Models, you can find the list of providers on [Portkey Docs](https://docs.portkey.ai/docs/integrations/llms)
- You can use Portkey in two ways—through [Virtual Keys](https://docs.portkey.ai/docs/product/ai-gateway/virtual-keys  ) or [Configs](https://docs.portkey.ai/docs/product/ai-gateway/configs).
- You can use Model specific parameters like `top_p`, `max Tokens`, etc. in using Portke Config. [Learn more](https://docs.portkey.ai/docs/product/ai-gateway/configs#configs)



## Using Portkey AI with Virtual Keys
```yaml filename="librechat.yaml"
      - name: "Portkey"
        apiKey: "dummy"  
        baseURL: ${PORTKEY_GATEWAY_URL}
        headers:
            x-portkey-api-key: "${PORTKEY_API_KEY}"
            x-portkey-virtual-key: "PORTKEY_OPENAI_VIRTUAL_KEY"
        models:
            default: ["gpt-4o-mini"]
            fetch: true
        titleConvo: true
        titleModel: "current_model"
        summarize: false
        summaryModel: "current_model"
        forcePrompt: false
        modelDisplayLabel: "Portkey:OpenAI"
        iconURL: https://images.crunchbase.com/image/upload/c_pad,f_auto,q_auto:eco,dpr_1/rjqy7ghvjoiu4cd1xjbf
```



## Using Portkey AI with Configs
```yaml filename="librechat.yaml"
    - name: "Portkey"
      apikey: "dummy"
      baseURL: ${PORTKEY_GATEWAY_URL}
      headers:
        x-portkey-api-key: "${PORTKEY_API_KEY}"
        x-portkey-config: "pc-libre-xxx"
      models:
        default: ["llama-3.2"]
        fetch: true
      titleConvo: true
      titleModel: "current_model"
      summarize: false
      summaryModel: "current_model"
      forcePrompt: false
      modelDisplayLabel: "Portkey:Llama"
      iconURL: https://images.crunchbase.com/image/upload/c_pad,f_auto,q_auto:eco,dpr_1/rjqy7ghvjoiu4cd1xjbf
```

![image](https://raw.githubusercontent.com/siddharthsambharia-portkey/Portkey-Product-Images/refs/heads/main/portkey-librechat.png)



================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/shuttleai.mdx
================================================
---
title: ShuttleAI
description: Example configuration for ShuttleAI
---

# [ShuttleAI](https://shuttleai.com/)

> ShuttleAI API key: [shuttleai.com/keys](https://shuttleai.com/keys)

**Notes:**

- **Known:** icon provided, fetching list of models is recommended.

```yaml
    - name: "ShuttleAI"
      apiKey: "${SHUTTLEAI_API_KEY}"
      baseURL: "https://api.shuttleai.com/v1"
      models:
        default: [
          "shuttle-2.5", "shuttle-2.5-mini"
          ]
        fetch: true
      titleConvo: true
      titleModel: "shuttle-2.5-mini"
      summarize: false
      summaryModel: "shuttle-2.5-mini"
      forcePrompt: false
      modelDisplayLabel: "ShuttleAI"
      dropParams: ["user", "stop"]
```

![image](https://github.com/danny-avila/LibreChat/assets/32828263/a694e6d0-5663-4c89-92b5-887742dca876)



================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/togetherai.mdx
================================================
---
title: together.ai
description: Example configuration for together.ai
---

# [together.ai](https://api.together.xyz/)

> together.ai API key: [api.together.xyz/settings/api-keys](https://api.together.xyz/settings/api-keys)

**Notes:**

- **Known:** icon provided.
- **Known issue:** fetching list of models is not supported.
- The example includes a model list, which was last updated on August 1, 2024, for your convenience.

<Callout type="tip" title="Fetch and order the models" collapsible>
This python script can fetch and order the llm models for you. The output will be saved in models.txt, formated in a way that should make it easier for you to include in the yaml config.

```py filename="fetch_togetherai.py"
import json

import requests

# API key
api_key = ""

# API endpoint
url = "https://api.together.xyz/v1/models"

# headers
headers = {
    "accept": "application/json",
    "Authorization": f"Bearer {api_key}"
}

# make request
response = requests.get(url, headers=headers)

# parse JSON response
data = response.json()

# extract an ordered list of unique model IDs
model_ids = sorted(
    [
        model['id']
        for model in data
        if model['type'] == 'chat'
    ]
)

# write result to a text file
with open("models_togetherai.json", "w") as file:
    json.dump(model_ids, file, indent=2)
```
</Callout>

```yaml
    - name: "together.ai"
      apiKey: "${TOGETHERAI_API_KEY}"
      baseURL: "https://api.together.xyz"
      models:
        default: [
          "Austism/chronos-hermes-13b",
          "Gryphe/MythoMax-L2-13b",
          "HuggingFaceH4/zephyr-7b-beta",
          "NousResearch/Hermes-2-Theta-Llama-3-70B",
          "NousResearch/Nous-Capybara-7B-V1p9",
          "NousResearch/Nous-Hermes-2-Mistral-7B-DPO",
          "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
          "NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT",
          "NousResearch/Nous-Hermes-2-Yi-34B",
          "NousResearch/Nous-Hermes-Llama2-13b",
          "NousResearch/Nous-Hermes-Llama2-70b",
          "NousResearch/Nous-Hermes-llama-2-7b",
          "Open-Orca/Mistral-7B-OpenOrca",
          "Qwen/Qwen1.5-0.5B-Chat",
          "Qwen/Qwen1.5-1.8B-Chat",
          "Qwen/Qwen1.5-110B-Chat",
          "Qwen/Qwen1.5-14B-Chat",
          "Qwen/Qwen1.5-32B-Chat",
          "Qwen/Qwen1.5-4B-Chat",
          "Qwen/Qwen1.5-72B-Chat",
          "Qwen/Qwen1.5-7B-Chat",
          "Qwen/Qwen2-1.5B",
          "Qwen/Qwen2-1.5B-Instruct",
          "Qwen/Qwen2-72B",
          "Qwen/Qwen2-72B-Instruct",
          "Qwen/Qwen2-7B",
          "Qwen/Qwen2-7B-Instruct",
          "Snowflake/snowflake-arctic-instruct",
          "Undi95/ReMM-SLERP-L2-13B",
          "Undi95/Toppy-M-7B",
          "WizardLM/WizardLM-13B-V1.2",
          "allenai/OLMo-7B-Instruct",
          "carson/ml31405bit",
          "carson/ml3170bit",
          "carson/ml318bit",
          "carson/ml318br",
          "codellama/CodeLlama-13b-Instruct-hf",
          "codellama/CodeLlama-34b-Instruct-hf",
          "codellama/CodeLlama-70b-Instruct-hf",
          "codellama/CodeLlama-7b-Instruct-hf",
          "cognitivecomputations/dolphin-2.5-mixtral-8x7b",
          "databricks/dbrx-instruct",
          "deepseek-ai/deepseek-coder-33b-instruct",
          "deepseek-ai/deepseek-llm-67b-chat",
          "garage-bAInd/Platypus2-70B-instruct",
          "google/gemma-2-27b-it",
          "google/gemma-2-9b-it",
          "google/gemma-2b-it",
          "google/gemma-7b-it",
          "gradientai/Llama-3-70B-Instruct-Gradient-1048k",
          "lmsys/vicuna-13b-v1.3",
          "lmsys/vicuna-13b-v1.5",
          "lmsys/vicuna-13b-v1.5-16k",
          "lmsys/vicuna-7b-v1.3",
          "lmsys/vicuna-7b-v1.5",
          "meta-llama/Llama-2-13b-chat-hf",
          "meta-llama/Llama-2-70b-chat-hf",
          "meta-llama/Llama-2-7b-chat-hf",
          "meta-llama/Llama-3-70b-chat-hf",
          "meta-llama/Llama-3-8b-chat-hf",
          "meta-llama/Meta-Llama-3-70B-Instruct",
          "meta-llama/Meta-Llama-3-70B-Instruct-Lite",
          "meta-llama/Meta-Llama-3-70B-Instruct-Turbo",
          "meta-llama/Meta-Llama-3-8B-Instruct",
          "meta-llama/Meta-Llama-3-8B-Instruct-Lite",
          "meta-llama/Meta-Llama-3-8B-Instruct-Turbo",
          "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
          "meta-llama/Meta-Llama-3.1-70B-Instruct-Reference",
          "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
          "meta-llama/Meta-Llama-3.1-70B-Reference",
          "meta-llama/Meta-Llama-3.1-8B-Instruct-Reference",
          "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
          "microsoft/WizardLM-2-8x22B",
          "mistralai/Mistral-7B-Instruct-v0.1",
          "mistralai/Mistral-7B-Instruct-v0.2",
          "mistralai/Mistral-7B-Instruct-v0.3",
          "mistralai/Mixtral-8x22B-Instruct-v0.1",
          "mistralai/Mixtral-8x7B-Instruct-v0.1",
          "openchat/openchat-3.5-1210",
          "snorkelai/Snorkel-Mistral-PairRM-DPO",
          "teknium/OpenHermes-2-Mistral-7B",
          "teknium/OpenHermes-2p5-Mistral-7B",
          "togethercomputer/CodeLlama-13b-Instruct",
          "togethercomputer/CodeLlama-34b-Instruct",
          "togethercomputer/CodeLlama-7b-Instruct",
          "togethercomputer/Koala-13B",
          "togethercomputer/Koala-7B",
          "togethercomputer/Llama-2-7B-32K-Instruct",
          "togethercomputer/Llama-3-8b-chat-hf-int4",
          "togethercomputer/Llama-3-8b-chat-hf-int8",
          "togethercomputer/SOLAR-10.7B-Instruct-v1.0-int4",
          "togethercomputer/StripedHyena-Nous-7B",
          "togethercomputer/alpaca-7b",
          "togethercomputer/guanaco-13b",
          "togethercomputer/guanaco-33b",
          "togethercomputer/guanaco-65b",
          "togethercomputer/guanaco-7b",
          "togethercomputer/llama-2-13b-chat",
          "togethercomputer/llama-2-70b-chat",
          "togethercomputer/llama-2-7b-chat",
          "upstage/SOLAR-10.7B-Instruct-v1.0",
          "zero-one-ai/Yi-34B-Chat"
        ]
        fetch: false # fetching list of models is not supported
      titleConvo: true
      titleModel: "togethercomputer/llama-2-7b-chat"
      summarize: false
      summaryModel: "togethercomputer/llama-2-7b-chat"
      forcePrompt: false
      modelDisplayLabel: "together.ai"
```


================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/vultrcloudinference.mdx
================================================
---
title: Vultr Cloud Inference
description: Example configuration for Vultr Cloud Inference
---

# [Vultr Cloud Inference](https://my.vultr.com/inference/)

> Vultr Cloud Inference API key: Required - [Vultr Cloud Inference](https://docs.vultr.com/vultr-cloud-inference)

**Notes:**

- The example includes the 4 models optimized for chat, which was last updated on June 28, 2024, for your convenience.
- The only model currently supporting title generation is llama2-7b-chat-Q5_K_M.gguf.

```yaml filename="librechat.yaml"
  custom:
    - name: 'Vultr Cloud Inference'
      apiKey: '${VULTRINFERENCE_TOKEN}'
      baseURL: 'https://api.vultrinference.com/v1/chat/completions'
      models:
        default: [
          "llama2-7b-chat-Q5_K_M.gguf",
          "llama2-13b-chat-Q5_K_M.gguf",
          "mistral-7b-Q5_K_M.gguf",
          "zephyr-7b-beta-Q5_K_M.gguf",
        ]
        fetch: true
      titleConvo: true
      titleModel: "llama2-7b-chat-Q5_K_M.gguf"
      modelDisplayLabel: "Vultr Cloud Inference"
```



================================================
FILE: pages/docs/configuration/librechat_yaml/ai_endpoints/xai.mdx
================================================
---
title: xAI
description: Example configuration for xAI
---

# [xAI](https://console.x.ai/)

> xAI API key: [x.ai](https://console.x.ai/)

**Notes:**

- **Known:** icon provided.

```yaml filename="librechat.yaml"
    - name: "xai"
      apiKey: "${XAI_API_KEY}"
      baseURL: "https://api.x.ai/v1"
      models:
        default: ["grok-beta"]
        fetch: false
      titleConvo: true
      titleMethod: "completion"
      titleModel: "grok-beta"
      summarize: false
      summaryModel: "grok-beta"
      forcePrompt: false
      modelDisplayLabel: "Grok"
```

![image](https://github.com/user-attachments/assets/815173c4-489f-4944-b7bf-5602e4a6e8e8)


================================================
FILE: pages/docs/configuration/librechat_yaml/object_structure/_meta.ts
================================================
export default {
  config: 'Root Settings',
  file_config: 'File Config',
  interface: 'Interface (UI)',
  model_specs: 'Model Specs',
  registration: 'Registration',
  balance: 'Balance',
  agents: 'Agents',
  mcp_servers: 'MCP Servers',
  aws_bedrock: 'AWS Bedrock',
  assistants_endpoint: 'Assistants API',
  custom_endpoint: 'Custom Endpoint',
  azure_openai: 'Azure OpenAI',
  model_config: 'Azure Model Config',
  default_params: 'API Default Parameters',
}



================================================
FILE: pages/docs/configuration/librechat_yaml/object_structure/actions.mdx
================================================
# Actions Object Structure

Actions can be used to dynamically create tools from OpenAPI specs. The `actions` object structure allows you to specify allowed domains for agent/assistant actions.

More info: [Agents - Actions](/docs/features/agents#actions)

## Example

```yaml filename="Actions Object Structure"
# Example Actions Object Structure
actions:
  allowedDomains:
    - "swapi.dev"
    - "librechat.ai"
    - "google.com"
```

## allowedDomains

**Key:**
<OptionTable
  options={[
    ['allowedDomains', 'Array of Strings', 'A list specifying allowed domains for agent/assistant actions.', 'Actions with domains not listed will be restricted from executing.'],
  ]}
/>

**Required**

**Example:**
```yaml filename="actions / allowedDomains"
allowedDomains:
  - "swapi.dev"
  - "librechat.ai"
  - "google.com"
```


================================================
FILE: pages/docs/configuration/librechat_yaml/object_structure/agents.mdx
================================================
# Agents Endpoint Object Structure

This page applies to the [`agents`](/docs/features/agents) endpoint.

## Example

```yaml filename="Agents Endpoint"
endpoints:
  agents:
    recursionLimit: 50
    maxRecursionLimit: 100
    disableBuilder: false
    # (optional) Agent Capabilities available to all users. Omit the ones you wish to exclude. Defaults to list below.
    # capabilities: ["execute_code", "file_search", "actions", "tools", "artifacts", "ocr", "chain"]
```
> This configuration enables the builder interface for agents.

## recursionLimit

<OptionTable
  options={[
    ['recursionLimit', 'Number', 'Sets the default number of steps an agent can take in a run.', 'Controls recursion depth to prevent infinite loops. When limit is reached, raises GraphRecursionError. This value can be configured from the UI up to the maxRecursionLimit.'],
  ]}
/>

**Default:** `25`

**Example:**
```yaml filename="endpoints / agents / recursionLimit"
recursionLimit: 50
```

For more information about agent steps, see [Max Agent Steps](/docs/features/agents#max-agent-steps).

## maxRecursionLimit

<OptionTable
  options={[
    ['maxRecursionLimit', 'Number', 'Sets the absolute maximum number of steps an agent can take in a run.', 'Defines the upper limit for the recursionLimit that can be set from the UI. This prevents users from setting excessively high values.'],
  ]}
/>

**Default:** If omitted, defaults to the value of recursionLimit or 25 if recursionLimit is also omitted.

**Example:**
```yaml filename="endpoints / agents / maxRecursionLimit"
maxRecursionLimit: 100
```

For more information about agent steps, see [Max Agent Steps](/docs/features/agents#max-agent-steps).

## disableBuilder

<OptionTable
  options={[
    ['disableBuilder', 'Boolean', 'Controls the visibility and use of the builder interface for agents.', 'When set to `true`, disables the builder interface for the agent, limiting direct manual interaction.'],
  ]}
/>

**Default:** `false`

**Example:**
```yaml filename="endpoints / agents / disableBuilder"
disableBuilder: false
```

## allowedProviders

<OptionTable
  options={[
    ['allowedProviders', 'Array/List of Strings', 'Specifies a list of endpoint providers (e.g., "openAI", "anthropic", "google") that are permitted for use with the Agents feature.', 'If defined, only agents configured with these providers can be initialized. If omitted or empty, all configured providers are allowed.'],
  ]}
/>

**Default:** `[]` (empty list, all providers allowed)

**Note:** Must be one of the following:
    - `openAI, azureOpenAI, google, anthropic, assistants, azureAssistants, bedrock`

**Example:**
```yaml filename="endpoints / agents / allowedProviders"
allowedProviders:
  - openAI
  - google
```

## capabilities

<OptionTable
  options={[
    ['capabilities', 'Array/List of Strings', 'Specifies the agent capabilities available to all users for the agents endpoint.', 'Defines the agent capabilities that are available to all users for the agents endpoint. You can omit the capabilities you wish to exclude from the list.'],
  ]}
/>

**Default:** `["execute_code", "file_search", "actions", "tools", "artifacts", "ocr", "chain"]`

**Example:**
```yaml filename="endpoints / agents / capabilities"
capabilities:
  - "execute_code"
  - "file_search"
  - "actions"
  - "tools"
  - "artifacts"
  - "ocr"
  - "chain"
```
**Note:** This field is optional. If omitted, the default behavior is to include all the capabilities listed in the default.

## Agent Capabilities

The `capabilities` field allows you to enable or disable specific functionalities for agents. The available capabilities are:

- **execute_code**: Allows the agent to execute code.
- **file_search**: Enables the agent to search and interact with files.
- **actions**: Permits the agent to perform predefined actions.
- **tools**: Grants the agent access to various tools.
- **ocr**: Enables uploading files as additional context, leveraging Optical Character Recognition for extracting text from images and documents.

By specifying the capabilities, you can control the features available to users when interacting with agents.

## Example Configuration

Here is an example of configuring the `agents` endpoint with custom capabilities:

```yaml filename="Agents Endpoint"
endpoints:
  agents:
    disableBuilder: false
    capabilities:
      - "execute_code"
      - "actions"
      - "artifacts"
      - "ocr"
```

In this example, the builder interface for agents is disabled, and only the `execute_code`, `actions`, and `ocr` capabilities are enabled.

## Notes

- It's not recommended to disable the builder interface unless you are using [modelSpecs](/docs/configuration/librechat_yaml/object_structure/model_specs) to define a list of agents to choose from.



================================================
FILE: pages/docs/configuration/librechat_yaml/object_structure/assistants_endpoint.mdx
================================================
# Assistants Endpoint Object Structure

This page applies to both the `assistants` and `azureAssistants` endpoints.

**Note:** To enable `azureAssistants`, see the [Azure OpenAI Configuration](/docs/configuration/azure#using-assistants-with-azure) for more information.

## Example

```yaml filename="Assistants Endpoint"
endpoints:
  # azureAssistants: # <-- Azure-specific configuration has the same structure as `assistants`
    #  pollIntervalMs: 500
    #  timeoutMs: 10000

  assistants:
    disableBuilder: false
    # Use either `supportedIds` or `excludedIds` but not both
    supportedIds: ["asst_supportedAssistantId1", "asst_supportedAssistantId2"]
    # excludedIds: ["asst_excludedAssistantId"]
    # `privateAssistants` do not work with `supportedIds` or `excludedIds`
    # privateAssistants: false
    # (optional) Models that support retrieval, will default to latest known OpenAI models that support the feature
    # retrievalModels: ["gpt-4-turbo-preview"]
    # (optional) Assistant Capabilities available to all users. Omit the ones you wish to exclude. Defaults to list below.
    # capabilities: ["code_interpreter", "retrieval", "actions", "tools", "image_vision"]
```
> This configuration enables the builder interface for assistants, sets a polling interval of 500ms to check for run updates, and establishes a timeout of 10 seconds for assistant run operations.

## disableBuilder

**Key:**
<OptionTable
  options={[
    ['disableBuilder', 'Boolean', 'Controls the visibility and use of the builder interface for assistants.', 'When set to `true`, disables the builder interface for the assistant, limiting direct manual interaction.'],
  ]}
/>

**Default:** `false`

**Example:**
```yaml filename="endpoints / assistants / disableBuilder"
disableBuilder: false
```

## pollIntervalMs

**Key:**
<OptionTable
  options={[
    ['pollIntervalMs', 'Integer', 'Specifies the polling interval in milliseconds for checking run updates or changes in assistant run states.', 'Specifies the polling interval in milliseconds for checking assistant run updates.'],
  ]}
/>

**Default:** `2000`

**Example:**
```yaml filename="endpoints / assistants / pollIntervalMs"
pollIntervalMs: 2500
```
**Note:** Currently, this is only used by Azure Assistants. Higher values are recommended for Azure Assistants to avoid rate limiting errors.

## timeoutMs

**Key:**
<OptionTable
  options={[
    ['timeoutMs', 'Integer', 'Defines the maximum time in milliseconds that an assistant can run before the request is cancelled.', 'Sets a timeout in milliseconds for assistant runs. Helps manage system load by limiting total run operation time.'],
  ]}
/>

**Default:** `180000`

**Example:**
```yaml filename="endpoints / assistants / timeoutMs"
timeoutMs: 10000
```
**Note:** Defaults to 3 minutes (180,000 ms). Run operation times can range between 50 seconds to 2 minutes but also exceed this. If the `timeoutMs` value is exceeded, the run will be cancelled.

## supportedIds

**Key:**
<OptionTable
  options={[
    ['supportedIds', 'Array/List of Strings', 'List of supported assistant Ids', 'Use this or `excludedIds` but not both (the `excludedIds` field will be ignored if so).'],
  ]}
/>

**Example:**
```yaml filename="endpoints / assistants / supportedIds"
supportedIds:
  - "asst_supportedAssistantId1"
  - "asst_supportedAssistantId2"
```

## excludedIds

**Key:**
<OptionTable
  options={[
    ['excludedIds', 'Array/List of Strings', 'List of excluded assistant Ids', 'Use this or `supportedIds` but not both (the `excludedIds` field will be ignored if so).'],
  ]}
/>

**Example:**
```yaml filename="endpoints / assistants / excludedIds"
excludedIds:
  - "asst_excludedAssistantId1"
  - "asst_excludedAssistantId2"
```

## privateAssistants

**Key:**
<OptionTable
  options={[
    ['privateAssistants', 'Boolean', 'Controls whether assistants are private to the user that created them', 'Does not work with `supportedIds` or `excludedIds` (`supportedIds` and `excludedIds` will be ignored).'],
  ]}
/>

**Default:** `false`

**Example:**
```yaml filename="endpoints / assistants / privateAssistants"
privateAssistants: false
```

## retrievalModels

**Key:**
<OptionTable
  options={[
    ['retrievalModels', 'Array/List of Strings', 'Specifies the models that support retrieval for the assistants endpoint.', 'Defines the models that support retrieval capabilities for the assistants endpoint. By default, it uses the latest known OpenAI models that support the official Retrieval feature.'],
  ]}
/>

**Default:** `[]` (uses the latest known OpenAI models that support retrieval)

**Example:**
```yaml filename="endpoints / assistants / retrievalModels"
retrievalModels:
  - "gpt-4-turbo-preview"
```

## capabilities

**Key:**
<OptionTable
  options={[
    ['capabilities', 'Array/List of Strings', 'Specifies the assistant capabilities available to all users for the assistants endpoint.', 'Defines the assistant capabilities that are available to all users for the assistants endpoint. You can omit the capabilities you wish to exclude from the list.'],
  ]}
/>

**Default:** `["code_interpreter", "image_vision", "retrieval", "actions", "tools"]`

**Example:**
```yaml filename="endpoints / assistants / capabilities"
capabilities:
  - "code_interpreter"
  - "retrieval"
  - "actions"
  - "tools"
  - "image_vision"
```
**Note:** This field is optional. If omitted, the default behavior is to include all the capabilities listed in the example.


================================================
FILE: pages/docs/configuration/librechat_yaml/object_structure/aws_bedrock.mdx
================================================
# AWS Bedrock Object Structure

Integrating AWS Bedrock with your application allows you to seamlessly utilize multiple AI models hosted on AWS. This section details how to configure the AWS Bedrock endpoint for your needs.

## Example Configuration

```yaml filename="Example AWS Bedrock Object Structure"
endpoints:
  bedrock:
    titleModel: "anthropic.claude-3-haiku-20240307-v1:0"
    streamRate: 35
    availableRegions:
      - "us-east-1"
      - "us-west-2"
```

## titleModel

**Key:**
<OptionTable
  options={[
    ['titleModel', 'String', 'Specifies the model to use for generating conversation titles.', 'Recommended: anthropic.claude-3-haiku-20240307-v1:0. Set to "current_model" to use the same model as the chat.'],
  ]}
/>

**Default:** Not specified

**Example:**
```yaml filename="titleModel"
titleModel: "anthropic.claude-3-haiku-20240307-v1:0"
```

## streamRate

**Key:**
<OptionTable
  options={[
    ['streamRate', 'Number', 'Sets the rate of processing each new token in milliseconds.', 'This can help stabilize processing of concurrent requests and provide smoother frontend stream rendering.'],
  ]}
/>

**Default:** Not specified

**Example:**
```yaml filename="streamRate"
streamRate: 35
```

## availableRegions

**Key:**
<OptionTable
  options={[
    ['availableRegions', 'Array', 'Specifies the AWS regions you want to make available for Bedrock.', 'If provided, users will see a dropdown to select the region. If not selected, the default region is used.'],
  ]}
/>

**Default:** Not specified

**Example:**
```yaml filename="availableRegions"
availableRegions:
  - "us-east-1"
  - "us-west-2"
```

## Notes

- The main configuration for AWS Bedrock is done through environment variables, additional forms of authentication are in development.


================================================
FILE: pages/docs/configuration/librechat_yaml/object_structure/azure_openai.mdx
================================================
# Azure OpenAI Object Structure

Integrating Azure OpenAI Service with your application allows you to seamlessly utilize multiple deployments and region models hosted by Azure OpenAI. This section details how to configure the Azure OpenAI endpoint for your needs. 

**[For a detailed guide on setting up Azure OpenAI configurations, click here](/docs/configuration/azure)**

## Example Configuration

```yaml filename="Example Azure OpenAI Object Structure"
endpoints:
  azureOpenAI:
    titleModel: "gpt-4-turbo"
    plugins: true
    groups:
      - group: "my-westus" # arbitrary name
        apiKey: "${WESTUS_API_KEY}"
        instanceName: "actual-instance-name" # name of the resource group or instance
        version: "2023-12-01-preview"
        # baseURL: https://prod.example.com
        # additionalHeaders:
        #   X-Custom-Header: value
        models:
          gpt-4-vision-preview:
            deploymentName: gpt-4-vision-preview
            version: "2024-02-15-preview"
          gpt-3.5-turbo:
            deploymentName: gpt-35-turbo
          gpt-3.5-turbo-1106:
            deploymentName: gpt-35-turbo-1106
          gpt-4:
            deploymentName: gpt-4
          gpt-4-1106-preview:
            deploymentName: gpt-4-1106-preview
      - group: "my-eastus"
        apiKey: "${EASTUS_API_KEY}"
        instanceName: "actual-eastus-instance-name"
        deploymentName: gpt-4-turbo
        version: "2024-02-15-preview"
        baseURL: "https://gateway.ai.cloudflare.com/v1/cloudflareId/azure/azure-openai/${INSTANCE_NAME}/${DEPLOYMENT_NAME}" # uses env variables
        additionalHeaders:
          X-Custom-Header: value
        models:
          gpt-4-turbo: true
```

## plugins

**Key:**
<OptionTable
  options={[
    ['plugins', 'Boolean', 'Enables or disables plugins for the Azure OpenAI endpoint. When set to `true`, it activates any plugins associated with this endpoint, allowing the system to use additional features or functionality provided by the plugins.', 'Choose one, either the official OpenAI API or Azure OpenAI API for plugins, not both.'],
  ]}
/>

**Default:** Not specified

**Example:**
```yaml filename="plugins"
plugins: true
```

## assistants

**Key:**
<OptionTable
  options={[
    ['assistants', 'Boolean', 'Enables or disables assistants for the Azure OpenAI endpoint. When set to `true`, activates assistants associated with this endpoint.', 'Choose one, either the official OpenAI API or Azure OpenAI API for assistants, not both.'],
  ]}
/>

**Default:** Not specified

**Example:**
```yaml filename="endpoints / azureOpenAI / assistants"
assistants: true
```

## groups

**Key:**
<OptionTable
  options={[
    ['groups', 'Array', 'Configuration for groups of models by geographic location or purpose. Each item in the `groups` array configures a set of models under a certain grouping, often by geographic region or distinct configuration.', ''],
  ]}
/>

**Default:** Not specified

**Note:** [See example above.](#example-configuration)


## Group Object Structure

Each item under `groups` is part of a list of records, each with the following fields:

### group

**Key:**
<OptionTable
  options={[
    ['group', 'String', 'Identifier for a group of models.', ''],
  ]}
/>

**Required:** yes

**Example:**
```yaml filename="endpoints / azureOpenAI / groups / {group_item} / group"
"group": "my-westus"
```

### apiKey

**Key:**
<OptionTable
  options={[
    ['apiKey', 'String', 'The API key for accessing the Azure OpenAI Service.', 'It\'s highly recommended to use a custom env. variable reference for this field, i.e. `${YOUR_VARIABLE}`'],
  ]}
/>

**Required:** yes

**Example:**
```yaml filename="endpoints / azureOpenAI / groups / {group_item} / apiKey"
apiKey: "${WESTUS_API_KEY}"
```

### instanceName

**Key:**
<OptionTable
  options={[
    ['instanceName', 'String', 'Name of the Azure instance.', 'It\'s recommended to use a custom env. variable reference for this field, i.e. `${YOUR_VARIABLE}`'],
  ]}
/>

**Required:** yes

**Example:**
```yaml filename="endpoints / azureOpenAI / groups / {group_item} / instanceName"
instanceName: "my-westus"
```


### version

**Key:**
<OptionTable
  options={[
    ['version', 'String', 'API version.', 'It\'s recommended to use a custom env. variable reference for this field, i.e. `${YOUR_VARIABLE}`'],
  ]}
/>

**Default:** Not specified

**Example:**
```yaml filename="endpoints / azureOpenAI / groups / {group_item} / version"
version: "2023-12-01-preview"
```

### baseURL

**Key:**
<OptionTable
  options={[
    ['baseURL', 'String', 'The base URL for the Azure OpenAI Service.', 'It\'s recommended to use a custom env. variable reference for this field, i.e. `${YOUR_VARIABLE}`'],
  ]}
/>

**Default:** Not specified

**Example:**
```yaml filename="endpoints / azureOpenAI / groups / {group_item} / baseURL"
baseURL: "https://prod.example.com"
```

### additionalHeaders

**Key:**
<OptionTable
  options={[
    ['additionalHeaders', 'Dictionary', 'Additional headers for API requests.', 'It\'s recommended to use a custom env. variable reference for the values of field, as shown in the example. `api-key` header value is sent on every request.'],
  ]}
/>

**Default:** Not specified

**Example:**
```yaml filename="endpoints / azureOpenAI / groups / {group_item} / additionalHeaders"
additionalHeaders:
  X-Custom-Header: ${YOUR_SECRET_CUSTOM_VARIABLE}
```

### serverless

**Key:**
<OptionTable
  options={[
    ['serverless', 'Boolean', 'Indicates the use of a serverless inference endpoint for Azure OpenAI chat completions. When set to `true`, specifies that the group is configured to use serverless inference endpoints as an Azure "Models as a Service" model.', 'More info [here](./azure_openai.md#serverless-inference-endpoints)'],
  ]}
/>

**Default:** Not specified

**Example:**
```yaml filename="endpoints / azureOpenAI / groups / {group_item} / serverless"
serverless: true
```

### addParams

**Key:**
<OptionTable
  options={[
    ['addParams', 'Object/Dictionary', 'Adds additional parameters to requests. Useful for specifying API-specific options.', ''],
  ]}
/>

**Default:** Not specified

**Example:**
```yaml filename="endpoints / azureOpenAI / groups / {group_item} / addParams"
addParams:
  safe_prompt: true
```

### dropParams

**Key:**
<OptionTable
  options={[
    ['dropParams', 'Array/List of Strings', 'Removes [default parameters](#default-parameters) from requests. Excludes specified [default parameters](#default-parameters).', 'For a list of default parameters sent with every request, see the ["Default Parameters"](#default-parameters) Section below.'],
  ]}
/>

**Default:** Not specified

**Example:**
```yaml filename="endpoints / azureOpenAI / groups / {group_item} / dropParams"
dropParams: ["stop", "user", "frequency_penalty", "presence_penalty"]
```

### forcePrompt

**Key:**
<OptionTable
  options={[
    ['forcePrompt', 'Boolean', 'If `true`, sends a `prompt` parameter instead of `messages`. This combines all messages into a single text payload, following OpenAI format, and uses the `/completions` endpoint of your baseURL rather than `/chat/completions`.', ''],
  ]}
/>

**Default:** Not specified

**Example:**
```yaml filename="endpoints / azureOpenAI / groups / {group_item} / forcePrompt"
forcePrompt: false
```

### models

**Key:**
<OptionTable
  options={[
    ['models', '', 'Configuration for individual models within a group. Configures settings for each model, including deployment name and version.', 'Model configurations can adopt the group\'s deployment name and/or version when configured as a boolean (set to `true`) or an object for detailed settings of either of those fields.'],
  ]}
/>

**Default:** Not specified

**Example:**
```yaml filename="endpoints / azureOpenAI / groups / {group_item} / models"
models:
  gpt-4-vision-preview: 
    deploymentName: "arbitrary-deployment-name"
    version: "2024-02-15-preview"
```



================================================
FILE: pages/docs/configuration/librechat_yaml/object_structure/balance.mdx
================================================
---
title: Balance
description: Configure token credit balances for users in LibreChat
---

# Balance Object Structure

## Overview

The `balance` object allows administrators to configure how token credit balances are managed for users within LibreChat. Settings include enabling balance tracking, initializing user balances, and configuring automatic token refill behavior.

**Fields under `balance`:**

- `enabled`
- `startBalance`
- `autoRefillEnabled`
- `refillIntervalValue`
- `refillIntervalUnit`
- `refillAmount`

**Notes:**

- `balance` configurations apply globally across the application.
- Defaults are provided but can be customized based on requirements.
- Conditional logic can dynamically modify these settings based on other configurations.


## Example

```yaml filename="balance"
balance:
  enabled: false
  startBalance: 20000
  autoRefillEnabled: false
  refillIntervalValue: 30
  refillIntervalUnit: "days"
  refillAmount: 10000
```


## enabled

**Key:**

<OptionTable
    options={[
        ['enabled', 'Boolean', 'Enables token credit tracking and balance management for users.', 'Set to true to activate balance tracking for token usage.'],
    ]}
/>

**Default:** `false`

**Example:**

```yaml filename="balance / enabled"
balance:
  enabled: true
```


## startBalance

**Key:**

<OptionTable
    options={[
        ['startBalance', 'Integer', 'Specifies the initial number of tokens credited to a user upon registration.', 'Tokens credited to a new user account.'],
    ]}
/>

**Default:** `20000`

**Example:**

```yaml filename="balance / startBalance"
balance:
  startBalance: 20000
```


## autoRefillEnabled

**Key:**

<OptionTable
    options={[
        ['autoRefillEnabled', 'Boolean', 'Determines whether automatic refilling of token credits is enabled.', 'Set to true to enable automatic token refills.'],
    ]}
/>

**Default:** `false`

**Example:**

```yaml filename="balance / autoRefillEnabled"
balance:
  autoRefillEnabled: true
```


## refillIntervalValue

**Key:**

<OptionTable
    options={[
        ['refillIntervalValue', 'Integer', 'Specifies the numerical value for the interval at which token credits are automatically refilled.', 'For example, 30 represents a 30-day interval.'],
    ]}
/>

**Default:** `30`

**Example:**

```yaml filename="balance / refillIntervalValue"
balance:
  refillIntervalValue: 30
```


## refillIntervalUnit

**Key:**

<OptionTable
    options={[
        ['refillIntervalUnit', 'String', 'Specifies the time unit for the refill interval (e.g., "days", "hours").', 'Indicates the unit of time for refillIntervalValue.'],
    ]}
/>

**Default:** `"days"`

**Example:**

```yaml filename="balance / refillIntervalUnit"
balance:
  refillIntervalUnit: "days"
```


## refillAmount

**Key:**

<OptionTable
    options={[
        ['refillAmount', 'Integer', 'Specifies the number of tokens to be added to the user\'s balance during each automatic refill.', 'The amount added to a user’s token credits at each refill interval.'],
    ]}
/>

**Default:** `10000`

**Example:**

```yaml filename="balance / refillAmount"
balance:
  refillAmount: 10000
```


================================================
FILE: pages/docs/configuration/librechat_yaml/object_structure/config.mdx
================================================
# Config Structure

**Note:** Fields not specifically mentioned as required are optional.

## version
- **required** 

<OptionTable
  options={[
    ['version', 'String', 'Specifies the version of the configuration file.', 'version: 1.0.8' ],
  ]}
/>

## cache

<OptionTable
  options={[
    ['cache', 'Boolean', 'Toggles caching on or off. Set to `true` to enable caching (default).', 'cache: true' ],
  ]}
/>

## fileStrategy

<OptionTable
  options={[
    ['fileStrategy', 'String', 'Determines where to save user uploaded/generated files. Defaults to `"local"` if omitted.', 'fileStrategy: "firebase"' ],
  ]}
/>

## filteredTools

<OptionTable
  options={[
    ['filteredTools', 'Array of Strings', 'Filters out specific tools from both Plugins and OpenAI Assistants endpoints.', 'filteredTools: ["scholarai", "calculator"]' ],
  ]}
/>

- **Notes**:
    - If `includedTools` and `filteredTools` are both specified, only `includedTools` will be recognized.
    - Affects both `gptPlugins` and `assistants` endpoints
    - You can find the names of the tools to filter in [`api/app/clients/tools/manifest.json`](https://github.com/danny-avila/LibreChat/blob/main/api/app/clients/tools/manifest.json)
        - Use the `pluginKey` value
    - Also, any listed under the ".well-known" directory [`api/app/clients/tools/.well-known`](https://github.com/danny-avila/LibreChat/blob/main/api/app/clients/tools/.well-known)
        - Use the `name_for_model` value

## includedTools

<OptionTable
  options={[
    ['includedTools', 'Array of Strings', 'Includes specific tools from both Plugins and OpenAI Assistants endpoints.', 'includedTools: ["calculator"]' ],
  ]}
/>

- **Notes**:
    - If `includedTools` and `filteredTools` are both specified, only `includedTools` will be recognized.
    - Affects both `gptPlugins` and `assistants` endpoints
    - You can find the names of the tools to filter in [`api/app/clients/tools/manifest.json`](https://github.com/danny-avila/LibreChat/blob/main/api/app/clients/tools/manifest.json)
        - Use the `pluginKey` value
    - Also, any listed under the ".well-known" directory [`api/app/clients/tools/.well-known`](https://github.com/danny-avila/LibreChat/blob/main/api/app/clients/tools/.well-known)
        - Use the `name_for_model` value

## secureImageLinks

<OptionTable
  options={[
    ['secureImageLinks', 'Boolean', 'Whether or not to secure access to image links that are hosted locally by the app. Default: false.', 'secureImageLinks: true' ],
  ]}
/>

## imageOutputType

- **Note**: Case-sensitive. Google endpoint only supports "jpeg" and "png" output types.
- **Options**: "png" | "webp" | "jpeg"

<OptionTable
  options={[
    ['imageOutputType', 'String', 'The image output type for image responses. Defaults to "png" if omitted.', 'imageOutputType: "webp"' ],
  ]}
/>

## ocr

**Key:**
<OptionTable
  options={[
    ['ocr', 'Object', 'Configures Optical Character Recognition (OCR) settings for extracting text from images.', ''],
  ]}
/>

**Subkeys:**
<OptionTable
  options={[
    ['apiKey', 'String', 'The API key for the OCR service.', ''],
    ['baseURL', 'String', 'The base URL for the OCR service API.', ''],
    ['strategy', 'String', 'The OCR strategy to use. Options are "mistral_ocr" or "custom_ocr".', ''],
    ['mistralModel', 'String', 'The Mistral model to use for OCR processing.', ''],
  ]}
/>

see: [OCR Config Object Structure](/docs/configuration/librechat_yaml/object_structure/ocr)

## fileConfig

**Key:**
<OptionTable
  options={[
    ['fileConfig', 'Object', 'Configures file handling settings for the application, including size limits and MIME type restrictions.', ''],
  ]}
/>

**Subkeys:**
<OptionTable
  options={[
    ['endpoints', 'Record/Object', 'Specifies file handling configurations for individual endpoints, allowing customization per endpoint basis.', ''],
    ['serverFileSizeLimit', 'Number', 'The maximum file size (in MB) that the server will accept. Applies globally across all endpoints unless overridden by endpoint-specific settings.', ''],
    ['avatarSizeLimit', 'Number', 'Maximum size (in MB) for user avatar images.', ''],
  ]}
/>

see: [File Config Object Structure](/docs/configuration/librechat_yaml/object_structure/file_config)

## rateLimits

**Key:**
<OptionTable
  options={[
    ['rateLimits', 'Object', 'Defines rate limiting policies to prevent abuse by limiting the number of requests.', ''],
  ]}
/>

**Subkeys:**
<OptionTable
  options={[
    ['fileUploads', 'Object', 'Configures rate limits specifically for file upload operations.', ''],
    ['conversationsImport', 'Object', 'Configures rate limits specifically for conversation import operations.', ''],
    ['stt', 'Object', 'Configures rate limits specifically for speech-to-text (stt) requests', ''],
    ['tts', 'Object', 'Configures rate limits specifically for text-to-speech (tts) requests', ''],
  ]}
/>

**fileUploads Subkeys:**
<OptionTable
  options={[
    ['ipMax', 'Number', 'Maximum number of uploads allowed per IP address per window.', ''],
    ['ipWindowInMinutes', 'Number', 'Time window in minutes for the IP-based upload limit.', ''],
    ['userMax', 'Number', 'Maximum number of uploads allowed per user per window.', ''],
    ['userWindowInMinutes', 'Number', 'Time window in minutes for the user-based upload limit.', ''],
  ]}
/>

**conversationsImport Subkeys:**
<OptionTable
  options={[
    ['ipMax', 'Number', 'Maximum number of imports allowed per IP address per window.', ''],
    ['ipWindowInMinutes', 'Number', 'Time window in minutes for the IP-based imports limit.', ''],
    ['userMax', 'Number', 'Maximum number of imports per user per window.', ''],
    ['userWindowInMinutes', 'Number', 'Time window in minutes for the user-based imports limit.', ''],
  ]}
/>

**tts Subkeys:**
<OptionTable
  options={[
    ['ipMax', 'Number', 'Maximum number of requests allowed per IP address per window.', ''],
    ['ipWindowInMinutes', 'Number', 'Time window in minutes for the IP-based requests limit.', ''],
    ['userMax', 'Number', 'Maximum number of requests per user per window.', ''],
    ['userWindowInMinutes', 'Number', 'Time window in minutes for the user-based requests limit.', ''],
  ]}
/>

**stt Subkeys:**
<OptionTable
  options={[
    ['ipMax', 'Number', 'Maximum number of requests allowed per IP address per window.', ''],
    ['ipWindowInMinutes', 'Number', 'Time window in minutes for the IP-based requests limit.', ''],
    ['userMax', 'Number', 'Maximum number of requests per user per window.', ''],
    ['userWindowInMinutes', 'Number', 'Time window in minutes for the user-based requests limit.', ''],
  ]}
/>

    - **Example**:
    ```yaml filename="rateLimits"
    rateLimits:
      fileUploads:
        ipMax: 100
        ipWindowInMinutes: 60
        userMax: 50
        userWindowInMinutes: 60
      conversationsImport:
        ipMax: 100
        ipWindowInMinutes: 60
        userMax: 50
        userWindowInMinutes: 60
      stt:
        ipMax: 100
        ipWindowInMinutes: 1
        userMax: 50
        userWindowInMinutes: 1
      tts:
        ipMax: 100
        ipWindowInMinutes: 1
        userMax: 50
        userWindowInMinutes: 1
    ```

## registration

**Key:**
<OptionTable
  options={[
    ['registration', 'Object', 'Configures registration-related settings for the application.', ''],
  ]}
/>

**Subkeys:**
<OptionTable
  options={[
    ['socialLogins', '', 'Social login configurations.', ''],
    ['allowedDomains', '', 'Specifies allowed domains for registration.', ''],
  ]}
/>

see also: 
- [socialLogins](/docs/configuration/librechat_yaml/object_structure/registration#sociallogins),
- [alloweddomains](/docs/configuration/librechat_yaml/object_structure/registration#alloweddomains),
- [Registration Object Structure](/docs/configuration/librechat_yaml/object_structure/registration)

## actions

**Key:**
<OptionTable
  options={[
    ['actions', 'Object', 'Configures actions-related settings, used by Agents and Assistants', ''],
  ]}
/>

**Subkeys:**
<OptionTable
  options={[
    ['allowedDomains', '', 'Specifies allowed domains for actions.', ''],
  ]}
/>

see also: 
- [alloweddomains](/docs/configuration/librechat_yaml/object_structure/actions#alloweddomains),
- [Actions Object Structure](/docs/configuration/librechat_yaml/object_structure/actions)

## interface

**Key:**
<OptionTable
  options={[
    ['interface', 'Object', 'Configures user interface elements within the application, allowing for customization of visibility and behavior of various components.', ''],
  ]}
/>

**Subkeys:**
<OptionTable
  options={[
    ['privacyPolicy', 'Object', 'Contains settings related to the privacy policy link provided.', ''],
    ['termsOfService', 'Object', 'Contains settings related to the terms of service link provided.', ''],
    ['endpointsMenu', 'Boolean', 'Controls the visibility of the endpoints dropdown menu.', ''],
    ['modelSelect', 'Boolean', 'Determines whether the model selection feature is available.', ''],
    ['parameters', 'Boolean', 'Toggles the visibility of parameter configuration options AKA conversation settings.', ''],
    ['sidePanel', 'Boolean', 'Controls the visibility of the right-most side panel.', ''],
    ['presets', 'Boolean', 'Enables or disables the presets menu', ''],
    ['prompts', 'Boolean', 'Enables or disables all prompt-related features for all users', ''],
    ['bookmarks', 'Boolean', 'Enables or disables all bookmarks-related features for all users', ''],
    ['multiConvo', 'Boolean', 'Enables or disables all "multi convo", AKA multiple response streaming, related features for all users', ''],
    ['agents', 'Boolean', 'Enables or disables all agents features for all users', ''],
  ]}
/>

see: [Interface Object Structure](/docs/configuration/librechat_yaml/object_structure/interface)

## modelSpecs

**Key:**
<OptionTable
  options={[
    ['modelSpecs', 'Object', 'Configures model specifications, allowing for detailed setup and customization of AI models and their behaviors within the application.', ''],
  ]}
/>

**Subkeys:**
<OptionTable
  options={[
    ['enforce', 'Boolean', 'Determines whether the model specifications should strictly override other configuration settings.', ''],
    ['prioritize', 'Boolean', 'Specifies if model specifications should take priority over the default configuration when both are applicable.', ''],
    ['list', 'Array of Objects', 'Contains a list of individual model specifications detailing various configurations and behaviors.', ''],
  ]}
/>

see: [Model Specs Object Structure](/docs/configuration/librechat_yaml/object_structure/model_specs)

## endpoints

**Key:**
<OptionTable
  options={[
    ['endpoints', 'Object', 'Defines custom API endpoints for the application.', ''],
  ]}
/>

**Subkeys:**
<OptionTable
  options={[
    ['custom', 'Array of Objects', 'Each object in the array represents a unique endpoint configuration.', ''],
    ['azureOpenAI', 'Object', 'Azure OpenAI endpoint-specific configuration', ''],
    ['assistants', 'Object', 'Assistants endpoint-specific configuration.', ''],
    ['azureAssistants', 'Object', 'Azure Assistants endpoint-specific configuration.', ''],
    ['agents', 'Object', 'Agents endpoint-specific configuration.', ''],
  ]}
/>

## mcpServers

**Key:**
<OptionTable
  options={[
    ['mcpServers', 'Object', 'Defines the configuration for Model Context Protocol (MCP) servers, allowing dynamic integration of MCP servers within the application.', ''],
  ]}
/>

**Subkeys:**
<OptionTable
  options={[
    ['<serverName>', 'Object', 'Each key under `mcpServers` represents an individual MCP server configuration, identified by a unique name.', ''],
  ]}
/>

- **Notes**:
  - Initialization happens at startup, and the app must be restarted for changes to take effect.
  - The `<serverName>` is a unique identifier for each MCP server configuration.
  - Each MCP server can be configured using one of three connection types:
    - `stdio`
    - `websocket`
    - `sse`
  - The `type` field specifies the connection type to the MCP server.
  - If `type` is omitted, it defaults based on the presence and format of `url` or `command`:
    - If `url` is specified and starts with `http` or `https`, `type` defaults to `sse`.
    - If `url` is specified and starts with `ws` or `wss`, `type` defaults to `websocket`.
    - If `command` is specified, `type` defaults to `stdio`.
  - Additional configuration options include:
    - `timeout`: Timeout in milliseconds for MCP server requests. Determines how long to wait for a response for tool requests.
    - `initTimeout`: Timeout in milliseconds for MCP server initialization. Determines how long to wait for the server to initialize.
  - see: [MCP Servers Object Structure](/docs/configuration/librechat_yaml/object_structure/mcp_servers)

**Example:**

```yaml filename="mcpServers"
mcpServers:
  everything:
    # type: sse # type can optionally be omitted
    url: http://localhost:3001/sse
    timeout: 30000
    initTimeout: 10000
  puppeteer:
    type: stdio
    command: npx
    args:
      - -y
      - "@modelcontextprotocol/server-puppeteer"
    timeout: 30000
    initTimeout: 10000
  filesystem:
    # type: stdio
    command: npx
    args:
      - -y
      - "@modelcontextprotocol/server-filesystem"
      - /home/user/LibreChat/
    iconPath: /home/user/LibreChat/client/public/assets/logo.svg
  mcp-obsidian:
    command: npx
    args:
      - -y
      - "mcp-obsidian"
      - /path/to/obsidian/vault
```

see: [MCP Servers Object Structure](/docs/configuration/librechat_yaml/object_structure/mcp_servers)

## Additional links
- [AWS Bedrock Object Structure](/docs/configuration/librechat_yaml/object_structure/aws_bedrock)
- [Custom Endpoint Object Structure](/docs/configuration/librechat_yaml/object_structure/custom_endpoint)
- [Azure OpenAI Endpoint Object Structure](/docs/configuration/librechat_yaml/object_structure/azure_openai)
- [Assistants Endpoint Object Structure](/docs/configuration/librechat_yaml/object_structure/assistants_endpoint)
- [Agents](/docs/configuration/librechat_yaml/object_structure/agents)
- [OCR Config Object Structure](/docs/configuration/librechat_yaml/object_structure/ocr)



================================================
FILE: pages/docs/configuration/librechat_yaml/object_structure/custom_endpoint.mdx
================================================
# Custom Endpoint Object Structure
Each endpoint in the `custom` array should have the following structure:

## Example 

```yaml filename="Endpoint Object Structure"
endpoints:
    custom:
        # Example using Mistral AI API
    - name: "Mistral"
        apiKey: "${YOUR_ENV_VAR_KEY}"
        baseURL: "https://api.mistral.ai/v1"
        models: 
        default: ["mistral-tiny", "mistral-small", "mistral-medium", "mistral-large-latest"]
        titleConvo: true
        titleModel: "mistral-tiny" 
        modelDisplayLabel: "Mistral"
        # addParams:
        #   safe_prompt: true # Mistral specific value for moderating messages
        # NOTE: For Mistral, it is necessary to drop the following parameters or you will encounter a 422 Error:
        dropParams: ["stop", "user", "frequency_penalty", "presence_penalty"]
```

## name

**Key:**
<OptionTable
  options={[
    ['name', 'String', 'A unique name for the endpoint.', 'Will be used as the "title" in the Endpoints Selector'],
  ]}
/>

**Required**

**Example:**
```yaml filename="endpoints / custom / name"
name: "Mistral"
```

## apiKey

**Key:**
<OptionTable
  options={[
    ['apiKey', 'String (apiKey | "user_provided")', 'Your API key for the service. Can reference an environment variable, or allow user to provide the value.', 'It\'s highly recommended to use the env. variable reference for this field, i.e. `${YOUR_VARIABLE}`'],
  ]}
/>

**Required**

**Example:**
```yaml filename="endpoints / custom / apiKey"
apiKey: "${MISTRAL_API_KEY}"
```
or
```yaml filename="endpoints / custom / apiKey"
apiKey: "your_api_key"
```
or
```yaml filename="endpoints / custom / apiKey"
apiKey: "user_provided"
```

## baseURL

**Key:**
<OptionTable
  options={[
    ['baseURL', 'String (baseURL | "user_provided")', 'Base URL for the API. Can reference an environment variable, or allow user to provide the value.', 'It\'s highly recommended to use the env. variable reference for this field, i.e. `${YOUR_VARIABLE}`'],
  ]}
/>

**Required**

**Example:**
```yaml filename="endpoints / custom / baseURL"
baseURL: "https://api.mistral.ai/v1"
```
or
```yaml filename="endpoints / custom / baseURL"
baseURL: "${MISTRAL_BASE_URL}"
```
or
```yaml filename="endpoints / custom / baseURL"
baseURL: "user_provided"
```

**Notes:**

* If the `baseURL` you set is the full completions endpoint, you can set the [directEndpoint](#directendpoint) field to `true` to use it directly.
  - This is necessary because the app appends "/chat/completions" or "/completion" to the `baseURL` by default.

## iconURL

**Key:**
<OptionTable
  options={[
    ['iconURL', 'String', 'The URL to use as the Endpoint Icon.', ''],
  ]}
/>

**Default:** `""`

**Example:**
```yaml filename="endpoints / custom / iconURL"
iconURL: https://github.com/danny-avila/LibreChat/raw/main/docs/assets/LibreChat.svg
```
**Notes:**

* If you want to use existing project icons, define the endpoint `name` as one of the main endpoints (case-sensitive):
  - "openAI" | "azureOpenAI" | "google" | "anthropic" | "assistants" | "gptPlugins"
* There are also "known endpoints" (case-insensitive), which have icons provided. If your endpoint `name` matches the following names, you should omit this field:
  - "Mistral"
  - "Deepseek"
  - "OpenRouter"
  - "groq"
  - "APIpie"
  - "Anyscale"
  - "Fireworks"
  - "Perplexity"
  - "together.ai"
  - "ollama"
  - "xai"
  - "MLX"

## models

**Key:**
<OptionTable
  options={[
    ['models', 'Object', 'Configuration for models.', ''],
  ]}
/>

**Required**

**Properties:**

### default

**Key:**
<OptionTable
  options={[
    ['default', 'Array of Strings', 'An array of strings indicating the default models to use.', 'If fetching models fails, these defaults are used as a fallback.'],
  ]}
/>

**Required**

**Example:**
```yaml filename="endpoints / custom / models / default"
default:
  - "mistral-tiny"
  - "mistral-small"
  - "mistral-medium"
```

### fetch

**Key:**
<OptionTable
  options={[
    ['fetch', 'Boolean', 'When set to `true`, attempts to fetch a list of models from the API.', 'May cause slowdowns during initial use of the app if the response is delayed. Defaults to `false`.'],
  ]}
/>

**Default:** `false`

**Example:**
```yaml filename="endpoints / custom / models / fetch"
fetch: true
```

### userIdQuery

**Key:**
<OptionTable
  options={[
    ['userIdQuery', 'Boolean', 'When set to `true`, adds the LibreChat user ID as a query parameter to the API models request.', ''],
  ]}
/>

**Default:** `false`

**Example:**
```yaml filename="endpoints / custom / models / userIdQuery"
userIdQuery: true
```

## titleConvo

**Key:**
<OptionTable
  options={[
    ['titleConvo', 'Boolean', 'Enables title conversation when set to `true`.', ''],
  ]}
/>

**Default:** `false`

**Example:**
```yaml filename="endpoints / custom / titleConvo"
titleConvo: true
```

## titleMethod

**Key:**
<OptionTable
  options={[
    ['titleMethod', 'String ("completion" | "functions")', 'Chooses between "completion" or "functions" for title method.', 'Defaults to "completion" if omitted.'],
  ]}
/>

**Default:** `"completion"`

**Example:**
```yaml filename="endpoints / custom / titleMethod"
titleMethod: "completion"
```

## titleModel

**Key:**
<OptionTable
  options={[
    ['titleModel', 'String', 'Specifies the model to use for titles.', 'Defaults to "gpt-3.5-turbo" if omitted. May cause issues if "gpt-3.5-turbo" is not available. You can also dynamically use the current conversation model by setting it to "current_model".'],
  ]}
/>

**Default:** `"gpt-3.5-turbo"`

**Example:**
```yaml filename="endpoints / custom / titleModel"
titleModel: "mistral-tiny"
```
or
```yaml filename="endpoints / custom / titleModel"
titleModel: "current_model"
```

## summarize

**Key:**
<OptionTable
  options={[
    ['summarize', 'Boolean', 'Enables summarization when set to `true`.', 'This feature requires an OpenAI Functions compatible API'],
  ]}
/>

**Default:** `false`

**Example:**
```yaml filename="endpoints / custom / summarize"
summarize: false
```

## summaryModel

**Key:**
<OptionTable
  options={[
    ['summaryModel', 'String', 'Specifies the model to use if summarization is enabled.', 'Defaults to "gpt-3.5-turbo" if omitted. May cause issues if "gpt-3.5-turbo" is not available.'],
  ]}
/>

**Default:** `"gpt-3.5-turbo"`

**Example:**
```yaml filename="endpoints / custom / summaryModel"
summaryModel: "mistral-tiny"
```

## forcePrompt

**Key:**
<OptionTable
  options={[
    ['forcePrompt', 'Boolean', 'If `true`, sends a `prompt` parameter instead of `messages`.', 'Combines all messages into a single text payload or "prompt", following OpenAI format, which uses the `/completions` endpoint of your baseURL rather than `/chat/completions`.'],
  ]}
/>

**Default:** `false`

**Example:**
```yaml filename="endpoints / custom / forcePrompt"
forcePrompt: false
```

## modelDisplayLabel

**Key:**
<OptionTable
  options={[
    ['modelDisplayLabel', 'String', 'The label displayed in messages next to the Icon for the current AI model.', 'The display order is: 1. Custom name set via preset (if available), 2. Label derived from the model name (if applicable), 3. This value is used if the above are not specified. Defaults to "AI".'],
  ]}
/>

**Default:** `"AI"`

**Example:**
```yaml filename="endpoints / custom / modelDisplayLabel"
modelDisplayLabel: "Mistral"
```

## addParams

**Key:**
<OptionTable
  options={[
    ['addParams', 'Object/Dictionary', 'Adds additional parameters to requests.', 'Adds/Overrides parameters. Useful for specifying API-specific options.'],
  ]}
/>

**Example:**
```yaml filename="endpoints / custom / addParams"
addParams:
  safe_prompt: true
  max_tokens: 2048
```
**Notes:**
- The `addParams` field allows you to include additional parameters that are not part of the default payload(see the ["Default Parameters"](/docs/configuration/librechat_yaml/object_structure/default_params) section). This is particularly useful for API-specific options.
- For example, if an API requires the `max_tokens` parameter, you can add it here. By default, LibreChat [does not send](/docs/configuration/librechat_yaml/object_structure/default_params) `max_tokens` to use the maximum amount of tokens available, which is the default behavior of the OpenAI API. However, some alternate APIs require this field, or it may default to a very low value, causing your responses to appear cut off. In such cases, you should specify the desired value for `max_tokens` in the `addParams` field.
## dropParams

**Key:**
<OptionTable
  options={[
    ['dropParams', 'Array/List of Strings', 'Removes default parameters from requests.', 'Excludes specified default parameters. Useful for APIs that do not accept or recognize certain parameters.'],
  ]}
/>

**Example:**
```yaml filename="endpoints / custom / dropParams"
dropParams:
  - "stop"
  - "user"
  - "frequency_penalty"
  - "presence_penalty"
```
**Note:** 
- The `dropParams` field allows you to remove ["Default Parameters"](/docs/configuration/librechat_yaml/object_structure/default_params) that are sent with every request. This is helpful when working with APIs that do not accept or recognize certain parameters.

## headers

**Key:**
<OptionTable
  options={[
    ['headers', 'Object/Dictionary', 'Adds additional headers to requests. Can reference an environment variable', 'The `headers` object specifies custom headers for requests. Useful for authentication and setting content types.'],
  ]}
/>

**Example:**
```yaml filename="endpoints / custom / headers"
headers:
  x-api-key: "${ENVIRONMENT_VARIABLE}"
  Content-Type: "application/json"
```
**Note:** Supports dynamic environment variable values, which use the format: `"${VARIABLE_NAME}"`.

## directEndpoint

**Key:**
<OptionTable
  options={[
    ['directEndpoint', 'Boolean', 'When set to `true`, treats the configured `baseURL` as the completions endpoint to be used', ''],
  ]}
/>

**Default:** `false`

**Example:**
```yaml filename="endpoints / custom / directEndpoint"
directEndpoint: true
```

## titleMessageRole

**Key:**
<OptionTable
  options={[
    ['titleMessageRole', 'String', 'Specifies the role value to use in the message payload for title generation.', 'Defaults to "system" if omitted. May cause issues if "system" is not a valid value, which is sometimes the case for single message payloads, as it is for title generation.'],
  ]}
/>

**Default:** `"system"`

**Example:**
```yaml filename="endpoints / custom / titleMessageRole"
titleMessageRole: "user"
```


================================================
FILE: pages/docs/configuration/librechat_yaml/object_structure/default_params.mdx
================================================
# Default Parameters
**Note** 
- The purpose of this part of the documentation is to help understand what `addParams` and `dropParams` do. You **CANNOT** globally configure the parameters and their values that LibeChat sends by default, it can only be configured within a single endpoint.

Custom endpoints share logic with the OpenAI endpoint, and thus have default parameters tailored to the OpenAI API.

```yaml filename="Default Parameters"
{
  "model": "your-selected-model",
  "temperature": 1,
  "top_p": 1,
  "presence_penalty": 0,
  "frequency_penalty": 0,
  "user": "LibreChat_User_ID",
  "stream": true,
  "messages": [
    {
      "role": "user",
      "content": "hi how are you",
    },
  ],
}
```

### Breakdown
- `model`: The selected model from list of models.
- `temperature`: Defaults to `1` if not provided via preset,
- `top_p`: Defaults to `1` if not provided via preset,
- `presence_penalty`: Defaults to `0` if not provided via preset,
- `frequency_penalty`: Defaults to `0` if not provided via preset,
- `user`: A unique identifier representing your end-user, which can help OpenAI to [monitor and detect abuse](https://platform.openai.com/docs/api-reference/chat/create#chat-create-user).
- `stream`: If set, partial message deltas will be sent, like in ChatGPT. Otherwise, generation will only be available when completed.
- `messages`: [OpenAI format for messages](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages); the `name` field is added to messages with `system` and `assistant` roles when a custom name is specified via preset.

**Note:** The `max_tokens` field is not sent to use the maximum amount of tokens available, which is default OpenAI API behavior. Some alternate APIs require this field, or it may default to a very low value and your responses may appear cut off; in this case, you should add it to `addParams` field as shown in the [Custom Endpoint Object Structure](/docs/configuration/librechat_yaml/object_structure/custom_endpoint).



================================================
FILE: pages/docs/configuration/librechat_yaml/object_structure/file_config.mdx
================================================
# File Config Object Structure

## Overview

The `fileConfig` object allows you to configure file handling settings for the application, including size limits and MIME type restrictions. This section provides a detailed breakdown of the `fileConfig` object structure.

There are 3 main fields under `fileConfig`:

  - `endpoints`
  - `serverFileSizeLimit`
  - `avatarSizeLimit`

**Notes:**

- At the time of writing, the Assistants endpoint [supports filetypes from this list](https://platform.openai.com/docs/assistants/tools/file-search#supported-files).
- OpenAI, Azure OpenAI, Google, and Custom endpoints support files through the [RAG API.](../../rag_api.mdx)
- Any other endpoints not mentioned, like Plugins, do not support file uploads (yet).
- The Assistants endpoint has a defined endpoint value of `assistants`. All other endpoints use the defined value `default`
  - For non-assistants endpoints, you can adjust file settings for all of them under `default`
  - If you'd like to adjust settings for a specific endpoint, you can list their corresponding endpoint names:
    - `assistants`
        - does not use "default" as it has defined defaults separate from the others.
    - `openAI`
    - `azureOpenAI`
    - `google`
    - `YourCustomEndpointName`
- You can omit values, in which case, the app will use the default values as defined per endpoint type listed below.
- LibreChat counts 1 megabyte as follows: `1 x 1024 x 1024`

## Example

```yaml filename="fileConfig"
fileConfig:
  endpoints:
    assistants:
      fileLimit: 5
      fileSizeLimit: 10
      totalSizeLimit: 50
      supportedMimeTypes:
        - "image/.*"
        - "application/pdf"
    openAI:
      disabled: true
    default:
      totalSizeLimit: 20
    YourCustomEndpointName:
      fileLimit: 5
      fileSizeLimit: 1000
      supportedMimeTypes:
        - "image/.*"
  serverFileSizeLimit: 1000
  avatarSizeLimit: 2

```

## serverFileSizeLimit

<OptionTable
  options={[
    ['serverFileSizeLimit', 'Integer', 'The global maximum size for any file uploaded to the server, specified in megabytes (MB).', 'Acts as an overarching limit for file uploads across all endpoints, ensuring that no file exceeds this size server-wide.'],
  ]}
/>

```yaml filename="fileConfig / serverFileSizeLimit"
fileConfig:
  serverFileSizeLimit: 1000
```


## avatarSizeLimit

<OptionTable
options={[
['avatarSizeLimit', 'Integer', 'The maximum size allowed for avatar images, specified in megabytes (MB).', 'Specifically tailored for user avatar uploads, allowing for control over image sizes to maintain consistent quality and loading times.'],
]}
/>

```yaml filename="fileConfig / avatarSizeLimit"
fileConfig:
  avatarSizeLimit: 2
```

## endpoints

<OptionTable
  options={[
    ['endpoints', 'Record/Object', 'Configures file handling settings for individual endpoints, allowing customization per endpoint basis.', 'Specifies file handling configurations for individual endpoints, allowing customization per endpoint basis.'],
  ]}
/>

**Description:** Each object under endpoints is a record that can have the following settings:

### Overview

  - `disabled`
      - Whether file handling is disabled for the endpoint.
  - `fileLimit`
      - The maximum number of files allowed per upload request.
  - `fileSizeLimit`
      - The maximum size for a single file. In units of MB (e.g. use `20` for 20 megabytes)
  - `totalSizeLimit`
      - The total maximum size for all files in a single request. In units of MB (e.g. use `20` for 20 megabytes)
  - `supportedMimeTypes`
      - A list of [Regular Expressions](https://en.wikipedia.org/wiki/Regular_expression) specifying what MIME types are allowed for upload. This can be customized to restrict file types.

## disabled

<OptionTable
  options={[
    ['disabled', 'Boolean', 'Indicates whether file uploading is disabled for a specific endpoint.', 'Setting this to `true` prevents any file uploads to the specified endpoint, overriding any other file-related settings.'],
  ]}
/>

**Default:** `false`

```yaml filename="fileConfig / endpoints / {endpoint_record} / disabled"
openAI:
  disabled: true
```

## fileLimit

**Key:**
<OptionTable
  options={[
    ['fileLimit', 'Integer', 'The maximum number of files allowed in a single upload request.', 'Helps control the volume of uploads and manage server load.'],
  ]}
/>

**Default:** Varies by endpoint

```yaml filename="fileConfig / endpoints / {endpoint_record} / fileLimit"
assistants:
  fileLimit: 5
```

## fileSizeLimit

**Key:**
<OptionTable
  options={[
    ['fileSizeLimit', 'Integer', 'The maximum size allowed for each individual file, specified in megabytes (MB).', 'This limit ensures that no single file exceeds the specified size, allowing for better resource allocation and management.'],
  ]}
/>

**Default:** Varies by endpoint

```yaml filename="fileConfig / endpoints / {endpoint_record} / fileSizeLimit"
YourCustomEndpointName:
  fileSizeLimit: 1000
```

## totalSizeLimit

**Key:**
<OptionTable
  options={[
    ['totalSizeLimit', 'Integer', 'The total maximum size allowed for all files in a single request, specified in megabytes (MB).', 'This setting is crucial for preventing excessive bandwidth and storage usage by any single upload request.'],
  ]}
/>

**Default:** Varies by endpoint

```yaml filename="fileConfig / endpoints / {endpoint_record} / totalSizeLimit"
assistants:
  totalSizeLimit: 50
```

## supportedMimeTypes

**Key:**
<OptionTable
  options={[
    ['supportedMimeTypes', 'Array of Strings', 'A list of regular expressions defining the MIME types permitted for upload.', 'This allows for precise control over the types of files that can be uploaded. Invalid regex is ignored.'],
  ]}
/>

**Default:** Varies by endpoint

```yaml filename="fileConfig / endpoints / {endpoint_record} / supportedMimeTypes"
assistants:
  supportedMimeTypes:
      - "image/.*"
      - "application/pdf"
```



================================================
FILE: pages/docs/configuration/librechat_yaml/object_structure/interface.mdx
================================================
# Interface Object Structure

## Overview

The `interface` object allows for customization of various user interface elements within the application, including visibility and behavior settings for components such as menus, panels, and links. This section provides a detailed breakdown of the `interface` object structure.

These are fields under `interface`:

  - `privacyPolicy`
  - `termsOfService`
  - `endpointsMenu`
  - `modelSelect`
  - `parameters`
  - `sidePanel`
  - `presets`
  - `prompts`
  - `bookmarks`
  - `multiConvo`
  - `agents`
  - `customWelcome`
  - `runCode`

**Notes:**

- The `interface` configurations are applied globally within the application.
- Default values are provided for most settings but can be overridden based on specific requirements or conditions.
- Conditional logic in the application can further modify these settings based on other configurations like model specifications.

## Example

```yaml filename="interface"
interface:
  privacyPolicy:
    externalUrl: "https://example.com/privacy"
    openNewTab: true
  termsOfService:
    externalUrl: "https://example.com/terms"
    openNewTab: true
    modalAcceptance: true
    modalTitle: "Terms of Service"
    modalContent: |
      # Terms of Service
      ## Introduction
      Welcome to LibreChat!
  endpointsMenu: true
  modelSelect: false
  parameters: true
  sidePanel: true
  presets: false
  prompts: true
  bookmarks: true
  multiConvo: true
  agents: true
  customWelcome: "Hey {{user.name}}! Welcome to LibreChat"
  runCode: true
```

## privacyPolicy

**Key:**
<OptionTable
  options={[
    ['privacyPolicy', 'Object', 'Contains settings related to the privacy policy link provided in the user interface.', 'Allows for the specification of a custom URL and the option to open it in a new tab.'],
  ]}
/>

**Sub-keys:**
<OptionTable
  options={[
    ['externalUrl', 'String (URL)', 'The URL pointing to the privacy policy document.', ''],
    ['openNewTab', 'Boolean', 'Specifies whether the link should open in a new tab.', ''],
  ]}
/>

## termsOfService

**Key:**
<OptionTable
  options={[
    ['termsOfService', 'Object', 'Contains settings related to the terms of service link provided in the user interface.', 'Allows for the specification of a custom URL and the option to open it in a new tab, as well as a modal acceptance dialog for the terms of service.'],
  ]}
/>

**Sub-keys:**
<OptionTable
  options={[
    ['externalUrl', 'String (URL)', 'The URL pointing to the terms of service document.', 'https://librechat.ai/tos'],
    ['openNewTab', 'Boolean', 'Specifies whether the link should open in a new tab.', 'true'],
    ['modalAcceptance', 'Boolean', 'Specifies whether to show a modal terms and conditions dialog for users to accept in order to be able to use LibreChat.', 'true'],
    ['modalTitle', 'String', 'Specifies a custom title for the modal terms and conditions dialog (optional).', 'Terms of Service'],
    ['modalContent', 'String', 'Specifies the content of the modal terms and conditions dialog in MarkDown format.', 'See librechat.yaml.example for how to correctly format the multi-line parameter.'],
  ]}
/>

## endpointsMenu

- ⚠️ **Deprecated:** This setting is deprecated and no longer has any effect on v0.7.8 and later. Use `modelSelect` to enable configured endpoint options. If you would like to limit the available endpoints while using model specs, configure the [`modelSpecs.addedEndpoints`](/docs/configuration/librechat_yaml/object_structure/model_specs#addedendpoints) setting.

**Key:**
<OptionTable
  options={[
    ['endpointsMenu', 'Boolean', 'Controls the visibility of the endpoints menu in the interface.', 'Toggling this setting allows administrators to customize the availability of endpoint selections within the application.'],
  ]}
/>

**Default:** `true`

**Example:**
```yaml filename="interface / endpointsMenu"
interface:
  endpointsMenu: false
```

## modelSelect

**Key:**
<OptionTable
  options={[
    ['modelSelect', 'Boolean', 'Determines whether the model selection feature is available in the UI.', 'Enabling this feature allows users to select different models directly from the interface.'],
  ]}
/>

**Default:** `true`

**Notes:**
- This is required to be `true` if using [`modelSpecs.addedEndpoints`](/docs/configuration/librechat_yaml/object_structure/model_specs#addedendpoints).
- If `modelSpecs.addedEndpoints` is used and `interface.modelSelect` is not explicitly set, it defaults to `true`.

**Example:**
```yaml filename="interface / modelSelect"
interface:
  modelSelect: true
```

## parameters

**Key:**
<OptionTable
  options={[
    ['parameters', 'Boolean', 'Toggles the visibility of parameter configuration options within the interface.', 'This setting is crucial for users who need to adjust parameters for specific functionalities within the application.'],
  ]}
/>

**Default:** `true`

**Example:**
```yaml filename="interface / parameters"
interface:
  parameters: false
```

## sidePanel

**Key:**
<OptionTable
  options={[
    ['sidePanel', 'Boolean', 'Controls the visibility of the side panel in the application\'s interface.', 'The side panel typically contains additional navigation or information relevant to the application\'s context.'],
  ]}
/>

**Default:** `true`

**Example:**
```yaml filename="interface / sidePanel"
interface:
  sidePanel: true
```

## presets

**Key:**
<OptionTable
  options={[
    ['presets', 'Boolean', 'Enables or disables the use of presets in the application\'s UI.', 'Presets can simplify user interactions by providing pre-configured settings or operations, enhancing user experience and efficiency.'],
  ]}
/>

**Default:** `true`

**Example:**
```yaml filename="interface / presets"
interface:
  presets: true
```

## prompts

**Key:**
<OptionTable
  options={[
    ['prompts', 'Boolean', 'Enables or disables all prompt-related features for all users.', 'When disabled, users will not have access to create, edit, or use custom prompts within the application.'],
  ]}
/>

**Default:** `true`

**Example:**
```yaml filename="interface / prompts"
interface:
  prompts: false
```

## bookmarks

**Key:**
<OptionTable
  options={[
    ['bookmarks', 'Boolean', 'Enables or disables all bookmarks-related features for all users.', 'When disabled, users will not be able to create, manage, or access bookmarks within the application.'],
  ]}
/>

**Default:** `true`

**Example:**
```yaml filename="interface / bookmarks"
interface:
  bookmarks: true
```

## multiConvo

**Key:**
<OptionTable
  options={[
    ['multiConvo', 'Boolean', 'Enables or disables all "multiConvo", AKA multiple response streaming, related features for all users.', 'When disabled, users will not be able to stream responses from 2 AI models at the same time.'],
  ]}
/>

**Default:** `true`

**Example:**
```yaml filename="interface / multiConvo"
interface:
  multiConvo: true
```

## agents

More info on [Agents](/docs/features/agents)

**Key:**
<OptionTable
  options={[
    ['agents', 'Boolean', 'Enables or disables use of the Agents'],
  ]}
/>

**Default:** `true`

**Example:**
```yaml filename="interface / agents"
interface:
  agents: true
```

## customWelcome

**Key:**
<OptionTable
  options={[
    ['customWelcome', 'String', 'Allows administrators to define a custom welcome message for the chat interface, with the option to personalize it using the {{user.name}} parameter.'],
  ]}
/>

**Default:** _None (if not specified, a default greeting is used)_

**Example:**
```yaml filename="interface / customWelcome"
interface:
    customWelcome: "Hey {{user.name}}! Welcome to LibreChat"
```

**Note:** You can use `{{user.name}}` within the `customWelcome` message to dynamically insert the user's name for a personalized greeting experience.

## runCode

Enables/disables the "Run Code" button for Markdown Code Blocks. More info on the [LibreChat Code Interpreter API](/docs/features/code_interpreter)

**Note:** This setting does not disable the [Agents Code Interpreter Capability](/docs/features/agents#code-interpreter). To disable the Agents Capability, see the [Agents Endpoint configuration](/docs/configuration/librechat_yaml/object_structure/agents) instead.

**Key:**
<OptionTable
  options={[
    ['runCode', 'Boolean', 'Enables or disables the "Run Code" button for Markdown Code Blocks.'],
  ]}
/>

**Default:** `true`

**Example:**
```yaml filename="interface / runCode"
interface:
  runCode: true
```



================================================
FILE: pages/docs/configuration/librechat_yaml/object_structure/mcp_servers.mdx
================================================
# MCP Servers Object Structure

## Example

```yaml filename="MCP Servers Object Structure"
# Example MCP Servers Object Structure
mcpServers:
  everything:
    # type: sse # type can optionally be omitted
    url: http://localhost:3001/sse
  puppeteer:
    type: stdio
    command: npx
    args:
      - -y
      - "@modelcontextprotocol/server-puppeteer"
  filesystem:
    # type: stdio
    command: npx
    args:
      - -y
      - "@modelcontextprotocol/server-filesystem"
      - /home/user/LibreChat/
    iconPath: /home/user/LibreChat/client/public/assets/logo.svg
  mcp-obsidian:
    command: npx
    args:
      - -y
      - "mcp-obsidian"
      - /path/to/obsidian/vault
```

## `<serverName>`

**Key:**
<OptionTable
  options={[
    ['<serverName>', 'Object', 'Each key under `mcpServers` represents an individual MCP server configuration, identified by a unique name. This name is used to reference the server configuration within the application.', ''],
  ]}
/>

### Subkeys

<OptionTable
  options={[
    ['type', 'String', 'Specifies the connection type to the MCP server. Valid options are `"stdio"`, `"websocket"`, or `"sse"`. If omitted, it defaults based on the presence and format of `url` or `command`.', 'type: "stdio"'],
    ['command', 'String', '(For `stdio` type) The command or executable to run to start the MCP server.', 'command: "npx"'],
    ['args', 'Array of Strings', '(For `stdio` type) Command line arguments to pass to the `command`.', 'args: ["-y", "@modelcontextprotocol/server-puppeteer"]'],
    ['url', 'String', '(For `websocket` or `sse` type) The URL to connect to the MCP server.', 'url: "http://localhost:3001/sse"'],
    ['iconPath', 'String', '(Optional) Defines the tool\'s display icon shown in the tool selection dialog.', 'iconPath: "/path/to/icon.svg"'],
    ['timeout', 'Number', '(Optional) Timeout in milliseconds for MCP server requests. Determines how long to wait for a response for tool requests.', 'timeout: 30000'],
    ['initTimeout', 'Number', '(Optional) Timeout in milliseconds for MCP server initialization. Determines how long to wait for the server to initialize.', 'initTimeout: 10000'],
    ['env', 'Object', '(Optional, `stdio` type only) Environment variables to use when spawning the process.', 'env:\n  NODE_ENV: "production"'],
    ['stderr', 'String or Stream or Number', '(Optional, `stdio` type only) How to handle `stderr` of the child process. Defaults to `"inherit"`.', 'stderr: "inherit"'],
  ]}
/>

#### `type`

- **Type:** String
- **Description:** Specifies the connection type to the MCP server. Valid options are `"stdio"`, `"websocket"`, or `"sse"`.
- **Default Value:** Determined based on the presence and format of `url` or `command`.

#### `command`

- **Type:** String
- **Description:** (For `stdio` type) The command or executable to run to start the MCP server.

#### `args`

- **Type:** Array of Strings
- **Description:** (For `stdio` type) Command line arguments to pass to the `command`.

#### `url`

- **Type:** String
- **Description:** (For `websocket` or `sse` type) The URL to connect to the MCP server.
- **Notes:**
  - For `sse` type, the URL must start with `http://` or `https://`.
  - For `websocket` type, the URL must start with `ws://` or `wss://`.

#### `iconPath`

- **Type:** String (Optional)
- **Description:** Defines the tool's display icon shown in the tool selection dialog.

#### `env`

- **Type:** Object (Optional, `stdio` type only)
- **Description:** Environment variables to use when spawning the process.

#### `stderr`

- **Type:** String or Stream or Number (Optional, `stdio` type only)
- **Description:** How to handle `stderr` of the child process. This matches the semantics of Node's [`child_process.spawn`](https://nodejs.org/api/child_process.html#child_processspawncommand-args-options).
- **Default Value:** `"inherit"` (messages to `stderr` will be printed to the parent process's `stderr`).

### Notes

- **Type Inference:**
  - If `type` is omitted:
    - If `url` is specified and starts with `http://` or `https://`, `type` defaults to `sse`.
    - If `url` is specified and starts with `ws://` or `wss://`, `type` defaults to `websocket`.
    - If `command` is specified, `type` defaults to `stdio`.
- **Connection Types:**
  - **`stdio`**: Starts an MCP server as a child process and communicates via standard input/output.
  - **`websocket`**: Connects to an external MCP server via WebSocket.
  - **`sse`**: Connects to an external MCP server via Server-Sent Events (SSE).

## Examples

### `stdio` MCP Server

```yaml filename="stdio MCP Server"
puppeteer:
  type: stdio
  command: npx
  args:
    - -y
    - "@modelcontextprotocol/server-puppeteer"
  timeout: 30000
  initTimeout: 10000
  env:
    NODE_ENV: "production"
  stderr: inherit
```

### `sse` MCP Server

```yaml filename="sse MCP Server"
everything:
  url: http://localhost:3001/sse
```

### `websocket` MCP Server

```yaml filename="websocket MCP Server"
myWebSocketServer:
  url: ws://localhost:8080
```

### MCP Server with Custom Icon

```yaml filename="MCP Server with Icon"
filesystem:
  command: npx
  args:
    - -y
    - "@modelcontextprotocol/server-filesystem"
    - /home/user/LibreChat/
  iconPath: /home/user/LibreChat/client/public/assets/logo.svg
```

---

**Importing MCP Server Configurations**

The `mcpServers` configurations allow LibreChat to dynamically interact with various MCP servers, which can perform specialized tasks or provide specific functionalities within the application. This modular approach facilitates extending the application's capabilities by simply adding or modifying server configurations.

---

## Additional Information

- **Default Behavior:**
  - Initialization happens at startup, and the app must be restarted for changes to take effect.
  - If both `url` and `command` are specified, the `type` must be explicitly defined to avoid ambiguity.
- **Environment Variables (`env`):**
  - Useful for setting up specific runtime environments or configurations required by the MCP server process.
- **Error Handling (`stderr`):**
  - Configuring `stderr` allows you to manage how error messages from the MCP server process are handled. The default `"inherit"` means that the errors will be printed to the parent process's `stderr`.

## References

- [Model Context Protocol (MCP) Documentation](https://github.com/modelcontextprotocol)
- [Node.js child_process.spawn](https://nodejs.org/api/child_process.html#child_processspawncommand-args-options)

---

By properly configuring the `mcpServers` in your `librechat.yaml`, you can enhance LibreChat's functionality and integrate custom tools and services seamlessly.



================================================
FILE: pages/docs/configuration/librechat_yaml/object_structure/model_config.mdx
================================================
# Model Config Structure

Each item under `models` is part of a list of records, either a boolean value or Object:

**When specifying a model as an object:**

An object allows for detailed configuration of the model, including its `deploymentName` and/or `version`. This mode is used for more granular control over the models, especially when working with multiple versions or deployments under one instance or resource group.

**Example**:
```yaml filename="endpoints / azureOpenAI / groups / {group_item} / models / {model_item=Object}"
models:
  gpt-4-vision-preview:
    deploymentName: "gpt-4-vision-preview"
    version: "2024-02-15-preview"
```

**Notes:**
- **Deployment Names** and **Versions** are critical for ensuring that the correct model is used.
    - Double-check these values for accuracy to prevent unexpected behavior.

### deploymentName

**Key:**
<OptionTable
  options={[
    ['deploymentName', 'String', 'The name of the deployment for the model. Identifies the deployment of the model within Azure.', 'This does not have to be the matching OpenAI model name as is convention, but must match the actual name of your deployment on Azure.'],
  ]}
/>

**Required:** yes

**Example:**
```yaml filename="endpoints / azureOpenAI / groups / {group_item} / models / {model_item=Object} / deploymentName"
deploymentName: "gpt-4-vision-preview"
```

## version

**Key:**
<OptionTable
  options={[
    ['version', 'String', 'Specifies the version of the model. Defines the version of the model to be used.', ''],
  ]}
/>

**Required:** yes

**Example:**
```yaml filename="endpoints / azureOpenAI / groups / {group_item} / models / {model_item=Object} / version"
version: "2024-02-15-preview"
```

**Enabling a Model with Default Group Configuration**

**Key:**
<OptionTable
  options={[
    ['models', 'Boolean', 'Enables a model with default group configuration.', 'When a model is enabled (`true`) without using an object, it uses the group\'s configuration values for deployment name and version.'],
  ]}
/>

**Example:**
```yaml filename="endpoints / azureOpenAI / groups / {group_item} / models"
models:
  gpt-4-turbo: true
```


================================================
FILE: pages/docs/configuration/librechat_yaml/object_structure/model_specs.mdx
================================================
# Model Specs Object Structure

## **Overview**

The `modelSpecs` object helps you provide a simpler UI experience for AI models within your application.

There are 3 main fields under `modelSpecs`:

  - `enforce` (optional; default: false)
  - `prioritize` (optional; default: true)
  - `list` (required)
  - `addedEndpoints` (optional)

**Notes:**

- If `enforce` is set to true, model specifications can potentially conflict with other interface settings such as `endpointsMenu`, `modelSelect`, `presets`, and `parameters`.
- The `list` array contains detailed configurations for each model, including presets that dictate specific behaviors, appearances, and capabilities.
- If [interface](interface.mdx) fields are not specified in the configuration, having a list of model specs will disable the following interface elements:
    - `endpointsMenu`
    - `modelSelect`
    - `parameters`
    - `presets`
- If you would like to enable these interface elements along with model specs, you can set them to `true` in the `interface` object.

## Example

```yaml filename="modelSpecs"
modelSpecs:
  enforce: true
  prioritize: true
  list:
    - name: "meeting-notes-gpt4"
      label: "Meeting Notes Assistant (GPT4)"
      default: true
      description: "Generate meeting notes by simply pasting in the transcript from a Teams recording."
      iconURL: "https://example.com/icon.png"
      preset:
        endpoint: "azureOpenAI"
        model: "gpt-4-turbo-1106-preview"
        maxContextTokens: 128000 # Maximum context tokens
        max_tokens: 4096 # Maximum output tokens
        temperature: 0.2
        modelLabel: "Meeting Summarizer"
        greeting: |
          This assistant creates meeting notes based on transcripts of Teams recordings.
          To start, simply paste the transcript into the chat box.
        promptPrefix: |
          Based on the transcript, create coherent meeting minutes for a business meeting. Include the following sections:
          - Date and Attendees
          - Agenda
          - Minutes
          - Action Items

          Focus on what items were discussed and/or resolved. List any open action items.
          The format should be a bulleted list of high level topics in chronological order, and then one or more concise sentences explaining the details.
          Each high level topic should have at least two sub topics listed, but add as many as necessary to support the high level topic. 

          - Do not start items with the same opening words.

          Take a deep breath and be sure to think step by step.
```

---

## **Top-level Fields**

### enforce

<OptionTable
  options={[
    ['enforce', 'Boolean', 'Determines whether the model specifications should strictly override other configuration settings.', 'Setting this to `true` can lead to conflicts with interface options if not managed carefully.'],
  ]}
/>

**Default:** `false`

**Example:**
```yaml filename="modelSpecs / enforce"
modelSpecs:
  enforce: true
```

---

### prioritize

<OptionTable
  options={[
    ['prioritize', 'Boolean', 'Specifies if model specifications should take priority over the default configuration when both are applicable.', 'When set to `true`, it ensures that a modelSpec is always selected in the UI. Doing this may prevent users from selecting different endpoints for the selected spec.'],
  ]}
/>

**Default:** `true`

**Example:**
```yaml filename="modelSpecs / prioritize"
modelSpecs:
  prioritize: false
```

---

### addedEndpoints

<OptionTable
  options={[
    ['addedEndpoints', 'Array of Strings', 'Allows specific endpoints (e.g., "openAI", "google") to be selectable in the UI alongside the defined model specs.', 'Requires `interface.modelSelect` to be `true`. If this field is used and `interface.modelSelect` is not explicitly set, `modelSelect` will default to `true`.'],
  ]}
/>

**Default:** `[]` (empty list)

**Note:** Must be one of the following:
- `openAI, azureOpenAI, google, anthropic, assistants, azureAssistants, bedrock`

**Example:**
```yaml filename="modelSpecs / addedEndpoints"
modelSpecs:
  # ... other modelSpecs fields
  addedEndpoints:
    - openAI
    - google
```

---

### list

**Required**

<OptionTable
  options={[
    ['list', 'Array of Objects', 'Contains a list of individual model specifications detailing various configurations and behaviors.', 'Each object in the list details the configuration for a specific model, including its behaviors, appearance, and capabilities related to the application\'s functionality.'],
  ]}
/>

## **Model Spec (List Item)**

Within each **Model Spec**, or each **list** item, you can configure the following fields:

---

### name

<OptionTable
  options={[
    ['name', 'String', 'Unique identifier for the model.', 'No default. Must be specified.'],
  ]}
/>

**Description:**  
Unique identifier for the model.

---

### label

<OptionTable
  options={[
    ['label', 'String', 'A user-friendly name or label for the model, shown in the header dropdown.', 'No default. Optional.'],
  ]}
/>

**Description:**  
A user-friendly name or label for the model, shown in the header dropdown.

---

### default

<OptionTable
  options={[
    ['default', 'Boolean', 'Specifies if this model spec is the default selection, to be auto-selected on every new chat.', ''],
  ]}
/>

**Description:**  
Specifies if this model spec is the default selection, to be auto-selected on every new chat.

---

### iconURL

<OptionTable
  options={[
    ['iconURL', 'String', 'URL or a predefined endpoint name for the model\'s icon.', 'No default. Optional.'],
  ]}
/>

**Description:**  
URL or a predefined endpoint name for the model's icon.

---

### description

<OptionTable
  options={[
    ['description', 'String', 'A brief description of the model and its intended use or role, shown in the header dropdown menu.', 'No default. Optional.'],
  ]}
/>

**Description:**  
A brief description of the model and its intended use or role, shown in the header dropdown menu.

---

### showIconInMenu

<OptionTable
  options={[
    ['showIconInMenu', 'Boolean', 'Controls whether the model\'s icon appears in the header dropdown menu.', ''],
  ]}
/>

**Description:**  
Controls whether the model's icon appears in the header dropdown menu. Defaults to `true`.

---

### showIconInHeader

<OptionTable
  options={[
    ['showIconInHeader', 'Boolean', 'Controls whether the model\'s icon appears in the header dropdown button, left of its name.', ''],
  ]}
/>

**Description:**  
Controls whether the model's icon appears in the header dropdown button, left of its name. Defaults to `true`.

---

### preset

<OptionTable
  options={[
    ['preset', 'Object', 'Detailed preset configurations that define the behavior and capabilities of the model.', 'See "Preset Object Structure" below.'],
  ]}
/>

**Description:**  
Detailed preset configurations that define the behavior and capabilities of the model (see Preset Object Structure below).

---

## Preset Fields

The `preset` field for a `modelSpecs.list` item is made up of a comprehensive configuration blueprint for AI models within the system. It is designed to specify the operational settings of AI models, tailoring their behavior, outputs, and interactions with other system components and endpoints.

### System Options

#### endpoint

**Required**

**Accepted Values:**
- `openAI`
- `azureOpenAI`
- `google`
- `anthropic`
- `assistants`
- `azureAssistants`
- `bedrock`
- `agents`

**Note:** If you are using a custom endpoint, the `endpoint` value must match the defined [custom endpoint name](/docs/configuration/librechat_yaml/object_structure/custom_endpoint#name) exactly.

<OptionTable
  options={[
    ['endpoint', 'Enum (EModelEndpoint) or String (nullable)', 'Specifies the endpoint the model communicates with to execute operations. This setting determines the external or internal service that the model interfaces with.', ''],
  ]}
/>

**Example:**
```yaml filename="modelSpecs / list / {spec_item} / preset / endpoint"
preset:
  endpoint: "openAI"
```

---

#### modelLabel

<OptionTable
  options={[
    ['modelLabel', 'String (nullable)', 'The label used to identify the model in user interfaces or logs. It provides a human-readable name for the model, which is displayed in the UI, as well as made aware to the AI.', 'None'],
  ]}
/>

**Default:** `None`

**Example:**
```yaml filename="modelSpecs / list / {spec_item} / preset / modelLabel"
preset:
  modelLabel: "Customer Support Bot"
```

---

#### greeting

<OptionTable
  options={[
    ['greeting', 'String', 'A predefined message that is visible in the UI before a new chat is started. This is a good way to provide instructions to the user, or to make the interface seem more friendly and accessible.', ''],
  ]}
/>

**Default:** `None`

**Example:**
```yaml filename="modelSpecs / list / {spec_item} / preset / greeting"
preset:
  greeting: "This assistant creates meeting notes based on transcripts of Teams recordings. To start, simply paste the transcript into the chat box."
```

---

#### promptPrefix

<OptionTable
  options={[
    ['promptPrefix', 'String (nullable)', 'A static text prepended to every prompt sent to the model, setting a consistent context for responses.', 'When using "assistants" as the endpoint, this becomes the OpenAI field `additional_instructions`.'],
  ]}
/>

**Default:** `None`

**Example 1:**
```yaml filename="modelSpecs / list / {spec_item} / preset / promptPrefix"
preset:
  promptPrefix: "As a financial advisor, ..."
```

**Example 2:**
```yaml filename="modelSpecs / list / {spec_item} / preset / promptPrefix"
preset:
  promptPrefix: |
    Based on the transcript, create coherent meeting minutes for a business meeting. Include the following sections:
    - Date and Attendees
    - Agenda
    - Minutes
    - Action Items

    Focus on what items were discussed and/or resolved. List any open action items.
    The format should be a bulleted list of high level topics in chronological order, and then one or more concise sentences explaining the details.
    Each high level topic should have at least two sub topics listed, but add as many as necessary to support the high level topic. 

    - Do not start items with the same opening words.

    Take a deep breath and be sure to think step by step.
```

---

#### resendFiles

<OptionTable
  options={[
    ['resendFiles', 'Boolean', 'Indicates whether files should be resent in scenarios where persistent sessions are not maintained.', ''],
  ]}
/>

**Default:** `false`

**Example:**
```yaml filename="modelSpecs / list / {spec_item} / preset / resendFiles"
preset:
  resendFiles: true
```

---

#### imageDetail

**Accepted Values:**
- low
- auto
- high

<OptionTable
  options={[
    ['imageDetail', 'Enum (eImageDetailSchema)', 'Specifies the level of detail required in image analysis tasks, applicable to models with vision capabilities (OpenAI spec).', ''],
  ]}
/>

**Example:**
```yaml filename="modelSpecs / list / {spec_item} / preset / imageDetail"
preset:
  imageDetail: "high"
```

---

#### maxContextTokens

<OptionTable
  options={[
    ['maxContextTokens', 'Number', 'The maximum number of context tokens to provide to the model.', 'Useful if you want to limit the maximum context for this preset.'],
  ]}
/>

**Example:**
```yaml filename="modelSpecs / list / {spec_item} / preset / maxContextTokens"
preset:
  maxContextTokens: 4096
```

---

### Agent Options

Note that these options are only applicable when using the `agents` endpoint.

You should exclude any model options and defer to the agent's configuration as defined in the UI.

---

#### agent_id

<OptionTable
  options={[
    ['agent_id', 'String', 'Identification of an assistant.', ''],
  ]}
/>

**Example:**
```yaml filename="modelSpecs / list / {spec_item} / preset / agent_id"
preset:
  agent_id: "agent_someUniqueId"
```

---

### Assistant Options

Note that these options are only applicable when using the `assistants` or `azureAssistants` endpoint.

Similar to [Agents](#agent-options), you should exclude any model options and defer to the assistant's configuration.

---

#### assistant_id

<OptionTable
  options={[
    ['assistant_id', 'String', 'Identification of an assistant.', ''],
  ]}
/>

**Example:**
```yaml filename="modelSpecs / list / {spec_item} / preset / assistant_id"
preset:
  assistant_id: "asst_someUniqueId"
```

---

#### instructions

**Note:** this is distinct from [`promptPrefix`](#promptPrefix), as this overrides existing assistant instructions for current runs.

Only use this if you want to override the assistant's core instructions.

Use [`promptPrefix`](#promptPrefix) for `additional_instructions`.

More information:

- https://platform.openai.com/docs/api-reference/models#runs-createrun-instructions
- https://platform.openai.com/docs/api-reference/runs/createRun#runs-createrun-additional_instructions

<OptionTable
  options={[
    ['instructions', 'String', 'Overrides the assistant\'s default instructions.', ''],
  ]}
/>

**Example:**
```yaml filename="modelSpecs / list / {spec_item} / preset / instructions"
preset:
  instructions: "Please handle customer queries regarding order status."
```

---

#### append_current_datetime

Adds the current date and time to `additional_instructions` for each run. Does not overwrite `promptPrefix`, but adds to it.

<OptionTable
  options={[
    ['append_current_datetime', 'Boolean', 'Adds the current date and time to `additional_instructions` as defined by `promptPrefix`', ''],
  ]}
/>

**Example:**
```yaml filename="modelSpecs / list / {spec_item} / preset / append_current_datetime"
preset:
  append_current_datetime: true
```

---

### Model Options

> **Note:** Each parameter below includes a note on which endpoints support it.  
> **OpenAI / AzureOpenAI / Custom** typically support `temperature`, `presence_penalty`, `frequency_penalty`, `stop`, `top_p`, `max_tokens`.  
> **Google / Anthropic** typically support `topP`, `topK`, `maxOutputTokens`, `promptCache` (Anthropic only).  
> **Bedrock** supports `region`, `maxTokens`, and a few others.  

#### model

> **Supported by:** All endpoints (except `agents`)

<OptionTable
  options={[
    ['model', 'String (nullable)', 'The model name to use for the preset, matching a configured model under the chosen endpoint.', 'None'],
  ]}
/>

**Default:** `None`

**Example:**
```yaml
preset:
  model: "gpt-4-turbo"
```

---

#### temperature

> **Supported by:** `openAI`, `azureOpenAI`, `google` (as `temperature`), `anthropic` (as `temperature`), and custom (OpenAI-like)  

<OptionTable
  options={[
    ['temperature', 'Number', 'Controls how deterministic or “creative” the model responses are.', ''],
  ]}
/>

**Example:**
```yaml
preset:
  temperature: 0.7
```

---

#### presence_penalty

> **Supported by:** `openAI`, `azureOpenAI`, custom (OpenAI-like)  
> *Not typically used by Google/Anthropic/Bedrock*

<OptionTable
  options={[
    ['presence_penalty', 'Number', 'Penalty for repetitive tokens, encouraging exploration of new topics.', ''],
  ]}
/>

**Example:**
```yaml
preset:
  presence_penalty: 0.3
```

---

#### frequency_penalty

> **Supported by:** `openAI`, `azureOpenAI`, custom (OpenAI-like)  
> *Not typically used by Google/Anthropic/Bedrock*

<OptionTable
  options={[
    ['frequency_penalty', 'Number', 'Penalty for repeated tokens, reducing redundancy in responses.', ''],
  ]}
/>

**Example:**
```yaml
preset:
  frequency_penalty: 0.5
```

---

#### stop

> **Supported by:** `openAI`, `azureOpenAI`, custom (OpenAI-like)  
> *Not typically used by Google/Anthropic/Bedrock*

<OptionTable
  options={[
    ['stop', 'Array of Strings', 'Stop tokens for the model, instructing it to end its response if encountered.', ''],
  ]}
/>

**Example:**
```yaml
preset:
  stop:
    - "END"
    - "STOP"
```

---

#### top_p

> **Supported by:** `openAI`, `azureOpenAI`, custom (OpenAI-like)  
> **Google/Anthropic** often use `topP` (capital “P”) instead of `top_p`.

<OptionTable
  options={[
    ['top_p', 'Number', 'Nucleus sampling parameter (0-1), controlling the randomness of tokens.', ''],
  ]}
/>

**Example:**
```yaml
preset:
  top_p: 0.9
```

---

#### topP

> **Supported by:** `google` & `anthropic`  
> (similar purpose to `top_p`, but named differently in those APIs)

<OptionTable
  options={[
    ['topP', 'Number', 'Nucleus sampling parameter for Google/Anthropic endpoints.', ''],
  ]}
/>

**Example:**
```yaml
preset:
  topP: 0.8
```

---

#### topK

> **Supported by:** `google` & `anthropic`  
> (k-sampling limit on the next token distribution)

<OptionTable
  options={[
    ['topK', 'Number', 'Limits the next token selection to the top K tokens.', ''],
  ]}
/>

**Example:**
```yaml
preset:
  topK: 40
```

---

#### max_tokens

> **Supported by:** `openAI`, `azureOpenAI`, custom (OpenAI-like)  
> *For Google/Anthropic, use `maxOutputTokens` or `maxTokens` (depending on the endpoint).*

<OptionTable
  options={[
    ['max_tokens', 'Number', 'The maximum number of tokens in the model response.', ''],
  ]}
/>

**Example:**
```yaml
preset:
  max_tokens: 4096
```

---

#### maxOutputTokens

> **Supported by:** `google`, `anthropic`  
> *Equivalent to `max_tokens` for these providers.*

<OptionTable
  options={[
    ['maxOutputTokens', 'Number', 'The maximum number of tokens in the response (Google/Anthropic).', ''],
  ]}
/>

**Example:**
```yaml
preset:
  maxOutputTokens: 2048
```

---

#### promptCache

> **Supported by:** `anthropic`  
> (Toggle Anthropic’s “prompt-caching” feature)

<OptionTable
  options={[
    ['promptCache', 'Boolean', 'Enables or disables Anthropic’s built-in prompt caching.', ''],
  ]}
/>

**Example:**
```yaml
preset:
  promptCache: true
```

---

#### region

> **Supported by:** `bedrock`  
> (Used to specify an AWS region for Amazon Bedrock)

<OptionTable
  options={[
    ['region', 'String', 'AWS region for Amazon Bedrock endpoints.', ''],
  ]}
/>

**Example:**
```yaml
preset:
  region: "us-east-1"
```

---

#### maxTokens

> **Supported by:** `bedrock`  
> (Used in place of `max_tokens`)

<OptionTable
  options={[
    ['maxTokens', 'Number', 'Maximum output tokens for Amazon Bedrock endpoints.', ''],
  ]}
/>

**Example:**
```yaml
preset:
  maxTokens: 1024
```



================================================
FILE: pages/docs/configuration/librechat_yaml/object_structure/ocr.mdx
================================================
# OCR Config Object Structure

## Overview

The `ocr` object allows you to configure Optical Character Recognition (OCR) settings for the application, enabling the extraction of text from images. This section provides a detailed breakdown of the `ocr` object structure.

There are 4 main fields under `ocr`:

  - `mistralModel`
  - `apiKey`
  - `baseURL`
  - `strategy`

**Notes:**

- If using the Mistral OCR API, you don't need to edit your `librechat.yaml` file.
    - You only need the following environment variables to get started: `OCR_API_KEY` and `OCR_BASEURL`.
- OCR functionality allows the application to extract text from images, which can then be processed by AI models.
- The default strategy is `mistral_ocr`, which uses Mistral's OCR capabilities.
- You can also configure a custom OCR service by setting the strategy to `custom_ocr`.
- If using the default Mistral OCR, you may optionally specify a specific Mistral model to use.
- Environment variable parsing is supported for `apiKey`, `baseURL`, and `mistralModel` parameters.
- A `user_provided` strategy option is planned for future releases but is not yet implemented.

## Example

```yaml filename="ocr"
ocr:
  mistralModel: "mistral-ocr-latest"
  apiKey: "your-mistral-api-key"
  strategy: "mistral_ocr"
```

Example with custom OCR:

```yaml filename="ocr with custom OCR"
ocr:
  apiKey: "your-custom-ocr-api-key"
  baseURL: "https://your-custom-ocr-service.com/api"
  strategy: "custom_ocr"
```

## mistralModel

<OptionTable
  options={[
    ['mistralModel', 'String', 'The Mistral model to use for OCR processing.', 'Optional. Specifies which Mistral model should be used when the strategy is set to mistral_ocr.'],
  ]}
/>

```yaml filename="ocr / mistralModel"
ocr:
  mistralModel: "mistral-ocr-latest"
```

## apiKey

<OptionTable
  options={[
    ['apiKey', 'String', 'The API key for the OCR service.', 'Optional. Defaults to the environment variable OCR_API_KEY if not specified.'],
  ]}
/>

```yaml filename="ocr / apiKey"
ocr:
  apiKey: "your-ocr-api-key"
```

## baseURL

<OptionTable
  options={[
    ['baseURL', 'String', 'The base URL for the OCR service API.', 'Optional. Defaults to the environment variable OCR_BASEURL if not specified.'],
  ]}
/>

```yaml filename="ocr / baseURL"
ocr:
  baseURL: "https://your-ocr-service.com/api"
```

## strategy

<OptionTable
  options={[
    ['strategy', 'String', 'The OCR strategy to use.', 'Determines which OCR service to use. Options are "mistral_ocr" or "custom_ocr". Defaults to "mistral_ocr".'],
  ]}
/>

```yaml filename="ocr / strategy"
ocr:
  strategy: "custom_ocr"
```

**Available Strategies:**

- `mistral_ocr`: Uses Mistral's OCR capabilities.
- `custom_ocr`: Uses a custom OCR service specified by the baseURL.



================================================
FILE: pages/docs/configuration/librechat_yaml/object_structure/registration.mdx
================================================
# Registration Object Structure

## Example

```yaml filename="Registration Object Structure"
# Example Registration Object Structure
registration:
  socialLogins: ["google", "facebook", "github", "discord", "openid"]
  allowedDomains:
    - "gmail.com"
    - "protonmail.com"
```

## socialLogins

**Key:**
<OptionTable
  options={[
    ['socialLogins', 'Array of Strings', 'Defines the available social login providers and their display order.', 'The order of the providers in the list determines their appearance order on the login/registration page. Each provider listed must be properly configured within the system to be active and available for users. This configuration allows for a tailored authentication experience, emphasizing the most relevant or preferred social login options for your user base.'],
  ]}
/>


**Example:**
```yaml filename="registration / socialLogins"
socialLogins: ["google", "facebook", "github", "discord", "openid"]
```

## allowedDomains

**Key:**
<OptionTable
  options={[
    ['allowedDomains', 'Array of Strings', 'A list specifying allowed email domains for registration.', 'Users with email domains not listed will be restricted from registering.'],
  ]}
/>

**Required**

**Example:**
```yaml filename="registration / allowedDomains"
allowedDomains:
  - "gmail.com"
  - "protonmail.com"
```


================================================
FILE: pages/docs/configuration/librechat_yaml/object_structure/shared_endpoint_settings.mdx
================================================
# Shared Endpoint Settings

This page describes the shared configuration settings for all endpoints. The settings highlighted here are available to all configurations under the ["Endpoints"](/docs/configuration/librechat_yaml/object_structure/config#endpoints) field unless noted otherwise.

## Example Configuration

```yaml filename="Shared Endpoint Settings"
endpoints:
  openAI:
    streamRate: 25
    titleModel: "gpt-4o-mini"
  anthropic:
    streamRate: 25
    titleModel: "claude-3-5-haiku-20241022"
  bedrock:
    streamRate: 25
    titleModel: "us.amazon.nova-lite-v1:0"
  google:
    streamRate: 1
    titleModel: "gemini-2.0-flash-lite"
  azureOpenAI:
    streamRate: 20
    titleModel: "gpt-4o-mini"
  assistants:
    streamRate: 30
  azureAssistants:
    streamRate: 30
  # the `all` setting would override all the above values, making them unnecessary to be set
  all:
    streamRate: 20
```

## streamRate

**Key:**
<OptionTable
  options={[
    ['streamRate', 'Number', 'The rate at which data is streamed from the endpoint. Useful for controlling the pace of streaming data.', 'streamRate: 25'],
  ]}
/>

**Default:** 1

> Allows for streaming data at the fastest rate possible while allowing the system to wait for the next tick

## titleModel

**Key:**
<OptionTable
  options={[
    ['titleModel', 'String', 'Specifies the model to use for titles.', 'Defaults to system default for the current endpoint if omitted. May cause issues if the system default model is not available. You can also dynamically use the current conversation model by setting it to "current_model".'],
  ]}
/>

**Default:** System default for the current endpoint
**Notes:** It isn't yet possible to set a `titleModel` for the `all` key. This setting must be set individually for each endpoint.

---

**Notes:**
- The `all` setting would override all individual endpoint values, making those specific settings unnecessary if used.
- The value can be customized for each endpoint or set globally using the `all` key.
- Recommended values are between 25-40 for a smooth streaming experience
- Using a higher rate is a must when serving the app to many users at scale.

---

# Endpoint Settings
- [Custom Endpoints](/docs/configuration/librechat_yaml/object_structure/custom_endpoint)
- [OpenAI](/docs/configuration/pre_configured_ai/openai)
- [Anthropic](/docs/configuration/pre_configured_ai/anthropic)
- [Bedrock](/docs/configuration/pre_configured_ai/bedrock)
- [Google](/docs/configuration/pre_configured_ai/google)
- [Azure OpenAI](/docs/configuration/librechat_yaml/object_structure/azure_openai)
- [Assistants](/docs/configuration/librechat_yaml/object_structure/assistants_endpoint)


================================================
FILE: pages/docs/configuration/mongodb/mongodb_atlas.mdx
================================================
---
title: MongoDB Atlas
description: How to set up an online MongoDB database for LibreChat using MongoDB Atlas
---

# Setting Up an Online MongoDB Database

## 1. Create a MongoDB Atlas Account
1. Open a new tab in your web browser and go to [account.mongodb.com/account/register](https://account.mongodb.com/account/register).
2. Fill out the required information and create your account.

## 2. Create a New Project
1. After setting up your account, click on the "New Project" button and give it a name (e.g., "LibreChat").

## 3. Build a Database
1. Click on the "Build a Database" button.

## 4. Choose the Free Tier
1. Select the "Shared Clusters" option, which is the free tier.

## 5. Name Your Cluster
1. Give your cluster a name (e.g., "LibreChat-Cluster") and click "Create Cluster".

## 6. Set Up Database Credentials
1. Click on the "Database Access" option in the sidebar.
2. Click on the "Add New Database User" button.
3. Enter a username and a secure password, then click "Add User".

## 7. Configure Network Access
1. Click on the "Network Access" option in the sidebar.
2. Click on the "Add IP Address" button.
3. Select "Allow Access from Anywhere" and click "Confirm".

## 8. Get Your Connection String
1. Click on the "Databases" option in the sidebar.
2. Click on the "Connect" button.
3. Select "Connect Your Application".
4. Copy the connection string provided.
5. Replace `<password>` in the connection string with the password you set in Step 6. Remove the `<>` characters around the password.
6. Remove `&w=majority` from the end of the connection string.
7. Add your desired database name (e.g., "LibreChat") after the host name in the connection string.

Your final connection string should look something like this:

```sh filename="Connection String"
mongodb+srv://username:password@cluster-url.mongodb.net/LibreChat?retryWrites=true
```

## 9. Update the .env File
1. In your LibreChat project, open the `.env` file.
2. Find the `MONGO_URI` variable and paste your connection string:

```sh filename=".env"
MONGO_URI=mongodb+srv://username:password@cluster-url.mongodb.net/LibreChat?retryWrites=true
```

That's it! You've now set up an online MongoDB database for LibreChat using MongoDB Atlas, and you've updated your LibreChat application to use this database connection. Your application should now be able to connect to the online MongoDB database.

## Note about Docker

<Callout type="note" title="Docker">
**Note:** If you're using LibreChat with Docker, you'll need to utilize the `docker-compose.override.yml` file. This override file allows you to prevent the installation of the included MongoDB instance. Instead, your LibreChat Docker container will use the online MongoDB Atlas database you've just set up. For more information on using the override file, please refer to our [Docker Override Guide](/docs/configuration/docker_override).
</Callout>




================================================
FILE: pages/docs/configuration/mongodb/mongodb_auth.mdx
================================================
---
title: MongoDB Authentication
description: Setup authentication on your docker mongodb with the docker-compose.override.yml file
---

# MongoDB Authentication (Docker)

This guide will demonstrate how to use the `docker-compose.override.yml` file to allows us to enable explicit authentication for MongoDB.

For more info about the override file, please consult: [Docker Compose Override](/docs/configuration/docker_override)

**Notes:**

- The default configuration is secure by blocking external port access, but we can take it a step further with access credentials.
- As noted by the developers of MongoDB themselves, authentication in MongoDB is fairly complex. We will be taking a simple approach that will be good enough for most cases, especially for existing configurations of LibreChat. To learn more about how mongodb authentication works with docker, see here: https://hub.docker.com/_/mongo/
- This guide focuses exclusively on terminal-based setup procedures.
- While the steps outlined may also be applicable to Docker Desktop environments, or with non-Docker, local MongoDB, or other container setups, details specific to those scenarios are not provided.

**There are 3 basic steps:**

- Create an admin user within your mongodb container
- Enable authentication and create a "readWrite" user for "LibreChat"
- Configure the MONGO_URI with newly created user

## TL;DR

These are all the necessary commands if you'd like to run through these quickly or for reference:

<Callout type="abstract" title="TL;DR - All Commands" emoji='💻' collapsible>

```bash filename="Shut down container initially"
docker compose down
```

```bash filename="Start MongoDB container"
docker compose up -d mongodb
```

```bash filename="Open MongoDB shell on 'chat-mongodb' container"
docker exec -it chat-mongodb mongosh
```

```bash filename="Switch to admin database"
use admin
```

```bash filename="Create new admin user"
db.createUser({ user: "adminUser", pwd: "securePassword", roles: ["userAdminAnyDatabase", "readWriteAnyDatabase"] })
```

```bash filename="Exit MongoDB shell"
exit
```

```bash filename="Shut down container after setup"
docker compose down
```

```bash filename="Restart MongoDB container with authentication"
docker compose up -d mongodb
```

```bash filename="Log into MongoDB shell with credentials"
docker exec -it chat-mongodb mongosh -u adminUser -p securePassword --authenticationDatabase admin
```

```bash filename="Switch to LibreChat database"
use LibreChat
```

```bash filename="Create user in LibreChat database"
db.createUser({ user: 'user', pwd: 'userpasswd', roles: [ { role: "readWrite", db: "LibreChat" } ] });
```

```bash filename="Exit MongoDB shell after creating user"
exit
```

```bash filename="Shut down container after user creation"
docker compose down
```

```bash filename="Start all services with final settings"
docker compose up
```
</Callout>

### Example

Example `docker-compose.override.yml` file using the [`librechat.yaml` config file](/docs/configuration/librechat_yaml), [MongoDB Authentication](/docs/configuration/mongodb/mongodb_auth), and `mongo-express` for [managing your MongoDB database](/blog/2023-11-30_mongoexpress):

<Callout type="example" title="Example `docker-compose.override.yml` file" collapsible>

```yaml filename="docker-compose.override.yml"
version: '3.4'

services:
  api:
    volumes:
      - ./librechat.yaml:/app/librechat.yaml
    environment:
      - MONGO_URI=mongodb://user:userpasswd@mongodb:27017/LibreChat
  mongodb:
    command: mongod --auth
  mongo-express:
    image: mongo-express
    container_name: mongo-express
    environment:
      ME_CONFIG_MONGODB_SERVER: mongodb
      ME_CONFIG_BASICAUTH_USERNAME: admin
      ME_CONFIG_BASICAUTH_PASSWORD: password
      ME_CONFIG_MONGODB_URL: 'mongodb://adminUser:securePassword@mongodb:27017'
      ME_CONFIG_MONGODB_ADMINUSERNAME: adminUser
      ME_CONFIG_MONGODB_ADMINPASSWORD: securePassword
    ports:
      - '8081:8081'
    depends_on:
      - mongodb
    restart: always
```
</Callout>

## Step 1: Creating an Admin User

First, we must stop the default containers from running, and only run the mongodb container.

```bash filename="Stop all running containers"
docker compose down
```

```bash filename="Start mongodb container in detached mode"
docker compose up -d mongodb
```

> Note: The `-d` flag detaches the current terminal instance as the container runs in the background. If you would like to see the mongodb log outputs, omit it and continue in a separate terminal.

Once running, we will enter the container's terminal and execute `mongosh`:

```bash filename="Connect to the MongoDB shell"
docker exec -it chat-mongodb mongosh
```
You should see the following output:

```bash filename="Output"
~/LibreChat$ docker exec -it chat-mongodb mongosh
Current Mongosh Log ID: 65bfed36f7d7e3c2b01bcc3d
Connecting to:          mongodb://127.0.0.1:27017/?directConnection=true&serverSelectionTimeoutMS=2000&appName=mongosh+2.1.1
Using MongoDB:          7.0.4
Using Mongosh:          2.1.1

For mongosh info see: https://docs.mongodb.com/mongodb-shell/

test> 
```

Optional: While we're here, we can disable telemetry for mongodb if desired, which is anonymous usage data collected and sent to MongoDB periodically:

Execute the command below.

> Notes:
> - All subsequent commands should be run in the current terminal session, regardless of the environment (Docker, Linux, `mongosh`, etc.)
> - I will represent the actual terminal view with # example input/output or simply showing the output in some cases

Command:

```bash filename="Disable Telemetry"
disableTelemetry()
```

Example input/output:

```bash filename="example input/output"
test> disableTelemetry()
Telemetry is now disabled.
```

Now, we must access the admin database, which mongodb creates by default to create our admin user:

```bash filename="Switch to Admin Database"
use admin
```

> switched to db admin

Replace the credentials as desired and keep in your secure records for the rest of the guide.

Run command to create the admin user:

```bash filename="Create Admin User"
db.createUser({ user: "adminUser", pwd: "securePassword", roles: ["userAdminAnyDatabase", "readWriteAnyDatabase"] })
```

You should see an "ok" output.

You can also confirm the admin was created by running `show users`:

```bash filename="example input/output"
admin> show users
[
  {
    _id: 'admin.adminUser',
    userId: UUID('86e90441-b5b7-4043-9662-305540dfa6cf'),
    user: 'adminUser',
    db: 'admin',
    roles: [
      { role: 'userAdminAnyDatabase', db: 'admin' },
      { role: 'readWriteAnyDatabase', db: 'admin' }
    ],
    mechanisms: [ 'SCRAM-SHA-1', 'SCRAM-SHA-256' ]
  }
]
```

:warning: **Important:** if you are using `mongo-express` to [manage your database (guide here)](../../features/manage_your_database.md), you need the additional permissions for the `mongo-express` service to run correctly:

```bash filename="Grant Roles to Admin User"
db.grantRolesToUser("adminUser", ["clusterAdmin", "readAnyDatabase"])
```

Exit the Mongosh/Container Terminal by running `exit`:
```bash filename="Exit the Mongosh/Container Terminal"
admin> exit
```

And shut down the running container:
```bash filename="Shut down the running container"
docker compose down
```

## Step 2: Enabling Authentication and Creating a User with `readWrite` Access

We must now create/edit the `docker-compose.override.yml` file to enable authentication for our mongodb container. You can use this configuration to start or reference:

```yaml filename="docker-compose.override.yml"
version: '3.4'

services:
  api:
    volumes:
      - ./librechat.yaml:/app/librechat.yaml # Optional for using the librechat config file.
  mongodb:
    command: mongod --auth # <--- Add this to enable authentication
```

After configuring the override file as above, run the mongodb container again:

```bash filename="Start the MongoDB container"
docker compose up -d mongodb
```

And access mongosh as the admin user:

```bash filename="Connect to MongoDB container using mongo shell"
docker exec -it chat-mongodb mongosh -u adminUser -p securePassword --authenticationDatabase admin
```

Confirm you are authenticated:
```bash filename="Check MongoDB Connection Status Command"
db.runCommand({ connectionStatus: 1 })
```

```bash filename="example input/output"
test> db.runCommand({ connectionStatus: 1 })
{
  authInfo: {
    authenticatedUsers: [ { user: 'adminUser', db: 'admin' } ],
    authenticatedUserRoles: [
      { role: 'readWriteAnyDatabase', db: 'admin' },
      { role: 'userAdminAnyDatabase', db: 'admin' }
    ]
  },
  ok: 1
}
test>
```

Switch to the "LibreChat" database

> Note: This the default database unless you changed it via the MONGO_URI; default URI: `MONGO_URI=mongodb://mongodb:27017/LibreChat`

```bash filename="Switch to the LibreChat database"
use LibreChat
```

Now we'll create the actual credentials to be used by our Mongo connection string, which will be limited to read/write access of the "LibreChat" database. As before, replace the example with your desired credentials:

`db.createUser({ user: 'user', pwd: 'userpasswd', roles: [ { role: "readWrite", db: "LibreChat" } ] });`

You should see an "ok" output again.

You can verify the user creation with the `show users` command.

Exit the Mongosh/Container Terminal again with `exit`, and bring the container down:

```bash filename="End the current shell session"
exit
```

```bash filename="Stop the current Docker Compose services"
docker compose down
```

I had an issue where the newly created user would not persist after creating it. To solve this, I simply repeated the steps to ensure it was created. Here they are for your convenience:

```bash filename="Shut down container"
docker compose down
```

```bash filename="Start Mongo container"
docker compose up -d mongodb
```

```bash filename="Access MongoDB shell as admin"
docker exec -it chat-mongodb mongosh -u adminUser -p securePassword --authenticationDatabase admin
```

```bash filename="Switch to LibreChat database"
use LibreChat
```

```bash filename="Show current users in LibreChat database"
show users
```

```bash filename="Create a new user in LibreChat database"
db.createUser({ user: 'user', pwd: 'userpasswd', roles: [ { role: "readWrite", db: "LibreChat" } ] });
```

If it's still not persisting, you can try running the commands with all containers running, but note that the `LibreChat` container will be in an error/retrying state.

## Step 3: Update the `MONGO_URI` to Use the New Credentials

Finally, we add the new connection string with our newly created credentials to our `docker-compose.override.yml` file under the `api` service:

```yaml filename="docker-compose.override.yml"
    environment:
      - MONGO_URI=mongodb://user:userpasswd@mongodb:27017/LibreChat
```

So our override file looks like this now:

```yaml filename="docker-compose.override.yml"
version: '3.4'

services:
  api:
    volumes:
      - ./librechat.yaml:/app/librechat.yaml
    environment:
      - MONGO_URI=mongodb://user:userpasswd@mongodb:27017/LibreChat
  mongodb:
    command: mongod --auth
```

You should now run `docker compose up` successfully authenticated with read/write access to the LibreChat database

Example successful connection:
```bash filename="successful connection example"
LibreChat         | 2024-02-04 20:59:43 info: Server listening on all interfaces at port 3080. Use http://localhost:3080 to access it
chat-mongodb      | {"t":{"$date":"2024-02-04T20:59:53.880+00:00"},"s":"I",  "c":"NETWORK",  "id":22943,   "ctx":"listener","msg":"Connection accepted","attr":{"remote":"192.168.160.4:58114","uuid":{"uuid":{"$uuid":"027bdc7b-a3f4-429a-80ee-36cd172058ec"}},"connectionId":17,"connectionCount":10}}
```

If you're having Authentication errors, run the last part of Step 2 again. I'm not sure why it's finicky but it will work after a few tries.




================================================
FILE: pages/docs/configuration/mongodb/mongodb_community.mdx
================================================
---
title: "MongoDB Community Server"
description: "Setting up a MongoDB Community Server for your LibreChat database."
---

# Setting Up a MongoDB Community Server

## 1. Download MongoDB Community Server
- Go to the official MongoDB website: [https://www.mongodb.com/try/download/community](https://www.mongodb.com/try/download/community)
- Select your operating system and download the appropriate package.

## 2. Install MongoDB Community Server
Follow the installation instructions for your operating system to install MongoDB Community Server.

## 3. Create a Data Directory
MongoDB requires a data directory to store its data files. Create a directory on your system where you want to store the MongoDB data files (e.g., `/path/to/data/directory`).

## 4. Start the MongoDB Server
- Open a terminal or command prompt.
- Navigate to the MongoDB installation directory (e.g., `/path/to/mongodb/bin`).
- Run the following command to start the MongoDB server, replacing `/path/to/data/directory` with the path to the data directory you created in Step 3:

```sh filename="Start the MongoDB Server"
./mongod --dbpath=/path/to/data/directory
```

## 5. Configure MongoDB for Remote Access (Optional)
If you plan to access the MongoDB server from a remote location (e.g., a different machine or a LibreChat instance hosted elsewhere), you need to configure MongoDB for remote access:

- Create a configuration file (e.g., `/path/to/mongodb/config/mongodb.conf`) with the following content:

```yaml filename="mongodb.conf"
# Network interfaces
net:
  port: 27017
  bindIp: 0.0.0.0
```
- Stop the MongoDB server if it's running.
- Start the MongoDB server with the configuration file:

```yaml filename="Start the MongoDB server"
./mongod --config /path/to/mongodb/config/mongodb.conf
```

## 6. Get the Connection String
The connection string for your MongoDB Community Server will be in the following format:

```sh filename="Connection String"
mongodb://[hostname]:[port]
```
Replace `[hostname]` with the IP address or hostname of the machine where MongoDB is running, and `[port]` with the port number (usually 27017).

## 7. Update the .env File
- In your LibreChat project, open the `.env` file.
- Find the `MONGO_URI` variable and paste your connection string:

```sh filename=".env"
MONGO_URI=mongodb://[hostname]:[port]
```

That's it! You've now set up a MongoDB Community Server for LibreChat. Your LibreChat application should be able to connect to the local MongoDB instance using the connection string you provided.

## Note about Docker

<Callout type="note" title="Docker">
**Note:** If you're using LibreChat with Docker, you'll need to utilize the `docker-compose.override.yml` file. This override file allows you to prevent the installation of the included MongoDB instance. Instead, your LibreChat Docker container will use the local MongoDB Community Server database you've just set up. For more information on using the override file, please refer to our [Docker Override Guide](/docs/configuration/docker_override).

**Example:**
```yaml filename="docker-compose.override.yml"
services:
  api:
    environment:
    - MONGO_URI=mongodb://user:pass@host1:27017,host2:27017,host3:27017/LibreChat?authSource=admin&replicaSet=setname
```
</Callout>



================================================
FILE: pages/docs/configuration/pre_configured_ai/_meta.ts
================================================
export default {
  index: 'Intro',
  bedrock: 'AWS Bedrock',
}



================================================
FILE: pages/docs/configuration/pre_configured_ai/anthropic.mdx
================================================
# Anthropic

- Create an account at **[https://console.anthropic.com/](https://console.anthropic.com/)**
- Go to **[https://console.anthropic.com/account/keys](https://console.anthropic.com/account/keys)** and get your api key
- You will need to set the following environment variable to your key or you can set it to `user_provided` for users to provide their own.

```bash filename=".env"
ANTHROPIC_API_KEY=user_provided
```

- You can determine which models you would like to have available with `ANTHROPIC_MODELS`.

```bash filename=".env"
ANTHROPIC_MODELS=claude-3-opus-20240229,claude-3-sonnet-20240229,claude-3-haiku-20240307,claude-2.1,claude-2,claude-1.2,claude-1,claude-1-100k,claude-instant-1,claude-instant-1-100k
```


================================================
FILE: pages/docs/configuration/pre_configured_ai/assistants.mdx
================================================
# Assistants

- The [Assistants API by OpenAI](https://platform.openai.com/docs/assistants/overview) has a dedicated endpoint.
- The Assistants API enables the creation of AI assistants, offering functionalities like code interpreter, knowledge retrieval of files, and function execution.
    - [Read here for an in-depth documentation](https://platform.openai.com/docs/assistants/overview) of the feature, how it works, what it's capable of.
- As with the regular [OpenAI API](/docs/configuration/pre_configured_ai/openai), go to **[https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)** to get a key.
- You will need to set the following environment variable to your key or you can set it to `user_provided` for users to provide their own.

```bash filename=".env"
ASSISTANTS_API_KEY=your-key
```

- You can determine which models you would like to have available with `ASSISTANTS_MODELS`; otherwise, the models list fetched from OpenAI will be used (only Assistants API compatible models will be shown).

```bash filename=".env"
ASSISTANTS_MODELS=gpt-3.5-turbo-0125,gpt-3.5-turbo-16k-0613,gpt-3.5-turbo-16k,gpt-3.5-turbo,gpt-4,gpt-4-0314,gpt-4-32k-0314,gpt-4-0613,gpt-3.5-turbo-0613,gpt-3.5-turbo-1106,gpt-4-0125-preview,gpt-4-turbo-preview,gpt-4-1106-preview
```

- If necessary, you can also set an alternate base URL instead of the official one with `ASSISTANTS_BASE_URL`, which is similar to the OpenAI counterpart `OPENAI_REVERSE_PROXY`

```bash filename=".env"
ASSISTANTS_BASE_URL=http://your-alt-baseURL:3080/
```

- There is additional, optional configuration, depending on your needs, such as disabling the assistant builder UI, that are available via the `librechat.yaml` [custom config file](/docs/configuration/librechat_yaml/object_structure/assistants_endpoint):
    - Control the visibility and use of the builder interface for assistants. [More info](/docs/configuration/librechat_yaml/object_structure/assistants_endpoint#disablebuilder)
    - Specify the polling interval in milliseconds for checking run updates or changes in assistant run states. [More info](/docs/configuration/librechat_yaml/object_structure/assistants_endpoint#pollintervalms)
    - Set the timeout period in milliseconds for assistant runs. Helps manage system load by limiting total run operation time. [More info](/docs/configuration/librechat_yaml/object_structure/assistants_endpoint#timeoutms)
    - Specify which assistant Ids are supported or excluded [More info](/docs/configuration/librechat_yaml/object_structure/assistants_endpoint#supportedids)

## Strict function calling
With librechat you can add add the 'x-strict': true flag at operation-level in the openapi spec for actions.
This will automatically generate function calls with 'strict' mode enabled.
Note that strict mode supports only a partial subset of json. Read https://platform.openai.com/docs/guides/structured-outputs/some-type-specific-keywords-are-not-yet-supported for details.

For example:
```json filename="mathapi.json"
{
  "openapi": "3.1.0",
  "info": {
    "title": "Math.js API",
    "description": "API for performing mathematical operations, such as addition, subtraction, etc.",
    "version": "1.0.0"
  },
  "servers": [
    {
      "url": "https://api.mathjs.org/v4"
    }
  ],
  "paths": {
    "/": {
      "post": {
        "summary": "Evaluate a mathematical expression",
        "description": "Sends a mathematical expression in the request body to evaluate.",
"operationId": "math",
"x-strict": true,
"parameters": [
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "expr": {
                    "type": "string",
                    "description": "The mathematical expression to evaluate (e.g., `2+3`)."
                  }
                },
                "required": ["expr"]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "The result of the evaluated expression.",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "result": {
                      "type": "number",
                      "description": "The evaluated result of the expression."
                    }
                  }
                }
              }
            }
          },
          "400": {
            "description": "Invalid expression provided.",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "error": {
                      "type": "string",
                      "description": "Error message describing the invalid expression."
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```

<Callout type="note" title="Notes">
**Notes:**
- At the time of writing, only the following models support the [Retrieval](https://platform.openai.com/docs/assistants/tools/knowledge-retrieval) capability:
    - gpt-3.5-turbo-0125
    - gpt-4-0125-preview
    - gpt-4-turbo-preview
    - gpt-4-1106-preview
    - gpt-3.5-turbo-1106
- Vision capability is not yet supported.
- If you have previously set the [`ENDPOINTS` value in your .env file](./dotenv.md#endpoints), you will need to add the value `assistants`
</Callout>




================================================
FILE: pages/docs/configuration/pre_configured_ai/bedrock.mdx
================================================
# AWS Bedrock

Head to the [AWS docs](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html) to sign up for AWS and setup your credentials.

You’ll also need to turn on model access for your account, which you can do by [following these instructions](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html).

## Authentication

- You will need to set the following environment variables:

```bash filename=".env"
BEDROCK_AWS_DEFAULT_REGION=us-east-1
BEDROCK_AWS_ACCESS_KEY_ID=your_access_key_id
BEDROCK_AWS_SECRET_ACCESS_KEY=your_secret_access_key
```

**Note:** You can also omit the access keys in order to use the default AWS credentials chain but you must set the default region:

```bash filename=".env"
BEDROCK_AWS_DEFAULT_REGION=us-east-1
```

Doing so prompts the credential provider to find credentials from the following sources (listed in order of precedence):

- Environment variables exposed via process.env
- SSO credentials from token cache
- Web identity token credentials
- Shared credentials and config ini files
- The EC2/ECS Instance Metadata Service

The default credential provider will invoke one provider at a time and only continue to the next if no credentials have been located.

For example, if the process finds values defined via the `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables, the files at ~/.aws/credentials and ~/.aws/config will not be read, nor will any messages be sent to the Instance Metadata Service.

## Configuring models

- You can optionally specify which models you want to make available with `BEDROCK_AWS_MODELS`:

```bash filename=".env"
BEDROCK_AWS_MODELS=anthropic.claude-3-5-sonnet-20240620-v1:0,meta.llama3-1-8b-instruct-v1:0
```

Note: If omitted, all known, supported model IDs will be included automatically.

- See all Bedrock model IDs here:
    - **[https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns)**

## Additional Configuration

You can further configure the Bedrock endpoint in your [`librechat.yaml`](/docs/configuration/librechat_yaml/) file:

```yaml
endpoints:
  bedrock:
    availableRegions:
      - "us-east-1"
      - "us-west-2"
    streamRate: 35
    titleModel: 'anthropic.claude-3-haiku-20240307-v1:0'
```

- `streamRate`: (Optional) Set the rate of processing each new token in milliseconds.
    - This can help stabilize processing of concurrent requests and provide smoother frontend stream rendering.

- `titleModel`: (Optional) Specify the model to use for generating conversation titles.
    - Recommended: `anthropic.claude-3-haiku-20240307-v1:0`.
    - Omit or set as `current_model` to use the same model as the chat.

- `availableRegions`: (Optional) Specify the AWS regions you want to make available.
    - If provided, users will see a dropdown to select the region. If not selected, the default region is used.
    - ![image](https://github.com/user-attachments/assets/6f3c5e82-9c6b-4643-8487-07db1061ba49)

## Notes

- The following models are not supported due to lack of streaming capability:
  - ai21.j2-mid-v1

- The following models are not supported due to lack of conversation history support:
  - ai21.j2-ultra-v1
  - cohere.command-text-v14
  - cohere.command-light-text-v14


================================================
FILE: pages/docs/configuration/pre_configured_ai/google.mdx
================================================
# Google

For the Google Endpoint, you can either use the **Generative Language API** (for Gemini models), or the **Vertex AI API** (for Gemini, PaLM2 & Codey models).

The Generative Language API uses an API key, which you can get from **Google AI Studio**.

For Vertex AI, you need a Service Account JSON key file, with appropriate access configured.

Instructions for both are given below.

## Generative Language API (Gemini)

**[See here for Gemini API pricing and rate limits](https://ai.google.dev/pricing)**

⚠️ While Google models are free, they are using your input/output to help improve the model, with data de-identified from your Google Account and API key.
⚠️ During this period, your messages “may be accessible to trained reviewers.”

To use Gemini models through Google AI Studio, you'll need an API key. If you don't already have one, create a key in Google AI Studio.

Get an API key here: **[aistudio.google.com](https://aistudio.google.com/app/apikey)**

Once you have your key, provide the key in your .env file, which allows all users of your instance to use it.

```bash filename=".env"
GOOGLE_KEY=mY_SeCreT_w9347w8_kEY
```

Or, you can make users provide it from the frontend by setting the following:
```bash filename=".env"
GOOGLE_KEY=user_provided
```

Since fetching the models list isn't yet supported, you should set the models you want to use in the .env file.

For your convenience, these are the latest models as of 5/18/24 that can be used with the Generative Language API:

```bash filename=".env"
GOOGLE_MODELS=gemini-1.5-flash-latest,gemini-1.0-pro,gemini-1.0-pro-001,gemini-1.0-pro-latest,gemini-1.0-pro-vision-latest,gemini-1.5-pro-latest,gemini-pro,gemini-pro-vision
```

<Callout type="note" title="Notes:">
Notes:
- A gemini-pro model or `gemini-pro-vision` are required in your list for attaching images.
- Using LibreChat, PaLM2 and Codey models can only be accessed through Vertex AI, not the Generative Language API.
    - Only models that support the `generateContent` method can be used natively with LibreChat + the Gen AI API.
- Selecting `gemini-pro-vision` for messages with attachments is not necessary as it will be switched behind the scenes for you
- Since `gemini-pro-vision`does not accept non-attachment messages, messages without attachments are automatically switched to use `gemini-pro` (otherwise, Google responds with an error)
- With the Google endpoint, you cannot use both Vertex AI and Generative Language API at the same time. You must choose one or the other.
- Some PaLM/Codey models and `gemini-pro-vision` may fail when `maxOutputTokens` is set to a high value. If you encounter this issue, try reducing the value through the conversation parameters.
</Callout>

Setting `GOOGLE_KEY=user_provided` in your .env file sets both the Vertex AI Service Account JSON key file and the Generative Language API key to be provided from the frontend like so:

![image](https://github.com/danny-avila/LibreChat/assets/110412045/728cbc04-4180-45a8-848c-ae5de2b02996)

## Vertex AI

**[See here for Vertex API pricing and rate limits](https://cloud.google.com/vertex-ai/generative-ai/pricing)**

To setup Google LLMs (via Google Cloud Vertex AI), first, signup for Google Cloud: **[cloud.google.com](https://cloud.google.com/)**

You can usually get **$300 starting credit**, which makes this option free for 90 days.

1. Once signed up, Enable the Vertex AI API on Google Cloud:
  - Go to **[Vertex AI page on Google Cloud console](https://console.cloud.google.com/vertex-ai)**
  - Click on `Enable API` if prompted
2. Create a Service Account with Vertex AI role:
  - **[Click here to create a Service Account](https://console.cloud.google.com/projectselector/iam-admin/serviceaccounts/create?walkthrough_id=iam--create-service-account#step_index=1)**
  - **Select or create a project**
  - Enter a service account ID (required), name and description are optional
      - ![image](https://github.com/danny-avila/LibreChat/assets/110412045/0c5cd177-029b-44fa-a398-a794aeb09de6)
  - Click on "Create and Continue" to give at least the "Vertex AI User" role
      - ![image](https://github.com/danny-avila/LibreChat/assets/110412045/22d3a080-e71e-446e-8485-bcc5bf558dbb)
  - **Click on "Continue/Done"**
3. Create a JSON key to Save in your Project Directory:
  - **Go back to [the Service Accounts page](https://console.cloud.google.com/projectselector/iam-admin/serviceaccounts)**
  - **Select your service account**
  - Click on "Keys"
       - ![image](https://github.com/danny-avila/LibreChat/assets/110412045/735a7bbe-25a6-4b4c-9bb5-e0d8aa91be3d)
  - Click on "Add Key" and then "Create new key"
       - ![image](https://github.com/danny-avila/LibreChat/assets/110412045/cfbb20d3-94a8-4cd1-ac39-f9cd8c2fceaa)
  - **Choose JSON as the key type and click on "Create"**
  - **Download the key file and rename it as 'auth.json'**
  - **Save it within the project directory, in `/api/data/`**
       - ![image](https://github.com/danny-avila/LibreChat/assets/110412045/f5b8bcb5-1b20-4751-81a1-d3757a4b3f2f)

**Saving your JSON key file in the project directory which allows all users of your LibreChat instance to use it.**

Alternatively, you can make users provide it from the frontend by setting the following:

```bash filename=".env"
# Note: this configures both the Vertex AI Service Account JSON key file
# and the Generative Language API key to be provided from the frontend.
GOOGLE_KEY=user_provided
```

Since fetching the models list isn't yet supported, you should set the models you want to use in the .env file.

For your convenience, these are the latest models as of 5/18/24 that can be used with the Generative Language API:

```bash filename=".env"
GOOGLE_MODELS=gemini-1.5-flash-preview-0514,gemini-1.5-pro-preview-0514,gemini-1.0-pro-vision-001,gemini-1.0-pro-002,gemini-1.0-pro-001,gemini-pro-vision,gemini-1.0-pro
```

<Callout type="note" title="If you are using Docker">
If you're using docker and want to provide the `auth.json` file, you will need to also mount the volume in docker-compose.override.yml

```yaml filename="docker-compose.override.yml"
version: '3.4'

services:
  api:
    volumes:
    - type: bind
      source: ./api/data/auth.json
      target: /app/api/data/auth.json
```
</Callout>

## Google Safety Settings

To set safety settings for both Vertex AI and Generative Language API, you can set the following in your .env file:

```bash filename=".env"
GOOGLE_SAFETY_SEXUALLY_EXPLICIT=BLOCK_ONLY_HIGH
GOOGLE_SAFETY_HATE_SPEECH=BLOCK_ONLY_HIGH
GOOGLE_SAFETY_HARASSMENT=BLOCK_ONLY_HIGH
GOOGLE_SAFETY_DANGEROUS_CONTENT=BLOCK_ONLY_HIGH
```

You can also exclude safety settings by setting the following in your .env file, which will use the provider defaults. This can be helpful if you are having issues with specific safety settings.

```bash filename=".env"
GOOGLE_EXCLUDE_SAFETY_SETTINGS=true
```

NOTE: You do not have access to the BLOCK_NONE setting by default.
To use this restricted `HarmBlockThreshold` setting, you will need to either:
- (a) Get access through an allowlist via your Google account team
- (b) Switch your account type to monthly invoiced billing following this instruction:
    https://cloud.google.com/billing/docs/how-to/invoiced-billing



================================================
FILE: pages/docs/configuration/pre_configured_ai/index.mdx
================================================
---
title: AI Setup
description: Configuration guides for setting up AI providers in LibreChat
---

# AI Providers Setup

This section provides detailed configuration guides to help you set up various AI providers and their respective APIs and credentials in LibreChat.

## Endpoints Configuration

The term "Endpoints" refers to the AI provider, configuration, or API that you need to set up and integrate with LibreChat. Each endpoint has its own configuration process, which may involve obtaining API keys, credentials, or following specific setup instructions.

The following guides are available to help you configure different endpoints:

- **[AWS Bedrock](/docs/configuration/pre_configured_ai/bedrock)**
    - Setup AWS Bedrock integration
- **[Anthropic](/docs/configuration/pre_configured_ai/anthropic)**
    - Integrate Anthropic AI models
- **[OpenAI](/docs/configuration/pre_configured_ai/openai)**
    - Set up OpenAI API integration
- **[Google](/docs/configuration/pre_configured_ai/google)**
    - Configure Google AI services
- **[Assistants](/docs/configuration/pre_configured_ai/assistants)**
    - Enable and configure OpenAI's Assistants

## Custom Endpoint Configuration

The **[librechat.yaml Configuration Guides](/docs/configuration/librechat_yaml)** provides detailed instructions on how to configure custom endpoints within LibreChat.

In addition to the pre-configured endpoints, the librechat config file allows you to add and configure custom endpoints. This includes integrating with AI providers like Ollama, Mistral AI, Openrouter, and a multitude of other third-party services.

By following these configuration guides, you can seamlessly integrate various AI providers, unlock their capabilities, and enhance your LibreChat experience with the power of multiple AI models and services.


================================================
FILE: pages/docs/configuration/pre_configured_ai/openai.mdx
================================================
# OpenAI

To get your OpenAI API key, you need to:

- Go to **[https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)**
- Create an account or log in with your existing one
- Add a payment method to your account
- Go to https://platform.openai.com/api-keys to get a key.
- You will need to set the following environment variable to your key, or you can set it to `user_provided` for users to provide their own.

```bash filename=".env"
OPENAI_API_KEY=user_provided
```

- You can determine which models you would like to have available with `OPENAI_MODELS`
  - When `OPENAI_API_KEY` is set to `user_provided` → only the models put in this list will be available
    - ⚠️New models won't automatically show up; you'll need to add them to this list first
  - When `OPENAI_API_KEY` is set to the actual API key value → as long as `OPENAI_MODELS` is left commented-out, it will do an API call to find out what models are available, which should include any new ones

```bash filename=".env"
OPENAI_MODELS=gpt-3.5-turbo-0125,gpt-3.5-turbo-0301,gpt-3.5-turbo,gpt-4,gpt-4-0613,gpt-4-vision-preview,gpt-3.5-turbo-0613,gpt-3.5-turbo-16k-0613,gpt-4-0125-preview,gpt-4-turbo-preview,gpt-4-1106-preview,gpt-3.5-turbo-1106,gpt-3.5-turbo-instruct,gpt-3.5-turbo-instruct-0914,gpt-3.5-turbo-16k
```

**Notes:**

- Selecting a vision model for messages with attachments is not necessary as it will be switched behind the scenes for you. If you didn't outright select a vision model, it will only be used for the vision request and you should still see the non-vision model you had selected after the request is successful
- OpenAI Vision models allow for messages without attachments



================================================
FILE: pages/docs/configuration/tools/_meta.ts
================================================
export default {
  index: 'Intro',
}



================================================
FILE: pages/docs/configuration/tools/azure_ai_search.mdx
================================================
---
title: Azure AI Search
description: How to configure Azure AI Search for answers to your questions with assistance from GPT.
---
# Azure AI Search Plugin
Through the plugins endpoint, you can use Azure AI Search for answers to your questions with assistance from GPT.

## Configurations

### Required

To get started, you need to get a Azure AI Search endpoint URL, index name, and a API Key. You can then define these as follows in your `.env` file:

```env
AZURE_AI_SEARCH_SERVICE_ENDPOINT="..."
AZURE_AI_SEARCH_INDEX_NAME="..."
AZURE_AI_SEARCH_API_KEY="..."
```
Or you need to get an Azure AI Search endpoint URL, index name, and an API Key. You can define them during the installation of the plugin.

### AZURE_AI_SEARCH_SERVICE_ENDPOINT

This is the URL of the search endpoint. It can be obtained from the top page of the search service in the Cognitive Search management console (e.g., `https://example.search.windows.net`).

### AZURE_AI_SEARCH_INDEX_NAME

This is the name of the index to be searched (e.g., `hotels-sample-index`).

### AZURE_AI_SEARCH_API_KEY

This is the authentication key to use when utilizing the search endpoint. Please issue it from the management console. Use the Value, not the name of the authentication key.

# Introduction to tutorial

## Create or log in to your account on Azure Portal

**1.** Visit **[https://azure.microsoft.com/en-us/](https://azure.microsoft.com/en-us/)** and click on `Get started` or `Try Azure for Free` to create an account and sign in.

**2.** Choose pay per use or Azure Free with $200.

![image](/images/azure-ai-search/azure_portal_welcome.png)

## Create the Azure AI Search service

**1.** Access your control panel.

**2.** Click on `Create a resource`.

![image](/images/azure-ai-search/azure_portal_menu.png)

**3.** Search for `Azure Search` in the bar and press enter.

![image](/images/azure-ai-search/azure_ai_search_in_marketplace.png)

**4.** Now, click on `Create`.

**5.** Configure the basics settings, create a new or select an existing Resource Group, name the Service Name with a name of your preference, and then select the location.

![image](/images/azure-ai-search/create_azure_ai_search.png)

**6.** Click on `Change Pricing Tier`.

![image](/images/azure-ai-search/azure_ai_tier.png)

Now select the free option or select your preferred option (may incur charges).

![image](/images/azure-ai-search/azure_ai_free_tier.png)

**7.** Click on `Review + create` and wait for the resource to be created.

![image](/images/azure-ai-search/create_azure_ai_search_button.png)

## Create your index

**1.** Click on `Import data`.

![image](/images/azure-ai-search/azure_ai_search_instance_info.png)

**2.** Follow the Microsoft tutorial: **[https://learn.microsoft.com/en-us/azure/search/search-get-started-portal](https://learn.microsoft.com/en-us/azure/search/search-get-started-portal)**, after finishing, save the name given to the index somewhere.

**3.** Now you have your `AZURE_AI_SEARCH_INDEX_NAME`, copy and save it in a local safe place.

## Get the Endpoint

**1.** In the `Url:` you have your `AZURE_AI_SEARCH_SERVICE_ENDPOINT`, copy and save it in a local safe place.

![image](/images/azure-ai-search/azure_ai_search_instance_info.png)

**2.** On the left panel, click on `keys`.

![image](/images/azure-ai-search/azure_ai_search_menu.png)

**3.** Click on `Add` and insert a name for your key.

**4.** Copy the key to get `AZURE_AI_SEARCH_API_KEY`.

![image](/images/azure-ai-search/azure_ai_search_keys.png)

# Configure in LibreChat:

**1.** Access the Plugins and click to install Azure AI Search.

![image](/images/azure-ai-search/librechat_settings.png)


**2.** Fill in the Endpoint, Index Name, and API Key, and click on `Save`.

# Conclusion

![image](/images/azure-ai-search/chat_with_azure_ai_search.png)

Now, you will be able to conduct searches using Azure AI Search. Congratulations! 🎉🎉

## Optional

The following are configuration values that are not required but can be specified as parameters during a search.

If there are concerns that the search result data may be too large and exceed the prompt size, consider reducing the size of the search result data by using AZURE_AI_SEARCH_SEARCH_OPTION_TOP and AZURE_AI_SEARCH_SEARCH_OPTION_SELECT.

For details on each parameter, please refer to the following document:
**[https://learn.microsoft.com/en-us/rest/api/searchservice/search-documents](https://learn.microsoft.com/en-us/rest/api/searchservice/search-documents)**

```env
AZURE_AI_SEARCH_API_VERSION=2023-10-01-Preview
AZURE_AI_SEARCH_SEARCH_OPTION_QUERY_TYPE=simple
AZURE_AI_SEARCH_SEARCH_OPTION_TOP=3
AZURE_AI_SEARCH_SEARCH_OPTION_SELECT=field1, field2, field3
```

#### AZURE_AI_SEARCH_API_VERSION

Specify the version of the search API. When using new features such as semantic search or vector search, you may need to specify the preview version. The default value is `2023-11-1`.

#### AZURE_AI_SEARCH_SEARCH_OPTION_QUERY_TYPE

Specify `simple` or `full`. The default value is `simple`.

#### AZURE_AI_SEARCH_SEARCH_OPTION_TOP

Specify the number of items to search for. The default value is 5.

#### AZURE_AI_SEARCH_SEARCH_OPTION_SELECT

Specify the fields of the index to be retrieved, separated by commas. Please note that these are not the fields to be searched.



================================================
FILE: pages/docs/configuration/tools/google_search.mdx
================================================
---
title: Google Search
description: How to set up and use the Google Search Plugin, which allows you to query Google with GPT's help.
---

# Google Search Plugin
Through the plugins endpoint, you can use google search for answers to your questions with assistance from GPT! To get started, you need to get a Google Custom Search API key, and a Google Custom Search Engine ID. You can then define these as follows in your `.env` file:  
```env  
GOOGLE_SEARCH_API_KEY="...."  
GOOGLE_CSE_ID="...."  
```  
  
You first need to create a programmable search engine and get the search engine ID: **[https://developers.google.com/custom-search/docs/tutorial/creatingcse](https://developers.google.com/custom-search/docs/tutorial/creatingcse)**  
  
Then you can get the API key, click the "Get a key" button on this page: **[https://developers.google.com/custom-search/v1/introduction](https://developers.google.com/custom-search/v1/introduction)**

### 1\. Go to the [Programmable Search Engine docs](https://developers.google.com/custom-search/docs/tutorial/creatingcse) to get a Search engine ID



### 2\. Click on "Control Panel" under "Defining a Programmable Engine in Control Panel"


Click to sign in(make a Google acct if you do not have one):

![google_search-1](https://github.com/danny-avila/LibreChat/assets/32828263/51db1a90-c2dc-493c-b32c-821257c27b4e)


### 3\. Register yourself a new account/Login to the Control Panel


After logging in, you will be redirected to the Control Panel to create a new search engine:

![google_search-2](https://github.com/danny-avila/LibreChat/assets/32828263/152cfe7c-4796-49c6-9160-92cddf38f1c8)


### 4\. Create a new search engine


Fill in a name, select to "Search the entire web" and hit "Create":

![google_search-3](https://github.com/danny-avila/LibreChat/assets/32828263/c63441fc-bdb2-4086-bb7a-fcbe3d67aef9)


### 5\. Copy your Search engine ID to your .env file

![google_search-4](https://github.com/danny-avila/LibreChat/assets/32828263/e03b5c79-87e5-4a68-b83e-61faf4f2f718)


### 6\. Go to [custom-search docs](https://developers.google.com/custom-search/v1/introduction) to get a Google search API key


### 7\. Click "Get a Key":

![google_search-5](https://github.com/danny-avila/LibreChat/assets/32828263/2b93a2f9-5ed2-4794-96a8-a114e346a602)


### 8\. Name your project and agree to the Terms of Service

![google_search-6](https://github.com/danny-avila/LibreChat/assets/32828263/82c9c3ef-7363-40cd-a89e-fc45088e4c86)


### 9\. Copy your Google search API key to your .env file

![google_search-7](https://github.com/danny-avila/LibreChat/assets/32828263/8170206a-4ba6-40e3-b20e-bdbac21d6695)



================================================
FILE: pages/docs/configuration/tools/index.mdx
================================================
---
title: Tools and Plugins
description: Tools and Plugins setup instructions
---

# Tools and Plugins

**Note:** A new approach has been devised to revamp the handling of tools and plugins from scratch. The plan is to prioritize implementation for Assistants initially, followed by creating a new agent system that seamlessly integrates with various endpoints such as Anthropic, Google, and others.

## Setup Instructions:

### Azure AI Search
- [Azure AI Search](/docs/configuration/tools/azure_ai_search)

### Google Search
- [Google Search](/docs/configuration/tools/google_search)

### OpenWeather
- [OpenWeather](/docs/configuration/tools/openweather)

### Stable Diffusion
- [Stable Diffusion](/docs/configuration/tools/stable_diffusion)

### Wolfram|Alpha
- [Wolfram|Alpha](/docs/configuration/tools/wolfram)

### DALL-E
- You just need an OpenAI key, and it's made distinct from your main API key to make Chats but it can be the same one

### Zapier
- You need a Zapier account. Get your **[API key from here](https://nla.zapier.com/credentials/)** after you've made an account
  - Create allowed actions - Follow step 3 in this **[Start Here guide](https://nla.zapier.com/start/)** from Zapier
    - ⚠️ NOTE: zapier is known to be finicky with certain actions. I found that writing email drafts is probably the best use of it

### Browser/Scraper
- This is not to be confused with 'browsing' on chat.openai.com (which is technically a plugin suite or multiple plugins)
  - This plugin uses OpenAI embeddings so an OpenAI key is necessary, similar to DALL-E, and it's made distinct from your main API key to make Chats but it can be the same one
  - This plugin will simply scrape html, and will not work with dynamic Javascript pages as that would require a more involved solution
  - A better solution for 'browsing' is planned but can't guarantuee when
  - This plugin is best used in combination with google so it doesn't hallucinate webpages to visit

### SerpAPI
- An alternative to Google search but not as performant in my opinion
  - You can get an API key here: **[https://serpapi.com/dashboard](https://serpapi.com/dashboard)**
  - For free tier, you are limited to 100 queries/month
  - With google, you are limited to 100/day for free, which is a better deal, and any after may cost you a few pennies

### YouTube
- Requires a YouTube API key from the [Google Cloud Console](https://console.cloud.google.com/)
  - Create a new project or select an existing one
  - Enable the YouTube Data API v3
  - Create credentials (API key)
  - Set the API key in your environment variables as `YOUTUBE_API_KEY`

The YouTube tool provides four main operations:

1. **Search Videos**
   - Search for YouTube videos with customizable results
   - Example: Search for "cooking pasta" videos, limit to 5 results

2. **Get Video Info**
   - Retrieve detailed information about a specific video
   - Includes title, description, views, likes, and comment count

3. **Get Comments**
   - Fetch comments from a specific video
   - Customizable number of comments (1-50, default: 10)
   - Returns author, text, and like count

4. **Get Transcript**
   - Retrieve video transcripts
   - Attempts to fetch English transcript first
   - Finally tries any available language if neither is found





================================================
FILE: pages/docs/configuration/tools/openweather.mdx
================================================
---
title: 🌤️ OpenWeather Plugin
description: Configure the OpenWeather plugin for LibreChat
---

# OpenWeather Plugin Configuration

The OpenWeather plugin allows you to get weather data including current conditions, forecasts, historical data, and daily summaries using OpenWeather's One Call API 3.0.

## Prerequisites

- An OpenWeather account
- An OpenWeather API key (specifically for the One Call API 3.0)

## Getting an API Key

1. Sign up for an OpenWeather account at [OpenWeather](https://home.openweathermap.org/users/sign_up)
2. After signing in, go to your [API keys](https://home.openweathermap.org/api_keys) page
3. Generate a new API key if you don't have one
4. Subscribe to the [One Call API 3.0](https://openweathermap.org/api/one-call-3) plan
5. Wait for your API key to be activated (can take up to 2 hours)

## Configuration

### Environment Variables

Add the following to your `.env` file:

```bash
OPENWEATHER_API_KEY=your_api_key_here
```

### Plugin Configuration

Add the plugin to any [agent](https://www.librechat.ai/docs/features/agents)

## Usage

The OpenWeather plugin supports the following actions:

- `current_forecast`: Get current weather and forecast data
- `timestamp`: Get historical weather data for a specific date
- `daily_aggregation`: Get aggregated weather data for a specific date
- `overview`: Get a human-readable weather summary

### Example Prompts

```
What's the current weather in London?
What was the weather like in Paris on 2023-01-01?
Give me a weather summary for Tokyo.
What's the temperature in New York in Fahrenheit?
```

### Parameters

- `city`: Name of the city (if lat/lon not provided)
- `lat`: Latitude coordinate (optional if city provided)
- `lon`: Longitude coordinate (optional if city provided)
- `units`: Temperature units ("Celsius", "Kelvin", or "Fahrenheit")
- `lang`: Language code for weather descriptions (e.g., "en", "fr", "es")
- `date`: Date in YYYY-MM-DD format (required for timestamp and daily_aggregation actions)
- `tz`: Timezone (optional, for daily_aggregation action)

## Troubleshooting

Common issues and solutions:

1. **403 Unauthorized Error**
   - Verify your API key is correct
   - Check if your API key has been activated (wait 2 hours after creation)
   - Ensure you have subscribed to the One Call API 3.0

2. **City Not Found**
   - Check the spelling of the city name
   - Try adding the country code (e.g., "London,UK")
   - Use latitude and longitude coordinates instead

3. **Invalid Date Format**
   - Ensure dates are in YYYY-MM-DD format
   - Historical data is only available from 1979-01-01
   - Future data is limited to 1.5 years ahead

## API Limits

- Check your [OpenWeather subscription](https://home.openweathermap.org/subscriptions) for your specific limits
- Consider implementing rate limiting in high-traffic environments

## Support

For issues with the plugin:
- You may open an issue at https://github.com/jmaddington/LibreChat/issues or 
- Check the [LibreChat Issues](https://github.com/danny-avila/LibreChat/issues)
- Review OpenWeather's [API documentation](https://openweathermap.org/api/one-call-3)
- Contact OpenWeather [support](https://openweathermap.org/support) for API-specific issues

## Notes

- Temperature values are automatically rounded to the nearest degree
- Default temperature unit is Celsius if not specified


================================================
FILE: pages/docs/configuration/tools/stable_diffusion.mdx
================================================
---
title: Stable Diffusion
description: How to set up and configure the Stable Diffusion plugin
---

# Stable Diffusion Plugin

To use Stable Diffusion with this project, you will either need to download and install **[AUTOMATIC1111 - Stable Diffusion WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui)** or, for a dockerized deployment, you can also use **[stable-diffusion-webui-docker](https://github.com/AbdBarho/stable-diffusion-webui-docker)**

With the docker deployment you can skip step 2 and step 3, use the setup instructions from their repository instead.

- Note: you need a compatible GPU ("CPU-only" is possible but very slow). Nvidia is recommended, but there is no clear resource on incompatible GPUs. Any decent GPU should work.

### 1. Follow download and installation instructions from **[stable-diffusion-webui readme](https://github.com/AUTOMATIC1111/stable-diffusion-webui)**

### 2. Edit your run script settings

#### Windows

 - Edit your **webui-user.bat** file by adding the following line before the call command:
- `set COMMANDLINE_ARGS=--api`

    - Your .bat file should like this with all other settings default
    ```shell 
    @echo off

    set PYTHON=
    set GIT=
    set VENV_DIR=
    set COMMANDLINE_ARGS=--api

    call webui.bat
    ```
#### Others (not tested but should work)

 - Edit your **webui-user.sh** file by adding the following line:
 - `export COMMANDLINE_ARGS="--api"`

     - Your .sh file should like this with all other settings default
    ```bash 

    export COMMANDLINE_ARGS="--api"

    #!/bin/bash
    #########################################################
    # Uncomment and change the variables below to your need:#
    #########################################################

    # ...rest
    ```

### 3. Run Stable Diffusion (either .sh or .bat file according to your operating system)

### 4. In the app, select the plugins endpoint, open the plugins store, and install Stable Diffusion
> **Note: The default port for Gradio is `7860`. If you changed it, please update the value accordingly.**

#### Docker Install
- Use `SD_WEBUI_URL=http://host.docker.internal:7860` in the `.env` file 
- Or `http://host.docker.internal:7860` from the webui

#### Local Install
- Use `SD_WEBUI_URL=http://127.0.0.1:7860` in the `.env` file 
- Or `http://127.0.0.1:7860` from the webui


#### Select the plugins endpoint

![plugins-endpoint](https://github.com/danny-avila/LibreChat/assets/32828263/7db788a5-2173-4115-b34b-43ea132dae69)

#### Open the Plugin store and Install Stable Diffusion
![plugin_store](https://github.com/danny-avila/LibreChat/assets/32828263/12a51feb-c030-4cf0-8429-16360270988d)
![stable_diffusion-1](https://github.com/danny-avila/LibreChat/assets/32828263/b4364f41-0f7e-4197-af86-7d6061797366)


### 5. Select the plugin and enjoy!
![stable_diffusion-2](https://github.com/danny-avila/LibreChat/assets/32828263/8fa898b9-0826-42eb-bba4-6f85ec5f6ec2)



================================================
FILE: pages/docs/configuration/tools/wolfram.mdx
================================================
---
title: Wolfram|Alpha
description: How to set up and configure the Wolfram Alpha plugin
---

# Wolfram Alpha Plugin

An AppID must be supplied in all calls to the Wolfram|Alpha API. 

- Note: Wolfram API calls are limited to 100 calls/day and 2000/month for regular users.

### Make an account 
- Visit: **[products.wolframalpha.com/api/](https://products.wolframalpha.com/api/)** to create your account

### Get your AppID
- Visit the **[Developer Portal](https://developer.wolframalpha.com/portal/myapps/)** and click on `Get an AppID`
- Select `LLM API` as the `API` and copy the key

### Configure it in LibreChat
- Select the plugins endpoint
![plugins_endpoint](https://github.com/danny-avila/LibreChat/assets/32828263/7db788a5-2173-4115-b34b-43ea132dae69)
- Open the Plugin store
![plugin_store](https://github.com/danny-avila/LibreChat/assets/32828263/12a51feb-c030-4cf0-8429-16360270988d)
- Install Wolfram and Provide your AppID
![wolfram-1](https://github.com/danny-avila/LibreChat/assets/32828263/bd165497-d529-441d-8372-a68db19adc3f)

> Alternatively: you (the admin) can set the value in `\.env` to bypass the prompt: `WOLFRAM_APP_ID=your_app_id`


### Select the plugin and enjoy!

![wolfram-2](https://github.com/danny-avila/LibreChat/assets/32828263/2825e961-6c46-4728-96cd-1012a0862943)



================================================
FILE: pages/docs/development/_meta.ts
================================================
export default {
  index: 'Intro',
  get_started: 'Get Started',
  tools_and_plugins: 'Tools and Plugins',
  testing: 'Testing',
  debugging: 'Debugging (WIP)',
  technical_docs: {
    // "title": "Contributing",
    type: 'separator',
  },
  architecture: 'Project Architecture (WIP)',
  guidelines: {
    type: 'page',
    title: 'Contributor Guidelines',
    href: 'https://github.com/danny-avila/LibreChat/blob/main/.github/CONTRIBUTING.md',
    newWindow: true,
  },
  conventions: 'Coding Conventions',
  security: {
    type: 'page',
    title: 'Security Policy',
    href: 'https://github.com/danny-avila/LibreChat/blob/main/.github/SECURITY.md',
    newWindow: true,
  },
}



================================================
FILE: pages/docs/development/architecture.mdx
================================================
---
title: Project Architecture
description: (WIP) Contributions are welcome
---

# Project Architecture

<Callout type="warning" title="Under construction, contributions are welcome!" />



================================================
FILE: pages/docs/development/conventions.mdx
================================================
---
title: Code Standards and Conventions
description: This guide covers the best practices for JavaScript coding, such as following the Airbnb Style Guide, using CommonJS modules, structuring the API using Express, Mongoose, and services, and testing and documenting the code using Jest, Supertest, Playwright, JSDoc, and TypeScript.
---

# Coding Conventions

## Node.js API Server

### General Guidelines

- Follow the [Airbnb JavaScript Style Guide](https://github.com/airbnb/javascript) for general JavaScript coding conventions.
- Use "clean code" principles, such as keeping functions and modules small, adhering to the single responsibility principle, and writing expressive and readable code.
- Use meaningful and descriptive variable and function names.
- Prioritize code readability and maintainability over brevity.
- Use the provided .eslintrc and .prettierrc files for consistent code formatting.
- Use CommonJS modules (require/exports) for Node.js modules.
- Organize and modularize the codebase using separate files for different concerns.

### API Design

- Follow RESTful principles when designing APIs.
- Use meaningful and descriptive names for routes, controllers, services, and models.
- Use appropriate HTTP methods (GET, POST, PUT, DELETE) for each route.
- Use proper status codes and response structures for consistent API responses (ie. 2xx for success, 4xx for bad request from client, 5xx for server error, etc.).
- Use try-catch blocks to catch and handle exceptions gracefully.
- Implement proper error handling and consistently return appropriate error responses.
- Use the logging system included in the `utils` directory to log important events and errors.
- Do JWT-based, stateless authentication using the `requireJWTAuth` middleware.

### File Structure

_Note: The API is undergoing a refactor to separate out the code for improved separation of concerns, testability, and maintainability. Any new APIs must follow the structure using the auth system as an example, which separates out the routes, controllers, services, and models into separate files._

#### Routes

Specifies each http request method, any middleware to be used, and the controller function to be called for each route.

- Define routes using the Express Router in separate files for each resource or logical grouping.
- Use descriptive route names and adhere to RESTful conventions.
- Keep routes concise and focused on a single responsibility.
- Prefix all routes with the /api namespace.

#### Controllers

Contains the logic for each route, including calling the appropriate service functions and returning the appropriate response status code and JSON body.

- Create a separate controller file for each route to handle the request/response logic.
- Name controller files using the PascalCase convention and append "Controller" to the file name (e.g., UserController.js).
- Use controller methods to encapsulate logic related to the route handling.
- Keep controllers thin by delegating complex operations to service or model files.

#### Services

Contains complex business logic or operations shared across multiple controllers.

- Name service files using the PascalCase convention and append "Service" to the file name (e.g., AuthService.js).
- Avoid tightly coupling services to specific models or databases for better reusability.
- Maintain a single responsibility principle within each service.

#### Models

Defines Mongoose models to represent data entities and their relationships.

- Use singular, PascalCase names for model files and their associated collections (e.g., User.js and users collection).
- Include only the necessary fields, indexes, and validations in the models.
- Keep models independent of the API layer by avoiding direct references to request/response objects.

### Database Access (MongoDB and Mongoose)

- Use Mongoose ([https://mongoosejs.com](https://mongoosejs.com)) as the MongoDB ODM.
- Create separate model files for each entity and ensure clear separation of concerns.
- Use Mongoose schema validation to enforce data integrity.
- Handle database connections efficiently and avoid connection leaks.
- Use Mongoose query builders to create concise and readable database queries.

### Testing and Documentation

_Note: the repo currently lacks sufficient automated unit and integration tests for both the client and the API. This is a great first issue for new contributors wanting to familiarize with the codebase._

- Write unit tests for all critical and complex functionalities using Jest.
- Write integration tests for all API endpoints using Supertest.
- Write end-to-end tests for all client-side functionalities using Playwright.
- Use descriptive test case and function names to clearly express the test's purpose.
- Document the code using JSDoc comments to provide clear explanations of functions, parameters, and return types. (WIP)

---

## React Client

### General TypeScript and React Best Practices

- Use [TypeScript best practices](https://onesignal.com/blog/effective-typescript-for-react-applications/) to benefit from static typing and improved tooling.
- Group related files together within folders.
- Name components using the PascalCase convention.
- Use concise and descriptive names that accurately reflect the component's purpose.
- Split complex components into smaller, reusable ones when appropriate.
- Keep the rendering logic within components minimal.
- Extract reusable parts into separate functions or hooks.
- Apply prop type definitions using TypeScript types or interfaces.
- Use form validation where appropriate. (note: we use [React Hook Form](https://react-hook-form.com/) for form validation and submission)

### Data Services

Use the conventions found in the `data-provider` directory for handling data services. For more information, see [this article](https://www.danorlandoblog.com/building-data-services-for-librechat-with-react-query/) which describes the methodology used.

### State Management

Use [Recoil](https://recoiljs.org/) for state management, but _DO NOT pollute the global state with unnecessary data_. Instead, use local state or props for data that is only used within a component or passed down from parent to child.



================================================
FILE: pages/docs/development/debugging.mdx
================================================
---
title: Debugging
description: (WIP) Contributions are welcome
---

# Debugging (WIP)

<Callout type="warning" title="Under construction, contributions are welcome!" />

see also: [Logging System](/docs/configuration/logging)


================================================
FILE: pages/docs/development/get_started.mdx
================================================
---
title: Getting Started
description: Learn how to contribute using GitHub Desktop, VS Code extensions, and Git rebase.
---

# Getting Started for Contributors

## Requirements

- [Git](https://git-scm.com/downloads) (Essential)
- [Node.js](https://nodejs.org/en/download) (Essential, use the LTS version)
- [MongoDB](https://www.mongodb.com/try/download/community) (Essential, for the database)
- [Git LFS](https://git-lfs.com/) (Useful for larger files)
- [GitHub Desktop](https://desktop.github.com/) (Optional)
- [VSCode](https://code.visualstudio.com/Download) (Recommended Source-code Editor)

### Recommended VSCode Extensions

Install these extensions in VS Code:

- [Prettier](https://marketplace.visualstudio.com/items?itemName=esbenp.prettier-vscode)
- [ESLint](https://marketplace.visualstudio.com/items?itemName=dbaeumer.vscode-eslint)
- [GitLens](https://marketplace.visualstudio.com/items?itemName=eamodio.gitlens)

## Prepare the Environment

### GitHub

- Fork the LibreChat repository: [https://github.com/danny-avila/LibreChat/fork](https://github.com/danny-avila/LibreChat/fork)

- Create a branch on your fork, name it properly, and link it to the original repository.

<Callout type="info" title="Screenshots:" emoji="📷" collapsible>
  ![image](https://github.com/danny-avila/LibreChat/assets/32828263/c4cff4d5-70ea-4263-9156-e7f220e049eb)
  ![image](https://github.com/danny-avila/LibreChat/assets/32828263/8ec85f02-f0f7-4cef-bb1c-6ff1bd1d7023)
  ![image](https://github.com/danny-avila/LibreChat/assets/32828263/09e4ea5c-0753-470d-a0c5-8281a523a81b)
</Callout>

- Download your new branch to your local PC

```sh filename="Download your LibreChat branch"
git clone -b branch-name https://github.com/username/LibreChat.git
```

> Replace `branch-name` and `username` with your details

### Open in VS Code

- After cloning your branch:
  ```sh filename="Navigate to LibreChat folder"
  cd LibreChat
  ```
  ```sh filename="Open in VS Code"
  code .
  ```

### Prepare LibreChat

- Open the terminal in VS Code with `ctrl{:kbd}`+`shift{:kbd}`+`` `{:kbd}``

  > Alternatively, use `ctrl{:kbd}`+`j{:kbd}` to open the bottom pane and select the terminal.

- ```sh filename="Install LibreChat dependencies"
  npm ci
  ```
- ```sh filename="Build Frontend"
  npm run frontend
  ```

- .env Configuration
  - Create the `.env` file. If you don't have one, duplicate `.env.example` and configure it.

<Callout type="warning" title="Warning">
  The default values in `.env.example` are usually fine, except for `MONGO_URI`. Provide your own.
  Make sure to install MongoDB and configure `MONGO_URI` correctly to connect to your MongoDB instance.
  Use [MongoDB Community Server](https://www.mongodb.com/try/download/community) or [MongoDB Atlas
  Cloud](https://www.mongodb.com/cloud/atlas/register).
</Callout>

### Development Workflow

For efficient work on LibreChat, use these commands:

- **Starting Backend:**

  - Use `npm run backend` for normal operation.
  - For active development, use `npm run backend:dev` to monitor changes.
  - Access at `http://localhost:3080/`.

- **Running Frontend in Development Mode:**

  - **Ensure backend is running.**
  - Use `npm run frontend:dev` to monitor frontend changes.
  - View at `http://localhost:3090/`.

<Callout type="tip" title="Pro Tips">
  - For real-time updates during frontend development, run `npm run frontend:dev` to avoid
  restarting both frontend and backend on port 3090. - Set `DEBUG_CONSOLE` to true in `.env` for
  verbose server output in the console.
</Callout>

## Local Testing

Before submission, test your updates locally, see: [Perform Tests Locally](/docs/development/testing)

By running tests, ensure your contributions are robust and ready for integration.

## Commit, Push, Pull Request (PR)

### Make a Commit

**Commits** mark logical checkpoints in development. Include clear messages explaining changes.

**Example:**

```bash filename=" "
git add .
git commit -m "Add login functionality"
```

### Push Changes

**Push** changes to the remote repository after completing a feature or fixing an issue.

**Example:**

```bash filename=" "
git push origin feature-branch-name
```

### Create a Pull Request (PR)

**Pull Request** merges changes from a feature branch into the main branch.

1. Pull latest changes from main branch and resolve conflicts.
2. Push updated feature branch.
3. Ensure code follows project guidelines.

**Example:**

```bash filename=" "
git checkout main
git pull origin main
git checkout feature-branch-name
git merge main
# Resolve conflicts if any
git push origin feature-branch-name
# Open PR on GitHub
```

Access your repository in a browser and click "Contribute"
![image](https://github.com/danny-avila/LibreChat/assets/32828263/4da0a287-e6d3-4e75-af6b-4cffc28f593c)

<Callout type="info" title="Note:">
  Provide a detailed PR description explaining changes and their value. Reference related issues.
</Callout>

<Callout type="tip" title="Tip">
  Use GitHub Desktop to track changes.
  ![image](https://github.com/Berry-13/LibreChat/assets/81851188/a04a7e81-7c75-4c77-8463-d35f603bedf7)
</Callout>

<Callout type="warning" title="Warning">
  If `git commit` fails due to ESLint errors, understand and fix the issue.
</Callout>

## Revert Commits Safely

To undo changes in a feature branch, follow these steps cautiously:

- ```bash filename="1. Update local repository from feature branch:"
  git pull origin feature-branch-name
  ```

- ```bash filename="2. Review commit history to determine commits to revert:"
  git log
  ```

- ```bash filename="3. Start an interactive rebase for 'N' commits to revert:"
  git rebase -i HEAD~N
  ```

  > Replace `pick` with `drop` for commits to remove. Save and exit editor.

- ```bash filename="4. Force push changes to remote repository:"
  git push --force origin feature-branch-name
  ```



================================================
FILE: pages/docs/development/index.mdx
================================================
---
title: Intro
description: LibreChat's introduction to development 
---

# Introduction

While Docker is our preferred method for installing LibreChat due to its ease of setting up and consistency across different environments, **we strongly recommend using `npm{:hack}` for development purposes.** This recommendation is based on several advantages that npm offers for developers:

- **Faster Iteration:** npm allows for quicker iteration cycles during development. Changes made to the codebase can be immediately reflected without the need to rebuild the entire Docker image, leading to a more efficient development process.
- **Direct Dependency Management:** Using npm gives developers direct control over the dependencies. It's easier to install, update, or remove packages, and manage project dependencies in real-time, which is crucial for development.
- **Simplified Debugging:** Debugging is more straightforward with npm, as developers can directly interact with the code and tools without the abstraction layer that Docker introduces. This direct interaction facilitates easier identification and resolution of issues.
- **Native Environment:** Developing with npm allows the application to run in its native environment on your machine. This can help in catching environment-specific issues early in the development cycle.

<Callout type="info" title="Dev Ressources" emoji="🧑‍💻">
  - 📚 If you're new to concepts like **repositories**, **pull requests (PRs)**, **forks**, and **branches**, start with the official GitHub documentation: 
    - **[Getting Started - About Collaborative Development Models](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/getting-started/about-collaborative-development-models)**
  - 👥 Our contributor guidelines can be found at: 
    - **[Contributor Guidelines](https://github.com/danny-avila/LibreChat/blob/main/.github/CONTRIBUTING.md)** 
  - 💻 To learn more about our coding standards see: 
    - **[Coding Conventions](/docs/development/conventions)**
</Callout>



================================================
FILE: pages/docs/development/testing.mdx
================================================
---
title: Testing During Development
description: How to locally test the app during development.
---

# Locally test the app during development

## Local Unit Tests

Before submitting your updates, it's crucial to verify they pass all unit tests. Follow these steps to run tests locally:

- Copy your `.env.example` file in the `/api` folder and rename it to `.env`
  ```bash filename="create a /api/.env file"
  cp .env.example ./api/.env
  ```

- Add `NODE_ENV=CI` to your `/api/.env` file
- `npm run test:client`
- `npm run test:api`

<Callout type="warning" title="Warning">
  When executed locally, this API unit test is expected to fail. This should be the only error
  encountered.
  ![image](https://github.com/danny-avila/LibreChat/assets/32828263/d222034c-9c3a-4764-b972-39e954c92170)
</Callout>



================================================
FILE: pages/docs/development/tools_and_plugins.mdx
================================================
---
title: Tools and Plugins
description: This doc shows you how to create custom plugins for LibreChat by extending the LangChain `Tool` class. You will learn how to use different APIs and functions with your plugins, and how to integrate them with the LangChain framework.
---

# Making your own Tools/Plugins

<Callout type="warning" title="Warning">
Please refer to the most recents tools used with assistants in `api/app/clients/tools/structured/` since plugins will be deprecated in favor of tools in the near future
</Callout>

Creating custom plugins for this project involves extending the `Tool` class from the `langchain/tools` module. 

**Note:** I will use the word plugin interchangeably with tool, as the latter is specific to LangChain, and we are mainly conforming to the library.

You are essentially creating DynamicTools in LangChain speak. See the **[LangChainJS docs](https://js.langchain.com/docs/how_to/custom_tools/)** for more info.

This guide will walk you through the process of creating your own custom plugins, using the `StableDiffusionAPI` and `WolframAlphaAPI` tools as examples.

When using the Functions Agent (the default mode for plugins), tools are converted to **[OpenAI functions](https://openai.com/blog/function-calling-and-other-api-updates)**; in any case, plugins/tools are invoked conditionally based on the LLM generating a specific format that we parse. 

The most common implementation of a plugin is to make an API call based on the natural language input from the AI, but there is virtually no limit in programmatic use case.

---


## Key Takeaways

Here are the key takeaways for creating your own plugin:

**1.** [**Import Required Modules:**](#step-1-import-required-modules) Import the necessary modules for your plugin, including the `Tool` class from `langchain/tools` and any other modules your plugin might need.

**2.** [**Define Your Plugin Class:**](#step-2-define-your-tool-class) Define a class for your plugin that extends the `Tool` class. Set the `name` and `description` properties in the constructor. If your plugin requires credentials or other variables, set them from the fields parameter or from a method that retrieves them from your process environment. Note: if your plugin requires long, detailed instructions, you can add a `description_for_model` property and make `description` more general.

**3.** [**Define Helper Methods:**](#step-3-define-helper-methods) Define helper methods within your class to handle specific tasks if needed.

**4.** [**Implement the `_call` Method:**](#step-4-implement-the-_call-method) Implement the `_call` method where the main functionality of your plugin is defined. This method is called when the language model decides to use your plugin. It should take an `input` parameter and return a result. If an error occurs, the function should return a string representing an error, rather than throwing an error. If your plugin requires multiple inputs from the LLM, read the [StructuredTools](#StructuredTools) section.

**5.** [**Export Your Plugin and Import into handleTools.js:**](#step-5-export-your-plugin-and-import-into-handletoolsjs) Export your plugin and import it into `handleTools.js`. Add your plugin to the `toolConstructors` object in the `loadTools` function. If your plugin requires more advanced initialization, add it to the `customConstructors` object.

**6.** [**Export Your Plugin into index.js:**](#step-6-export-your-plugin-into-indexjs) Export your plugin into `index.js` under `tools`. Add your plugin to the `module.exports` of the `index.js`, so you also need to declare it as `const` in this file.

**7.** [**Add Your Plugin to manifest.json:**]( #step-7-add-your-plugin-to-manifestjson) Add your plugin to `manifest.json`. Follow the strict format for each of the fields of the "plugin" object. If your plugin requires authentication, add those details under `authConfig` as an array. The `pluginKey` should match the class `name` of the Tool class you made, and the `authField` prop must match the process.env variable name.

Remember, the key to creating a custom plugin is to extend the `Tool` class and implement the `_call` method. The `_call` method is where you define what your plugin does. You can also define helper methods and properties in your class to support the functionality of your plugin.

**Note: You can find all the files mentioned in this guide in the `.\api\app\langchain\tools` folder.**

---

## StructuredTools

**Multi-Input Plugins**

If you would like to make a plugin that would benefit from multiple inputs from the LLM, instead of a singular input string as we will review, you need to make a LangChain **[StructuredTool](https://blog.langchain.dev/structured-tools/)** instead. A detailed guide for this is in progress, but for now, you can look at how I've made StructuredTools in this directory: `api\app\clients\tools\structured\`. This guide is foundational to understanding StructuredTools, and it's recommended you continue reading to better understand LangChain tools first. The blog linked above is also helpful once you've read through this guide.

---

## Step 1: Import Required Modules

Start by importing the necessary modules. This will include the `Tool` class from `langchain/tools` and any other modules your tool might need. For example:

```javascript
const { Tool } = require('langchain/tools');
// ... whatever else you need
```

## Step 2: Define Your Tool Class

Next, define a class for your plugin that extends the `Tool` class. The class should have a constructor that calls the `super()` method and sets the `name` and `description` properties. These properties will be used by the language model to determine when to call your tool and with what parameters.

**Important:** you should set credentials/necessary variables from the fields parameter, or alternatively from a method that gets it from your process environment
```javascript
class StableDiffusionAPI extends Tool {
  constructor(fields) {
    super();
    this.name = 'stable-diffusion';
    this.url = fields.SD_WEBUI_URL || this.getServerURL(); // <--- important!
    this.description = `You can generate images with 'stable-diffusion'. This tool is exclusively for visual content...`;
  }
  ...
}
```

**Optional:** As of v0.5.8, when using Functions, you can add longer, more detailed instructions, with the `description_for_model` property. When doing so, it's recommended you make the `description` property more generalized to optimize tokens. Each line in this property is prefixed with `// ` to mirror how the prompt is generated for ChatGPT (chat.openai.com). This format more closely aligns to the prompt engineering of official ChatGPT plugins.

```js
// ...
    this.description_for_model = `// Generate images and visuals using text with 'stable-diffusion'.
// Guidelines:
// - ALWAYS use {{"prompt": "7+ detailed keywords", "negative_prompt": "7+ detailed keywords"}} structure for queries.
// - Visually describe the moods, details, structures, styles, and/or proportions of the image. Remember, the focus is on visual attributes.
// - Craft your input by "showing" and not "telling" the imagery. Think in terms of what you'd want to see in a photograph or a painting.
// - Here's an example for generating a realistic portrait photo of a man:
// "prompt":"photo of a man in black clothes, half body, high detailed skin, coastline, overcast weather, wind, waves, 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3"
// "negative_prompt":"semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, out of frame, low quality, ugly, mutation, deformed"
// - Generate images only once per human query unless explicitly requested by the user`;
    this.description = 'You can generate images using text with \'stable-diffusion\'. This tool is exclusively for visual content.';
// ...
```

Within the constructor, note that we're getting a sensitive variable from either the fields object or from the **getServerURL** method we define to access an environment variable.

```js
this.url = fields.SD_WEBUI_URL || this.getServerURL();
```

Any credentials necessary are passed through `fields` when the user provides it from the frontend; otherwise, the admin can "authorize" the plugin for all users through environment variables. All credentials passed from the frontend are encrypted.

```js
// It's recommended you follow this convention when accessing environment variables.
  getServerURL() {
    const url = process.env.SD_WEBUI_URL || '';
    if (!url) {
      throw new Error('Missing SD_WEBUI_URL environment variable.');
    }
    return url;
  }
```

## Step 3: Define Helper Methods

You can define helper methods within your class to handle specific tasks if needed. For example, the `StableDiffusionAPI` class includes methods like `replaceNewLinesWithSpaces`, `getMarkdownImageUrl`, and `getServerURL` to handle various tasks.

```javascript
class StableDiffusionAPI extends Tool {
  ...
  replaceNewLinesWithSpaces(inputString) {
    return inputString.replace(/\r\n|\r|\n/g, ' ');
  }
  ...
}
```

## Step 4: Implement the `_call` Method

The `_call` method is where the main functionality of your plugin is implemented. This method is called when the language model decides to use your plugin. It should take an `input` parameter and return a result.

> In a basic Tool, the LLM will generate one string value as an input. If your plugin requires multiple inputs from the LLM, read the **[StructuredTools](#StructuredTools)** section.

```javascript
class StableDiffusionAPI extends Tool {
  ...
  async _call(input) {
    // Your tool's functionality goes here
    ...
    return this.result;
  }
}
```

**Important:** The _call function is what will the agent will actually call. When an error occurs, the function should, when possible, return a string representing an error, rather than throwing an error. This allows the error to be passed to the LLM and the LLM can decide how to handle it. If an error is thrown, then execution of the agent will stop.

## Step 5: Export Your Plugin and import into handleTools.js


**This process will be somewhat automated in the future, as long as you have your plugin/tool in api\app\langchain\tools**

```javascript
// Export
module.exports = StableDiffusionAPI;
```

```js
/* api\app\langchain\tools\handleTools.js */
const StableDiffusionAPI = require('./StableDiffusion');
...
```

In handleTools.js, find the beginning of the `loadTools` function and add your plugin/tool to the toolConstructors object.

```js
const loadTools = async ({ user, model, tools = [], options = {} }) => {
  const toolConstructors = {
    calculator: Calculator,
    google: GoogleSearchAPI,
    wolfram: WolframAlphaAPI,
    'dall-e': OpenAICreateImage,
    'stable-diffusion': StableDiffusionAPI // <----- Newly Added. Note: the key is the 'name' provided in the class. 
    // We will now refer to this name as the `pluginKey`
  };
```
  
If your Tool class requires more advanced initialization, you would add it to the customConstructors object.

The default initialization can be seen in the `loadToolWithAuth` function, and most custom plugins should be initialized this way.

Here are a few customConstructors, which have varying initializations

```javascript
  const customConstructors = {
    browser: async () => {
      let openAIApiKey = process.env.OPENAI_API_KEY;
      if (!openAIApiKey) {
        openAIApiKey = await getUserPluginAuthValue(user, 'OPENAI_API_KEY');
      }
      return new WebBrowser({ model, embeddings: new OpenAIEmbeddings({ openAIApiKey }) });
    },
  // ...
    plugins: async () => {
      return [
        new HttpRequestTool(),
        await AIPluginTool.fromPluginUrl(
          "https://www.klarna.com/.well-known/ai-plugin.json", new ChatOpenAI({ openAIApiKey: options.openAIApiKey, temperature: 0 })
        ),
      ]
    }
  };
```

## Step 6: Export your Plugin into index.js

Find the `index.js` under `api/app/clients/tools`. You need to put your plugin into the `module.exports`, to make it compile, you will also need to declare your plugin as `consts`:

```js
const StructuredSD = require('./structured/StableDiffusion');
const StableDiffusionAPI = require('./StableDiffusion');
...
module.exports = {
  ...
  StableDiffusionAPI,
  StructuredSD,
  ...
}
```
  
## Step 7: Add your Plugin to manifest.json

**This process will be somehwat automated in the future along with step 5, as long as you have your plugin/tool in api\app\langchain\tools, and your plugin can be initialized with the default method**

```json
  {
    "name": "Calculator",
    "pluginKey": "calculator",
    "description": "Perform simple and complex mathematical calculations.",
    "icon": "https://i.imgur.com/RHsSG5h.png",
    "isAuthRequired": "false",
    "authConfig": []
  },
  {
    "name": "Stable Diffusion",
    "pluginKey": "stable-diffusion",
    "description": "Generate photo-realistic images given any text input.",
    "icon": "https://i.imgur.com/Yr466dp.png",
    "authConfig": [
      {
        "authField": "SD_WEBUI_URL",
        "label": "Your Stable Diffusion WebUI API URL",
        "description": "You need to provide the URL of your Stable Diffusion WebUI API. For instructions on how to obtain this, see <a href='url'>Our Docs</a>."
      }
    ]
  },
```
  
Each of the fields of the "plugin" object are important. Follow this format strictly. If your plugin requires authentication, you will add those details under `authConfig` as an array since there could be multiple authentication variables. See the Calculator plugin for an example of one that doesn't require authentication, where the authConfig is an empty array (an array is always required).

**Note:** as mentioned earlier, the `pluginKey` matches the class `name` of the Tool class you made.
**Note:** the `authField` prop must match the process.env variable name

Here is an example of a plugin with more than one credential variable

```json
  [
  {
    "name": "Google",
    "pluginKey": "google",
    "description": "Use Google Search to find information about the weather, news, sports, and more.",
    "icon": "https://i.imgur.com/SMmVkNB.png",
    "authConfig": [
      {
        "authField": "GOOGLE_CSE_ID",
        "label": "Google CSE ID",
        "description": "This is your Google Custom Search Engine ID. For instructions on how to obtain this, see <a href='https://github.com/danny-avila/LibreChat/blob/main/docs/features/plugins/google_search.md'>Our Docs</a>."
      },
      {
        "authField": "GOOGLE_SEARCH_API_KEY",
        "label": "Google API Key",
        "description": "This is your Google Custom Search API Key. For instructions on how to obtain this, see <a href='https://github.com/danny-avila/LibreChat/blob/main/docs/features/plugins/google_search.md'>Our Docs</a>."
      }
    ]
  },
```

## Example: WolframAlphaAPI Tool

Here's another example of a custom tool, the `WolframAlphaAPI` tool. This tool uses the `axios` module to make HTTP requests to the Wolfram Alpha API.

```javascript
const axios = require('axios');
const { Tool } = require('langchain/tools');

class WolframAlphaAPI extends Tool {
  constructor(fields) {
    super();
    this.name = 'wolfram';
    this.apiKey = fields.WOLFRAM_APP_ID || this.getAppId();
    this.description = `Access computation, math, curated knowledge & real-time data through wolframAlpha...`;
  }

  async fetchRawText(url) {
    try {
      const response = await axios.get(url, { responseType: 'text' });
      return response.data;
    } catch (error) {
      console.error(`Error fetching raw text: ${error}`);
      throw error

    }
  }

  getAppId() {
    const appId = process.env.WOLFRAM_APP_ID || '';
    if (!appId) {
      throw new Error('Missing WOLFRAM_APP_ID environment variable.');
    }
    return appId;
  }

  createWolframAlphaURL(query) {
    const formattedQuery = query.replaceAll(/`/g, '').replaceAll(/\n/g, ' ');
    const baseURL = 'https://www.wolframalpha.com/api/v1/llm-api';
    const encodedQuery = encodeURIComponent(formattedQuery);
    const appId = this.apiKey || this.getAppId();
    const url = `${baseURL}?input=${encodedQuery}&appid=${appId}`;
    return url;
  }

  async _call(input) {
    try {
      const url = this.createWolframAlphaURL(input);
      const response = await this.fetchRawText(url);
      return response;
    } catch (error) {
      if (error.response && error.response.data) {
        console.log('Error data:', error.response.data);
        return error.response.data;
      } else {
        console.log(`Error querying Wolfram Alpha`, error.message);
        return 'There was an error querying Wolfram Alpha.';
      }
    }
  }
}

module.exports = WolframAlphaAPI;
```

In this example, the `WolframAlphaAPI` class has helper methods like `fetchRawText`, `getAppId`, and `createWolframAlphaURL` to handle specific tasks. The `_call` method makes an HTTP request to the Wolfram Alpha API and returns the response.




================================================
FILE: pages/docs/documentation/_meta.ts
================================================
export default {
  index: 'Contributing to the Docs',
  examples: 'Docs Components',
  syntax_highlighting: 'Syntax Highlighting',
}



================================================
FILE: pages/docs/documentation/examples.mdx
================================================
---
title: Docs Components Examples
description: Explore the various components available on the website that you can seamlessly incorporate into your contributions to the blog and documentation.
---


import {
  ArrowRight,
  ArrowUp,
  SvgBox,
  Brush,
  SvgCards,
  ChevronRight,
  Close,
  Cloud,
  Code,
  Diagram,
  Dropper,
  File,
  Files,
  SvgFolderTree,
  Formula,
  Gear,
  Globe,
  IdCard,
  Lightning,
  SvgLink,
  Markdown,
  Newsletter,
  One,
  Picture,
  Rows,
  Search,
  Sort,
  Stars,
  SvgSwitch,
  SvgTable,
  Terminal,
  Warning,
} from '@/components/svg'

import { Feature, Features } from '@/components/features'

# Docs Components Examples

## Carousel

<Tabs items={['Example', 'Code']}>
  <Tabs.Tab>
    <Carousel autoplay animationDuration="1500" perView="3">
      <img 
      src="https://github.com/danny-avila/LibreChat/assets/32828263/9b260f59-6acc-4fac-83b9-9d123d6b4a35" 
      alt="Slide 1"
      style={{ maxWidth: '75%', height: 'auto', borderRadius: '20px' }}
      />
      <img
      src="https://github.com/danny-avila/LibreChat/assets/32828263/d742c488-963a-4011-86f2-b6df184b70d1" 
      alt="Slide 2" 
      style={{ maxWidth: '75%', height: 'auto', borderRadius: '20px' }}
      />
      <img
      src="https://github.com/danny-avila/LibreChat/assets/32828263/5a338731-eb95-41ea-8327-f5e78b8cc157" 
      alt="Slide 3" 
      style={{ maxWidth: '75%', height: 'auto', borderRadius: '20px' }}
      />
    </Carousel>
  </Tabs.Tab>

  <Tabs.Tab>
  ```tsx copy filename="carousel"
<Carousel autoplay animationDuration="1500" showBullets perView="3">
  <img src="slide1.jpg" alt="Slide 1" />
  <img src="slide2.jpg" alt="Slide 2" />
  <img src="slide3.jpg" alt="Slide 3" />
</Carousel>
  ```

This example demonstrates how to use the `Carousel` component with the following props:

- `autoplay`: Enables autoplay for the carousel.
- `animationDuration="1500"`: Sets the animation duration to 1500 milliseconds.
- `showControls`: Shows the previous and next controls for the carousel.
- `showBullets`: Shows the bullet navigation for the carousel.
- `perView="3"`: Displays three slides at a time in the carousel.

Inside the `Carousel` component, you can place any content you want to display as slides. In this example, we're using `<img>` elements with source paths to `slide1.jpg`, `slide2.jpg`, and `slide3.jpg`.

  </Tabs.Tab>
</Tabs>

## Callouts

<Tabs items={['Collapsible Callouts', 'Standard Callouts']}>
  <Tabs.Tab>
    <Tabs items={['Example', 'Code']}>
      <Tabs.Tab>

  <Callout type="other" title="undefined or unsupported type" emoji='💀'collapsible>
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. 
  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet.
  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.

Dolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.
Vestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.
Integer euismod mi luctus proin habitant interdum proin.

Litora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.
Netus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.
Metus imperdiet fusce id rhoncus urna ridiculus sem.

  </Callout>

  <Callout type="default" title="default" collapsible>
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. 
  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. 
  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.

Dolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.
Vestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.
Integer euismod mi luctus proin habitant interdum proin.

Litora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.
Netus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.
Metus imperdiet fusce id rhoncus urna ridiculus sem.

  </Callout>

  <Callout type="example" title="example" collapsible>
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. 
  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. 
  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.

Dolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.
Vestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.
Integer euismod mi luctus proin habitant interdum proin.

Litora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.
Netus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.
Metus imperdiet fusce id rhoncus urna ridiculus sem.

  </Callout>

  <Callout type="note" title="note" collapsible>
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. 
  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. 
  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.

Dolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.
Vestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.
Integer euismod mi luctus proin habitant interdum proin.

Litora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.
Netus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.
Metus imperdiet fusce id rhoncus urna ridiculus sem.

  </Callout>

  <Callout type="abstract" title="abstract" collapsible>
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. 
  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. 
  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.

Dolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.
Vestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.
Integer euismod mi luctus proin habitant interdum proin.

Litora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.
Netus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.
Metus imperdiet fusce id rhoncus urna ridiculus sem.

  </Callout>

  <Callout type="info" title="info" collapsible>
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. 
  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. 
  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.

Dolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.
Vestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.
Integer euismod mi luctus proin habitant interdum proin.

Litora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.
Netus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.
Metus imperdiet fusce id rhoncus urna ridiculus sem.

  </Callout>

  <Callout type="tip" title="tip" collapsible>
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. 
  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. 
  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.

Dolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.
Vestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.
Integer euismod mi luctus proin habitant interdum proin.

Litora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.
Netus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.
Metus imperdiet fusce id rhoncus urna ridiculus sem.

  </Callout>

  <Callout type="success" title="success" collapsible>
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. 
  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. 
  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.

Dolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.
Vestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.
Integer euismod mi luctus proin habitant interdum proin.

Litora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.
Netus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.
Metus imperdiet fusce id rhoncus urna ridiculus sem.

  </Callout>

  <Callout type="question" title="question" collapsible>
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. 
  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. 
  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.

Dolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.
Vestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.
Integer euismod mi luctus proin habitant interdum proin.

Litora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.
Netus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.
Metus imperdiet fusce id rhoncus urna ridiculus sem.

  </Callout>

  <Callout type="warning" title="warning" collapsible>
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. 
  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. 
  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.

Dolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.
Vestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.
Integer euismod mi luctus proin habitant interdum proin.

Litora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.
Netus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.
Metus imperdiet fusce id rhoncus urna ridiculus sem.

  </Callout>

  <Callout type="error" title="error" collapsible>
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. 
  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. 
  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.

Dolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.
Vestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.
Integer euismod mi luctus proin habitant interdum proin.

Litora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.
Netus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.
Metus imperdiet fusce id rhoncus urna ridiculus sem.

  </Callout>

  <Callout type="danger" title="danger" collapsible>
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. 
  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. 
  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.

Dolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.
Vestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.
Integer euismod mi luctus proin habitant interdum proin.

Litora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.
Netus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.
Metus imperdiet fusce id rhoncus urna ridiculus sem.

  </Callout>

  <Callout type="bug" title="bug" collapsible>
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. 
  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. 
  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.

Dolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.
Vestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.
Integer euismod mi luctus proin habitant interdum proin.

Litora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.
Netus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.
Metus imperdiet fusce id rhoncus urna ridiculus sem.

  </Callout>

  <Callout type="quote" title="quote" collapsible>
  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Fermentum dapibus diam lacinia curabitur nullam habitasse etiam. 
  Aliquam dolor nostra himenaeos ullamcorper duis venenatis laoreet. 
  Netus dictum hac donec risus adipiscing tempus ultrices. Vivamus eros volutpat suscipit mi a lacus volutpat.

Dolor semper vehicula cras convallis id sociosqu lacus. At ligula porta habitasse nunc ridiculus odio litora.
Vestibulum suscipit nisi urna montes conubia fringilla nascetur. Turpis rutrum nunc ad elementum molestie interdum curae.
Integer euismod mi luctus proin habitant interdum proin.

Litora dui mus cum condimentum mus eget suscipit. Tellus phasellus duis phasellus elementum vitae pretium et.
Netus habitasse tincidunt semper nullam sociosqu nisl mollis. Est hendrerit nulla ante fusce faucibus convallis vulputate.
Metus imperdiet fusce id rhoncus urna ridiculus sem.

  </Callout>
      </Tabs.Tab>
      <Tabs.Tab>
```tsx copy filename="callout"
<Callout type="success" title="title" emoji='🦆' collapsible>
  - The type comes with a color scheme and default emoji
  - "title" is optional
  - "emoji" is optional
  - "collapsible" is only added when the collapsible feature is wanted
</Callout>
```
      </Tabs.Tab>
    </Tabs>
  </Tabs.Tab>
  <Tabs.Tab>
    <Tabs items={['Example', 'Code']}>
      <Tabs.Tab>

{' '}

<Callout type="" title="undefined callout" emoji="💀">
  undefined or unsupported `type` default to this color scheme
</Callout>

{' '}

<Callout type="default" title="default">
  this is a default callout
</Callout>

{' '}

<Callout type="example" title="example">
  this is a example callout
</Callout>

{' '}

<Callout type="note" title="note">
  this is a note callout
</Callout>

{' '}

<Callout type="abstract" title="abstract">
  this is a abstract callout
</Callout>

{' '}

<Callout type="info" title="info">
  this is a info callout
</Callout>

{' '}

<Callout type="tip" title="tip">
  this is a tip callout
</Callout>

{' '}

<Callout type="success" title="success">
  this is a success callout
</Callout>

{' '}

<Callout type="question" title="question">
  this is a question callout
</Callout>

{' '}

<Callout type="warning" title="warning">
  this is a warning callout
</Callout>

{' '}

<Callout type="error" title="error">
  this is a error callout
</Callout>

{' '}

<Callout type="danger" title="danger">
  this is a danger callout
</Callout>

{' '}

<Callout type="bug" title="bug">
  this is a bug callout
</Callout>

  <Callout type="quote" title="quote">
  this is a quote callout
  </Callout>
      </Tabs.Tab>
      <Tabs.Tab>
```tsx copy filename="callout"
<Callout type="success" title="title" emoji='🦆'>
  - The type comes with a color scheme and default emoji
  - "title" is optional
  - "emoji" is optional
  - "collapsible" is only added when the collapsible feature is wanted
</Callout>
```
      </Tabs.Tab>
    </Tabs>
  </Tabs.Tab>
</Tabs>

## Option Table

<Tabs items={['Example', 'Code']}>
  <Tabs.Tab>
    <OptionTable
      options={[
        ['EXAMPLE', 'boolean', 'This is an example.','EXAMPLE=true'],
      ]}
    />
  </Tabs.Tab>

  <Tabs.Tab>
```mdx filename="Option Table"
<OptionTable
  options={[
    ['EXAMPLE', 'boolean', 'This is an example.','EXAMPLE=true'],
  ]}
/>
```
  </Tabs.Tab>
</Tabs>

## File Tree

<Tabs items={['Example','Code']}>
&nbsp;

<Tabs.Tab>

<FileTree>
  <FileTree.Folder name="src" defaultOpen>
    <FileTree.File name="main.js" active />
    <FileTree.Folder name="components">
      <FileTree.File name="Header.js" />
      <FileTree.File name="Footer.js" />
    </FileTree.Folder>
  </FileTree.Folder>
  <FileTree.File name="package.json" />
</FileTree>

</Tabs.Tab>

<Tabs.Tab>

- `<FileTree.Folder>`: Represents a folder. It can hold nested Folder or File components.
- `<FileTree.File>`: Represents a file. This component only requires a name prop, which determines the file name displayed in the UI.
- `name`: This determines the name of the file or folder as displayed in the UI.
- `defaultOpen`: This shows the initial state of a Folder. If it is set to true, the Folder will be expanded when first rendered.
- `open`: If this prop is passed to a Folder, it will always be open regardless of user interaction. This is useful when you want to maintain certain sections of the file tree exposed.
- `active`: When this prop is used in a File, the file name will be highlighted. This is handy when you want to show users which file is currently active or selected.

```tsx copy filename="file-tree.tsx"
<FileTree>
  <FileTree.Folder name="src" defaultOpen>
    <FileTree.File name="main.js" active />
    <FileTree.Folder name="components">
      <FileTree.File name="Header.js" />
      <FileTree.File name="Footer.js" />
    </FileTree.Folder>
  </FileTree.Folder>
  <FileTree.File name="package.json" />
</FileTree>
```

[<span style={{ fontSize: '0.8rem' }}>Source Code ↗</span>](https://github.com/shuding/nextra/blob/main/packages/nextra/src/components/file-tree.tsx)

</Tabs.Tab>
</Tabs>

## Cards

<Tabs items={['Card Array', 'Single Card']}>
  <Tabs.Tab>
    <Tabs items={['Example', 'Code']}>
      <Tabs.Tab>

<Cards>
  <Cards.Card icon={<ArrowRight />} title="ArrowRight" href="/" />
  <Cards.Card icon={<ArrowUp />} title="ArrowUp" href="/" />
  <Cards.Card icon={<SvgBox />} title="SvgBox" href="/" />
  <Cards.Card icon={<Brush />} title="Brush" href="/" />
  <Cards.Card icon={<SvgCards />} title="SvgCards" href="/" />
  <Cards.Card icon={<ChevronRight />} title="ChevronRight" href="/" />
  <Cards.Card icon={<Close />} title="Close" href="/" />
  <Cards.Card icon={<Cloud />} title="Cloud" href="/" />
  <Cards.Card icon={<Code />} title="Code" href="/" />
  <Cards.Card icon={<Diagram />} title="Diagram" href="/" />
  <Cards.Card icon={<Dropper />} title="Dropper" href="/" />
  <Cards.Card icon={<File />} title="File" href="/" />
  <Cards.Card icon={<Files />} title="Files" href="/" />
  <Cards.Card icon={<SvgFolderTree />} title="SvgFolderTree" href="/" />
  <Cards.Card icon={<Formula />} title="Formula" href="/" />
  <Cards.Card icon={<Gear />} title="Gear" href="/" />
  <Cards.Card icon={<Globe />} title="Globe" href="/" />
  <Cards.Card icon={<IdCard />} title="IdCard" href="/" />
  <Cards.Card icon={<Lightning />} title="Lightning" href="/" />
  <Cards.Card icon={<SvgLink />} title="Link" href="/" />
  <Cards.Card icon={<Markdown />} title="Markdown" href="/" />
  <Cards.Card icon={<Newsletter />} title="Newsletter" href="/" />
  <Cards.Card icon={<One />} title="One" href="/" />
  <Cards.Card icon={<Picture />} title="Picture" href="/" />
  <Cards.Card icon={<Rows />} title="Rows" href="/" />
  <Cards.Card icon={<Search />} title="Search" href="/" />
  <Cards.Card icon={<Sort />} title="Sort" href="/" />
  <Cards.Card icon={<Stars />} title="Stars" href="/" />
  <Cards.Card icon={<SvgSwitch />} title="SvgSwitch" href="/" />
  <Cards.Card icon={<SvgTable />} title="SvgTable" href="/" />
  <Cards.Card icon={<Terminal />} title="Terminal" href="/" />
  <Cards.Card icon={<Warning />} title="Warning" href="/" />
  <Cards.Card icon="🚀&nbsp;&ensp;" title="Emoji" href="/" />
</Cards>

      </Tabs.Tab>
      <Tabs.Tab>

```tsx copy filename="Card Array"
import { Code } from '@/components/svg/'
;<Cards>
  <Cards.Card icon={<Code />} title="Code" href="/" />
</Cards>
```

      </Tabs.Tab>
    </Tabs>

  </Tabs.Tab>
  <Tabs.Tab>
    <Tabs items={['Example', 'Code']}>
      <Tabs.Tab>

{' '}

<Cards.Card icon={<ArrowRight />} title="ArrowRight" href="/" />
<Cards.Card icon={<ArrowUp />} title="ArrowUp" href="/" />
<Cards.Card icon={<SvgBox />} title="SvgBox" href="/" />
<Cards.Card icon={<Brush />} title="Brush" href="/" />
<Cards.Card icon={<SvgCards />} title="SvgCards" href="/" />
<Cards.Card icon={<ChevronRight />} title="ChevronRight" href="/" />
<Cards.Card icon={<Close />} title="Close" href="/" />
<Cards.Card icon={<Cloud />} title="Cloud" href="/" />
<Cards.Card icon={<Code />} title="Code" href="/" />
<Cards.Card icon={<Diagram />} title="Diagram" href="/" />
<Cards.Card icon={<Dropper />} title="Dropper" href="/" />
<Cards.Card icon={<File />} title="File" href="/" />
<Cards.Card icon={<Files />} title="Files" href="/" />
<Cards.Card icon={<SvgFolderTree />} title="SvgFolderTree" href="/" />
<Cards.Card icon={<Formula />} title="Formula" href="/" />
<Cards.Card icon={<Gear />} title="Gear" href="/" />
<Cards.Card icon={<Globe />} title="Globe" href="/" />
<Cards.Card icon={<IdCard />} title="IdCard" href="/" />
<Cards.Card icon={<Lightning />} title="Lightning" href="/" />
<Cards.Card icon={<SvgLink />} title="Link" href="/" />
<Cards.Card icon={<Markdown />} title="Markdown" href="/" />
<Cards.Card icon={<Newsletter />} title="Newsletter" href="/" />
<Cards.Card icon={<One />} title="One" href="/" />
<Cards.Card icon={<Picture />} title="Picture" href="/" />
<Cards.Card icon={<Rows />} title="Rows" href="/" />
<Cards.Card icon={<Search />} title="Search" href="/" />
<Cards.Card icon={<Sort />} title="Sort" href="/" />
<Cards.Card icon={<Stars />} title="Stars" href="/" />
<Cards.Card icon={<SvgSwitch />} title="SvgSwitch" href="/" />
<Cards.Card icon={<SvgTable />} title="SvgTable" href="/" />
<Cards.Card icon={<Terminal />} title="Terminal" href="/" />
<Cards.Card icon={<Warning />} title="Warning" href="/" />
<Cards.Card icon="🚀&nbsp;&ensp;" title="Emoji" href="/" />

      </Tabs.Tab>
      <Tabs.Tab>

```tsx copy filename="Single Cards"
import { Code } from '@/components/svg/'
;<Cards.Card icon={<Code />} title="Code" href="/" />
```

      </Tabs.Tab>
    </Tabs>

  </Tabs.Tab>
</Tabs>

## Default Button

<Tabs items={['Example', 'Code']}>
  <Tabs.Tab>
  Open LibreChat's website in a new tab:

{' '}

<Button onClick={() => window.open('https://librechat.ai', '_blank')}>🪶 librechat.ai</Button>

Open LibreChat's website in the current tab:

{' '}

<Button onClick={() => window.open('https://librechat.ai', '_self')}>🪶 librechat.ai</Button>

And here's a button with Alert:

  <Button className="my-extra-class" onClick={() => alert('This is an Alert')}>
    ⚠️ Alert Button
  </Button>
  </Tabs.Tab>

  <Tabs.Tab>
  ```tsx copy filename="button"
  <Button onClick={() => window.open("https://librechat.ai", '_blank')}>🪶 librechat.ai</Button>

{' '}

<Button onClick={() => window.open('https://librechat.ai', '_self')}>🪶 librechat.ai</Button>

  <Button className="my-extra-class" onClick={() => alert('This is an Alert')}>
    ⚠️ Alert Button
  </Button>
  ```
  [<span style={{ fontSize: '0.8rem' }}>Source Code ↗</span>](<https://github.com/shuding/nextra/blob/main/packages/nextra/src/components/button.tsx>)

  </Tabs.Tab>
</Tabs>

## Features

<Tabs items={['Example', 'Code']}>
  <Tabs.Tab>
    <Features>
      <Feature href="https://example.com">
        This is a feature with a link.
      </Feature>
      <Feature medium lightOnly>
        This is a medium "lightOnly" feature occupying two spaces.
      </Feature>
      <Feature large centered>
        This is a large feature with "centered" attribute.
      </Feature>
      <Feature large>
        ![](/images/socialcards/default-image.png)
      </Feature>
    </Features>

  </Tabs.Tab>

  <Tabs.Tab>

This example showcases the usage of the `<Features>` and `<Feature>` components for displaying feature sections. The `<Feature>` component can be customized with different props such as `href` for linking, `medium` or `large` for size, `lightOnly` for light mode only, and `centered` for centering the content.

The provided code includes a mix of Markdown content and the `<Features>` section. Inside the `<Features>` section, there are four `<Feature>` components with different configurations to demonstrate different use cases.

```tsx copy filename="features"
import { Feature, Features } from '@/components/features'
;<Features>
  <Feature href="https://example.com">This is a feature with a link.</Feature>
  <Feature medium lightOnly>
    This is a medium "lightOnly" feature occupying two spaces.
  </Feature>
  <Feature large centered>
    This is a large feature with "centered" attribute.
  </Feature>
  <Feature large>![](/images/banner.png)</Feature>
</Features>
```

[<span style={{ fontSize: '0.8rem' }}>Source: ./components/features/</span>]()

  </Tabs.Tab>
</Tabs>

## Simple Tabs

<Tabs items={['Example', 'Code']}>
  <Tabs.Tab>
    <Tabs items={['React', 'Vue', 'Angular']}>
      <Tabs.Tab>
        **React** is a JavaScript library for building user interfaces.
      </Tabs.Tab>
      <Tabs.Tab>
        **Vue** is a progressive JavaScript framework for building user interfaces.
      </Tabs.Tab>
      <Tabs.Tab>
        **Angular** is a platform for building mobile and desktop web applications.
      </Tabs.Tab>
    </Tabs>
  </Tabs.Tab>

  <Tabs.Tab>
  ```tsx copy filename="simple-tabs"
<Tabs items={['React', 'Vue', 'Angular']}>
  <Tabs.Tab>
    **React** is a JavaScript library for building user interfaces.
  </Tabs.Tab>
  <Tabs.Tab>
    **Vue** is a progressive JavaScript framework for building user interfaces.
  </Tabs.Tab>
  <Tabs.Tab>
    **Angular** is a platform for building mobile and desktop web applications.
  </Tabs.Tab>
</Tabs>
  ```
  [<span style={{ fontSize: '0.8rem' }}>Source Code ↗</span>](https://github.com/shuding/nextra/blob/main/packages/nextra/src/components/tabs.tsx)
  </Tabs.Tab>
</Tabs>

---

In this example, we have a single `<Tabs>` component with three tabs: "React", "Vue", and "Angular". Each tab contains a brief description of the respective technology.

The `items` prop on the `<Tabs>` component is an array of strings that represents the labels for each tab. The content for each tab is placed inside the corresponding `<Tabs.Tab>` component.

This demonstrates a straightforward usage of the `<Tabs>` component without any nesting.

## Nested Tabs

This example demonstrates how to create nested tabs using the `<Tabs>` and `<Tabs.Tab>` components. The outer `<Tabs>` component has two tabs: "Fruit" and "Vegetable". Inside each of these tabs, there are nested tabs for specific fruits and vegetables.

The `items` prop on the `<Tabs>` component is an array of strings that represents the labels for each tab. The content for each tab is placed inside the corresponding `<Tabs.Tab>` component.

<Tabs items={['Example', 'Code']}>
  <Tabs.Tab>
    <Tabs items={['Fruit', 'Vegetable']}>
      <Tabs.Tab>
        <Tabs items={['Apple', 'Orange']}>
          <Tabs.Tab>
            Apple is a fruit.
          </Tabs.Tab>
          <Tabs.Tab>
            Orange is a fruit.
          </Tabs.Tab>
        </Tabs>
      </Tabs.Tab>
      <Tabs.Tab>
        <Tabs items={['Carrot', 'Broccoli']}>
          <Tabs.Tab>
            Carrot is a vegetable.
          </Tabs.Tab>
          <Tabs.Tab>
            Broccoli is a vegetable.
          </Tabs.Tab>
        </Tabs>
      </Tabs.Tab>
    </Tabs>
  </Tabs.Tab>

  <Tabs.Tab>
  ```tsx copy filename="nested-tabs"
  <Tabs items={['A', 'B']}>
    <Tabs.Tab>
      <Tabs items={['1a', '2a']}>
        <Tabs.Tab>
          **Content for 1a**
        </Tabs.Tab>
        <Tabs.Tab>
          **Content for 2a**
        </Tabs.Tab>
      </Tabs>
    </Tabs.Tab>
    <Tabs.Tab>
      <Tabs items={['1b', '2b']}>
        <Tabs.Tab>
          **Content for 1b**
        </Tabs.Tab>
        <Tabs.Tab>
          **Content for 2b**
        </Tabs.Tab>
      </Tabs>
    </Tabs.Tab>
  </Tabs>
  ```
  [<span style={{ fontSize: '0.8rem' }}>Source Code ↗</span>](https://github.com/shuding/nextra/blob/main/packages/nextra/src/components/tabs.tsx)
  </Tabs.Tab>
</Tabs>

## Video

<Tabs items={['Example', 'Code']}>
  <Tabs.Tab>
    <video
      muted
      autoPlay
      playsInline
      controls
    >
      <source src="https://github.com/danny-avila/LibreChat/assets/32828263/0464f122-eb20-49cd-b201-57db50c57376" />
    </video>
  </Tabs.Tab>

  <Tabs.Tab>
  ```tsx copy filename="video"
    <video
      muted
      autoPlay
      playsInline
      controls
    >
      <source src="/videos/example.mp4" />
    </video>
  ```
  </Tabs.Tab>
</Tabs>

## Steps

<Tabs items={['Example','Code']}>
&nbsp;

<Tabs.Tab>

<Steps>- Step 1: example - Step 2: example - Step 3: example - Step 4: example</Steps>

</Tabs.Tab>

<Tabs.Tab>

[<span style={{ fontSize: '0.8rem' }}>Source Code ↗</span>](https://github.com/shuding/nextra/blob/main/packages/nextra/src/components/steps.tsx)

```tsx copy filename="steps.tsx"
<Steps>- Step 1: example - Step 2: example - Step 3: example - Step 4: example</Steps>
```

</Tabs.Tab>
</Tabs>



================================================
FILE: pages/docs/documentation/index.mdx
================================================
---
title: Intro
description: Comprehensive guide on how to contribute to our documentation
---

# Contributing to the Documentation

Contributions to the documentation are welcome! This guide explains how to contribute to the LibreChat documentation by writing and formatting new documentation. Our website is built with Nextra 3 and our docs use the `.mdx` format (augmented markdown).

### When to Write a Doc vs. a Blog Post

<Callout type="info" title="Blog vs Docs">
Consider publishing a blog post when a document is an extension of an existing one, relates to a specific situation, or requires external maintenance (features not actively used by the team).

**See: [Contributing to Blog](/blog/2024-04-17_blog_guide)**

</Callout>

## Getting Started

- Fork the LibreChat Documentation repository: [https://github.com/LibreChat-AI/librechat.ai](https://github.com/LibreChat-AI/librechat.ai)

- Create a branch on your fork, name it properly, and link it to the original repository.

```sh filename="Download your LibreChat.ai branch"
git clone -b branch-name https://github.com/username/librechat.ai.git
```

> Replace `branch-name` and `username` with your details

## Creating New Documents

To create a new document:

- Use the `.mdx` file extension (see [MDX documentation](https://mdxjs.com/) for more info).
- Name files using **lowercase letters** and **underscores** (e.g., `documentation_guidelines.mdx`).
- Place new documents in the relevant folder/sub-folder under `./docs`.
- Add the document to the table of contents in the `_meta.ts` file of the folder where your document is located. If you don't add it, it will be alphabetically sorted after the ordered docs.

## Markdown Formatting Guidelines

- Use headings and subheadings with `#`, `##`, and `###`.
  - Use `#` for the document title (**only one main title per document is allowed**).
  - Use `##` for main sections.
  - Use `###` for sub-sections within a section.
- Use `**` to make text **bold** and highlight important information (do not use in place of a heading).
- Use URL paths to link to other documents (e.g., `/docs/documentation` points the current [doc](/docs/documentation)).
- You can use HTML, TS, and JS to add additional features to a document.
- Ensure any HTML has closing tags (e.g., `<img src="" />` or `<a href="link"></a>`).
- Do not use HTML comments; instead, use [Markdown comments](https://gist.github.com/jonikarppinen/47dc8c1d7ab7e911f4c9?permalink_comment_id=4272770#gistcomment-4272770) **only if the text is actually hidden**.

### Docs Resources

<Callout type="tip" title="Docs Resources">

See some integrated components examples:

- [Components Examples](/docs/documentation/examples)

For more information, refer to:

- [Nextra](https://nextra.site/docs/docs-theme/start)
- [Nextra 3](https://the-guild.dev/blog/nextra-3#intro)
- [MDX](https://mdxjs.com/)

</Callout>

## Document Metadata

Add metadata to the header of your document using the following format:

> Note: The `ogImage` field is optional and can be omitted altogether. It is used to specify the image that will be displayed when sharing your document on social media platforms.

```yaml filename="metadata example:"
---
title: Document Title
description: This description will be used in social cards and search engine results.
ogImage: /images/docs/<category>/image.png (optional)
---
```

## Assets

Whenever possible, upload assets (e.g., images) to GitHub instead of storing them in the `/public` folder. This helps keep your repository organized and makes it easier to manage your assets.

### Images

**Note In the followng example:**
  - I provided screenshots for both lihgt and dark mode.
  - I used `Image from 'next/image'` wich gave me 4x improvement on the image file size for better performances.

see the following example in action here: [User Guides](/docs/user_guides)

```mdx filename="Example"
import Image from 'next/image'

<div style={{padding: "20px", display: "flex", justifyContent: "center", alignItems: "center", flexDirection: "column"}}>
  <div className="image-light-theme">
    <Image src="https://github.com/danny-avila/LibreChat/assets/32828263/cf0f3231-287a-407f-bd4d-3d5bad94e893" alt="ipad-light" width={1024} height={512} style={{borderRadius: "5px"}} />
  </div>

  <div className="image-dark-theme">
    <Image src="https://github.com/danny-avila/LibreChat/assets/32828263/a03ee02d-5099-4220-95b0-bfa2d3b00b4d" alt="ipad-dark" width={1024} height={512} style={{borderRadius: "5px"}} />
  </div>
</div>
```

### How to Upload Images and Videos on GitHub

<Callout title="Method A" emoji="🅰️" collapsible>
    - Go to the LibreChat repository
    - Find a random conversation or PR
    - Paste an image from your clipboard into the text input box. It will automatically be converted into a URL.
    - Copy and paste the resulting URL in your document. (Then exit the page without actually posting the comment.😉)

    ![image](https://github.com/danny-avila/LibreChat/assets/32828263/c1612f93-a6c0-4af7-9965-9f83872cff00)

</Callout>

<Callout title="Method B" emoji="🅱️" collapsible>
    - Upload directly from the web UI:

    ![image](https://github.com/danny-avila/LibreChat/assets/32828263/4f138ab4-31a5-4fae-a459-5335e5ff25a8)

</Callout>

## Test the Docs

<Callout type="warning" title="Before you submit">
### Review carefully before submitting your PR

Before submitting a PR for your blog post, **always** test to ensure everything looks and functions as intended.

#### Check the following:

- Your new document(s) for layout, accuracy and completeness
- The document's position in the Table of Contents (ToC)
- The image and link in your document

#### To test:

1. Prepare the environment by running `pnpm install`
2. Start the dev server with `pnpm dev`
3. Test the build by running `pnpm build` followed by `pnpm start`

</Callout>



================================================
FILE: pages/docs/documentation/syntax_highlighting.mdx
================================================
---
title: Syntax Highlighting
description: All about syntax highlighting
---

# Syntax Highlighting

Nextra uses [Shiki](https://shiki.matsu.io) to do syntax highlighting at build time. It's very reliable and performant. For example, adding this in your Markdown file:

````md filename="Markdown"
```js
console.log('hello, world')
```
````

Result in this:

```js
console.log('hello, world')
```

## Features

### Inlined Code

Inlined syntax highlighting like `let x = 1{:jsx}` is also supported via the `{:}` syntax:

<Tabs items={['Example', 'Code']}>
  <Tabs.Tab>
    Inlined syntax highlighting is also supported: `let x = 1{:jsx}`
  </Tabs.Tab>

  <Tabs.Tab>
```md filename="Markdown"
Inlined syntax highlighting is also supported: `let x = 1{:jsx}`
```
  </Tabs.Tab>
</Tabs>

### Filenames and Titles

You can add a filename or a title to your code blocks by adding a `filename` attribute:

Renders:

<Tabs items={['Example', 'Code']}>
  <Tabs.Tab>
  ```js filename="example.js"
  console.log('hello, world')
  ```
  </Tabs.Tab>

  <Tabs.Tab>
````md filename="Markdown"
```js filename="example.js"
console.log('hello, world')
```
````
  </Tabs.Tab>
</Tabs>

### Highlighting Lines

You can highlight specific lines of code by adding a `{}` attribute to the code block:

<Tabs items={['Example', 'Code']}>
  <Tabs.Tab>
  ```js {1,4-5}
  import { useState } from 'react'

function Counter() {
const [count, setCount] = useState(0)
return <button onClick={() => setCount(count + 1)}>{count}</button>
}

`````
</Tabs.Tab>

<Tabs.Tab>
````md filename="Markdown"
```js {1,4-5}
import { useState } from 'react'

function Counter() {
const [count, setCount] = useState(0)
return <button onClick={() => setCount(count + 1)}>{count}</button>
}
`````

`````
  </Tabs.Tab>
</Tabs>

### Highlighting Substrings

You can highlight specific substrings of code by adding a `//` attribute to the code block:

<Tabs items={['Example', 'Code']}>
  <Tabs.Tab>
  ```js /useState/
  import { useState } from 'react'

  function Counter() {
    const [count, setCount] = useState(0)
    return <button onClick={() => setCount(count + 1)}>{count}</button>
  }
  ```
  </Tabs.Tab>

  <Tabs.Tab>
````md filename="Markdown"
```ts /useState/
import { useState } from 'react'

function Counter() {
  const [count, setCount] = useState(0)
  return <button onClick={() => setCount(count + 1)}>{count}</button>
}
```
`````

  </Tabs.Tab>
</Tabs>

You can highlight only a part of the occurrences of that substring by adding a number it: `/str/1`, or multiple: `/str/1-3`, `/str/1,3`.

### Line Numbers

You can add line numbers to your code blocks by adding a `showLineNumbers` attribute:

<Tabs items={['Example', 'Code']}>
  <Tabs.Tab>
```ts showLineNumbers
import { useState } from 'react'

function Counter() {
const [count, setCount] = useState(0)
return <button onClick={() => setCount(count + 1)}>{count}</button>
}

`````
  </Tabs.Tab>

  <Tabs.Tab>
````md filename="Markdown"
```ts showLineNumbers
import { useState } from 'react'

function Counter() {
  const [count, setCount] = useState(0)
  return <button onClick={() => setCount(count + 1)}>{count}</button>
}
`````

`````
  </Tabs.Tab>
</Tabs>

### ANSI Highlighting

You can highlight ANSI escape codes:

<Tabs items={['Example', 'Code']}>
  <Tabs.Tab>
  ```ansi filename="ANSI escape codes"
  [0m [0;32m✓[0m [0;2msrc/[0mindex[0;2m.test.ts (1)[0m
    [0;2m Test Files [0m [0;1;32m1 passed[0;98m (1)[0m
    [0;2m      Tests [0m [0;1;32m1 passed[0;98m (1)[0m
    [0;2m   Start at [0m 23:32:41
    [0;2m   Duration [0m 11ms
    [42;1;39;0m PASS [0;32m Waiting for file changes...[0m
          [0;2mpress [0;1mh[0;2m to show help, press [0;1mq[0;2m to quit
  ```
  </Tabs.Tab>

  <Tabs.Tab>
````md filename="Markdown"
```ansi filename="ANSI escape codes"
[0m [0;32m✓[0m [0;2msrc/[0mindex[0;2m.test.ts (1)[0m
  [0;2m Test Files [0m [0;1;32m1 passed[0;98m (1)[0m
  [0;2m      Tests [0m [0;1;32m1 passed[0;98m (1)[0m
  [0;2m   Start at [0m 23:32:41
  [0;2m   Duration [0m 11ms
  [42;1;39;0m PASS [0;32m Waiting for file changes...[0m
         [0;2mpress [0;1mh[0;2m to show help, press [0;1mq[0;2m to quit
```
`````

  </Tabs.Tab>
</Tabs>

## Supported Languages

Check [this list](https://github.com/shikijs/shiki/blob/main/docs/languages.md) for all supported languages.

## Customize the Theme

Nextra uses CSS variables to define the colors for tokens. You can inject a [global CSS](https://nextjs.org/docs/basic-features/built-in-css-support#adding-a-global-stylesheet) to customize them under light/dark themes. For example this is the default tokens and you can override any of these:

```css filename="styles.css"
:root {
  --shiki-color-text: #414141;
  --shiki-color-background: transparent;
  --shiki-token-constant: #1976d2;
  --shiki-token-string: #22863a;
  --shiki-token-comment: #aaa;
  --shiki-token-keyword: #d32f2f;
  --shiki-token-parameter: #ff9800;
  --shiki-token-function: #6f42c1;
  --shiki-token-string-expression: #22863a;
  --shiki-token-punctuation: #212121;
  --shiki-token-link: #22863a;
}

.dark {
  --shiki-color-text: #d1d1d1;
  --shiki-token-constant: #79b8ff;
  --shiki-token-string: #ffab70;
  --shiki-token-comment: #6b737c;
  --shiki-token-keyword: #f97583;
  --shiki-token-parameter: #ff9800;
  --shiki-token-function: #b392f0;
  --shiki-token-string-expression: #4bb74a;
  --shiki-token-punctuation: #bbb;
  --shiki-token-link: #ffab70;
}
```

## With Dynamic Content

Since syntax highlighting is done at build time, you can't use dynamic content in your code blocks. However, since MDX is very powerful there is a workaround via client JS. For example:

<Tabs items={['Example', 'Code']}>
  <Tabs.Tab>

import { useEffect, useRef } from 'react'

export function DynamicCode({ children }) {
  const ref = useRef()
  const tokenRef = useRef()
  // Find the corresponding token from the DOM
  useEffect(() => {
    if (ref.current) {
      const token = [...ref.current.querySelectorAll('code span')].find(
        (el) => el.innerText === '1',
      )
      tokenRef.current = token
    }
  }, [])
  return (
    <>
      <div ref={ref} style={{ marginTop: '1.5rem' }}>
        {children}
      </div>
      <a
        onClick={() => {
          tokenRef.current.innerText = (parseInt(tokenRef.current.innerText) || 0) + 1
        }}
        style={{
          cursor: 'pointer',
          userSelect: 'none',
        }}
      >
        Increase the number
      </a>
      <a
        onClick={() => {
          tokenRef.current.innerText = '1 + 1'
        }}
        style={{
          marginLeft: '.5rem',
          cursor: 'pointer',
          userSelect: 'none',
        }}
      >
        Change to `1 + 1`
      </a>
    </>
  )
}

<DynamicCode>
```js filename="dynamic_code.js"
function hello () {
  const x = 2 + 3
  console.log(1)
}
```
</DynamicCode>

</Tabs.Tab>

  <Tabs.Tab>
  ````jsx
  import { useEffect, useRef } from 'react'

export function DynamicCode({ children }) {
  const ref = useRef()
  const tokenRef = useRef()
  // Find the corresponding token from the DOM
  useEffect(() => {
    if (ref.current) {
      const token = [...ref.current.querySelectorAll('code span')].find(
        (el) => el.innerText === '1',
      )
      tokenRef.current = token
    }
  }, [])
  return (
    <>
      <div ref={ref} style={{ marginTop: '1.5rem' }}>
        {children}
      </div>
      <a
        onClick={() => {
          tokenRef.current.innerText = (parseInt(tokenRef.current.innerText) || 0) + 1
        }}
        style={{
          cursor: 'pointer',
          userSelect: 'none',
        }}
      >
        Increase the number
      </a>
      <a
        onClick={() => {
          tokenRef.current.innerText = '1 + 1'
        }}
        style={{
          marginLeft: '.5rem',
          cursor: 'pointer',
          userSelect: 'none',
        }}
      >
        Change to `1 + 1`
      </a>
    </>
  )
}

  <DynamicCode>
  ```js filename="dynamic_code.js"
  function hello () {
    const x = 2 + 3
    console.log(1)
  }
  ```
  </DynamicCode>
  ````
  </Tabs.Tab>
</Tabs>

This workaround has a limitation that updated content won't be re-highlighted. For example if we update the number to `1 + 1`, it will be incorrectly highlighted.

## Disable Syntax Highlighting

You can opt out of syntax highlighting for using one of your own. You can disable syntax highlighting globally by setting `codeHighlight: false` in your Nextra configuration (`next.config.js` file).

<OptionTable
  options={[
    ['codeHighlight', 'boolean', 'Enable or disable syntax highlighting.', 'codeHighlight: true'],
  ]}
/>

## Custom Grammar

Shiki accepts a [VSCode TextMate Grammar](https://code.visualstudio.com/api/language-extensions/syntax-highlight-guide) for syntax highlighting with custom language grammars.

You can provide these grammars by overriding the `getHighlighter` function in `mdxOptions.rehypePrettyCodeOptions` option in your Nextra config inside `next.config.js`:

<Tabs items={['Example', 'Code']}>
  <Tabs.Tab>

```js filename="next.config.js" {13-18}
import { BUNDLED_LANGUAGES } from 'shiki'

nextra({
  // ... other options
  mdxOptions: {
    rehypePrettyCodeOptions: {
      getHighlighter: (options) =>
        getHighlighter({
          ...options,
          langs: [
            ...BUNDLED_LANGUAGES,
            // custom grammar options, see the Shiki documentation for how to provide these options
            {
              id: 'my-lang',
              scopeName: 'source.my-lang',
              aliases: ['mylang'], // Along with id, aliases will be included in the allowed names you can use when writing markdown.
              path: '@/public/syntax/grammar.tmLanguage.json',
            },
          ],
        }),
    },
  },
})
```

  </Tabs.Tab>

<Tabs.Tab>
````mdx
```js filename="next.config.js" {13-18}
import { BUNDLED_LANGUAGES } from 'shiki'

nextra({
  // ... other options
  mdxOptions: {
    rehypePrettyCodeOptions: {
      getHighlighter: (options) =>
        getHighlighter({
          ...options,
          langs: [
            ...BUNDLED_LANGUAGES,
            // custom grammar options, see the Shiki documentation for how to provide these options
            {
              id: 'my-lang',
              scopeName: 'source.my-lang',
              aliases: ['mylang'], // Along with id, aliases will be included in the allowed names you can use when writing markdown.
              path: '@/public/syntax/grammar.tmLanguage.json',
            },
          ],
        }),
    },
  },
})
```

````

  </Tabs.Tab>
</Tabs>

## Custom Themes

Within `mdxOptions.rehypePrettyCodeOptions` you may also provide custom themes instead of [relying on CSS Variables](/docs/guide/syntax-highlighting):

<Tabs items={['Example', 'Code']}>
  <Tabs.Tab>
  ```js filename="next.config.js" {4}
  nextra({
    // ... other options
    mdxOptions: {
      rehypePrettyCodeOptions: {
        // VSCode theme or built-in Shiki theme, see Shiki documentation for more information
        theme: JSON.parse(
          readFileSync('./public/syntax/arctis_light.json', 'utf8')
        )
      }
    }
  })
  ```
  </Tabs.Tab>

  <Tabs.Tab>
````md
```js filename="next.config.js" {4}
nextra({
  // ... other options
  mdxOptions: { rehypePrettyCodeOptions: {
      // VSCode theme or built-in Shiki theme, see Shiki documentation for more information
      theme: JSON.parse(
        readFileSync('./public/syntax/arctis_light.json', 'utf8')
      )
    }
  }
})
```
`````

  </Tabs.Tab>
</Tabs>



================================================
FILE: pages/docs/features/_meta.ts
================================================
export default {
  index: 'Overview',
  agents: 'Agents',
  code_interpreter: 'Code Interpreter API',
  artifacts: 'Artifacts - Generative UI',
  // local_setup: 'Local Setup',
  // custom_endpoints: 'Custom Endpoints',
  url_query: 'URL Query Parameters',
}



================================================
FILE: pages/docs/features/agents.mdx
================================================
---
title: AI Agents - LibreChat's No-Code Agentic Framework
description: Learn how to create, customize, and leverage LibreChat's AI Agents - a powerful framework for building custom AI assistants with any model provider.
---

# Agents: Build Custom AI Assistants

LibreChat's AI Agents feature provides a flexible framework for creating custom AI assistants powered by various model providers.

This feature is similar to OpenAI's Assistants API and ChatGPT's GPTs, but with broader model support and a no-code implementation, letting you build sophisticated assistants with specialized capabilities.

## Getting Started

To create a new agent, select "Agents" from the endpoint menu and open the Agent Builder panel found in the Side Panel.

![Agents - Endpoints Menu](/images/agents/endpoints_menu.png)

The creation form includes:

- **Avatar**: Upload a custom avatar to personalize your agent
- **Name**: Choose a distinctive name for your agent
- **Description**: Optional details about your agent's purpose
- **Instructions**: System instructions that define your agent's behavior
- **Model**: Select from available providers and models

**Existing agents can be selected from the top dropdown of the Side Panel.**
- **Also by mention with "@" in the chat input.**

![Agents - Mention](/images/agents/mention.png)

### Model Configuration

The model parameters interface allows fine-tuning of your agent's responses:

- Temperature (0-1 scale for response creativity)
- Max context tokens
- Max output tokens
- Additional provider-specific settings

## Agent Capabilities

> **Note:** All capabilities can be toggled via the `librechat.yaml` configuration file. See [docs/configuration/librechat_yaml/object_structure/agents#capabilities](/docs/configuration/librechat_yaml/object_structure/agents#capabilities) for more information.

### Code Interpreter

When enabled, the Code Interpreter capability allows your agent to:
- Execute code in multiple languages, including:
  - Python, JavaScript, TypeScript, Go, C, C++, Java, PHP, Rust, and Fortran
- Process files securely through the LibreChat Code Interpreter API
- Run code without local setup, configuration, or sandbox deployment
- Handle file uploads and downloads seamlessly
- [More info about the Code Interpreter API](/docs/features/code_interpreter)
  - **Requires an API Subscription from [code.librechat.ai](https://code.librechat.ai/pricing)**

### File Search

The File Search capability enables:
- RAG (Retrieval-Augmented Generation) functionality
- Semantic search across uploaded documents
- Context-aware responses based on file contents
- File attachment support at both agent and chat thread levels

### File Context (using Optical Character Recognition)

The File Context (OCR) capability allows your agent to extract and process text from images and documents:

- Extract text while maintaining document structure and formatting
- Process complex layouts including multi-column text and mixed content
- Handle tables, equations, and other specialized content
- Work with multilingual content
- [More info about OCR](/docs/features/ocr)
  - **Currently uses Mistral OCR API which may incur costs**

### Artifacts

The Artifacts capability enables your agent to generate and display interactive content:

- Create React components, HTML code, and Mermaid diagrams
- Display content in a separate UI window for clarity and interaction
- Configure artifact-specific instructions at the agent level
- [More info about Artifacts](/docs/features/artifacts)

When enabled, additional instructions specific to the use of artifacts are added by default. Options include:

- **Enable shadcn/ui instructions**: Adds instructions for using shadcn/ui components (a collection of re-usable components built using Radix UI and Tailwind CSS)
- **Custom Prompt Mode**: When enabled, the default artifacts system prompt will not be included, allowing you to provide your own custom instructions

Configuring artifacts at the agent level is the preferred approach, as it allows for more granular control compared to the legacy app-wide configuration.

If you enable Custom Prompt Mode, you should include at minimum the basic artifact format in your instructions. Here's a simple example of the minimum instructions needed:

```
When creating content that should be displayed as an artifact, use the following format:

:::artifact{identifier="unique-identifier" type="mime-type" title="Artifact Title"}
```
Your artifact content here
```
:::

For the type attribute, use one of:
- "text/html" for HTML content
- "application/vnd.mermaid" for Mermaid diagrams
- "application/vnd.react" for React components
- "image/svg+xml" for SVG images
```

### Tools

Agents can also be enhanced with various built-in tools:

- **DALL-E-3**: Image generation from text descriptions
- **Tavily Search**: Advanced search API with diverse data source integration
- **Calculator**: Mathematical calculations
- **Google Search**: Access to web search functionality
- **Stable Diffusion**: Text-to-image generation
- **Azure AI Search**: Information retrieval
- **Traversaal**: A robust search API for LLM Agents
- **Wolfram**: Computational and mathematical capabilities

- Tools can be disabled using the [`librechat.yaml`](/docs/configuration/librechat_yaml) configuration file:
  - [More info](/docs/configuration/librechat_yaml/object_structure/agents#capabilities)

### Actions

With the Actions capability, you can dynamically create tools from [OpenAPI specs](https://swagger.io/specification/) to add to your Agents.

![Agents - Endpoints Menu](/images/agents/actions.png)

**Clicking the button above will open a form where you can input the OpenAPI spec URL and create an action:**

![Agents - Endpoints Menu](/images/agents/actions_panel.png)

- Actions can be disabled using the [`librechat.yaml`](/docs/configuration/librechat_yaml) configuration file:
  - [More info](/docs/configuration/librechat_yaml/object_structure/agents#capabilities)
- Individual domains can be whitelisted for agent actions:
  - [More info](/docs/configuration/librechat_yaml/object_structure/actions#alloweddomains)
- Note that you can add add the 'x-strict': true flag at operation-level in the OpenAPI spec for actions.
If using an OpenAI model supporting it, this will automatically generate function calls with 'strict' mode enabled.
  - Strict mode supports only a partial subset of json. Read https://platform.openai.com/docs/guides/structured-outputs/some-type-specific-keywords-are-not-yet-supported for details.

### Agent Chain

The Agent Chain capability enables a Mixture-of-Agents (MoA) approach, allowing you to create a sequence of agents that work together:

- Create chains of specialized agents for complex tasks
- Each agent in the chain can access outputs from previous agents
- Configure the maximum number of steps for the agent chain
- **Note:** Access this feature from the Advanced Settings panel in the Agent Builder
- **Note:** This feature is currently in beta and may be subject to change
  - The current maximum of agents that can be chained is 10, but this may be configurable in the future

<img src="https://firebasestorage.googleapis.com/v0/b/superb-reporter-407417.appspot.com/o/agent_chain.png?alt=media&token=bfa209b9-d2ab-403f-b097-6300b4017fc8" alt="Agent Chain" width={488} height={269}/>

This feature introduces a layered Mixture-of-Agents architecture to LibreChat, where each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response, as described in [the eponymous "Mixture-of-Agents" paper](https://arxiv.org/abs/2406.04692).

### Advanced Settings

Advanced settings for your agent (found in the Advanced view of the Agent form) outside of "capabilities."

#### Max Agent Steps

This setting allows you to limit the number of steps an agent can take in a "run," which refers to the agent loop before a final response is given.

If left unconfigured, the default is 25 steps, but you can adjust this to suit your needs. For admins, you can set a global default as well as a global maximum in the [`librechat.yaml`](/docs/configuration/librechat_yaml/object_structure/agents#recursionlimit) file.

A "step" refers to either an AI API request or a round of tool usage (1 or many tools, depending on how many tool calls the LLM provides from a single request).

A single, non-tool response is 1 step. A singular round of tool usage is usually 3 steps:

1. API Request -> 2. Tool Usage (1 or many tools) -> 3. Follow-up API Request

### Model Context Protocol (MCP)

MCP is an open protocol that standardizes how applications provide context to Large Language Models (LLMs), acting like a universal adapter for AI tools and data sources.

Imagine MCP as the "USB-C of AI" - just as USB-C provides a universal connection standard for electronic devices, MCP offers a standardized way to connect AI models to diverse tools, data sources, and services.

By configuring `mcpServers` in the `librechat.yaml` file, you can:
- Add custom tools from various sources
- Integrate specialized data access tools
- Extend AI capabilities beyond default offerings

Here are some tools added by configuring the ["Filesystem" MCP server](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem):
![Agents - MCP](/images/agents/mcp_tools.png)

Key Features:
- Upon app restart, tools are available in the "Add Tools" button within the Agent Builder panel
- Seamlessly connect custom tools and data sources
- Leverage a growing ecosystem of MCP-compatible servers and integrations

Learn More:
- [Configuring MCP Servers](/docs/configuration/librechat_yaml/object_structure/mcp_servers)
- [Model Context Protocol Introduction](https://modelcontextprotocol.io/introduction)

#### ⚠️ MCP Limitations

Most existing MCP servers use `stdio` processes, which work well for local, single-user environments but pose significant challenges for multi-user and production platforms.

**The Challenges**

- STDIO servers are designed for local, single-user contexts
- Not scalable for remote or cloud deployments
- Limited multi-user support (concurrency, authentication, security)

**Emerging Solutions**

Server-Sent Events (SSE) are being explored as a more scalable transport mechanism. MCP developers are actively working on remote connection support, addressing complexities around deployment, authentication, and security.

**Current Status:** Promising, but still a work in progress.

LibreChat is at the forefront of implementing flexible, scalable MCP server integrations to support diverse usage scenarios.

## File Management

Agents support four distinct file upload categories:

1. **Image Upload**: For visual content processing
2. **File Search Upload**: Documents for RAG capabilities
3. **Code Interpreter Upload**: Files for code processing
4. **File Context (OCR)**: Documents processed with OCR and added to the agent's instructions

Files can be attached directly to the agent configuration or within individual chat threads.

![File Context using OCR for agents](/images/ocr/file_context_ocr.png)

Files uploaded as "File Context" are processed using OCR to extract text, which is then added to the Agent's instructions. This is ideal for documents, images with text, or PDFs where you need the full text content of a file to be available to the agent. Note, the OCR is performed at the time of upload and is not stored as a separate file, rather purely as text in the database.

## Sharing and Permissions

### Administrator Controls

Administrators have access to global permission settings:

- Enable/disable agent sharing across all users
- Control agent usage permissions
- Manage agent creation rights
- Configure platform-wide settings

The use of agents for all users can also be disabled via config, [more info](/docs/configuration/librechat_yaml/object_structure/interface).

### User-Level Sharing

Individual users can:
- Share their agents with all users (if enabled)
- Control editing permissions for shared agents
- Manage access to their created agents

### Notes

- Instructions, model parameters, attached files, and tools are only exposed to the user if they have editing permissions
  - An agent may leak any attached data, whether instructions or files, through conversation--make sure your instructions are robust against this
- Only original authors and administrators can delete shared agents
- Agents are private to authors unless shared

## Optional Configuration

LibreChat allows admins to configure the use of agents via the [`librechat.yaml`](/docs/configuration/librechat_yaml) file:

- Disable Agents for all users (including admins): [more info](/docs/configuration/librechat_yaml/object_structure/interface)
- Customize agent capabilities using: [more info](/docs/configuration/librechat_yaml/object_structure/agents)

## Best Practices

- Provide clear, specific instructions for your agent
- Carefully consider which tools are necessary for your use case
- Organize files appropriately across the four upload categories
- Review permission settings before sharing agents
- Test your agent thoroughly before deploying to other users

## Recap

1. Select "Agents" from the endpoint dropdown menu
2. Open the Agent Builder panel
3. Fill out the required agent details
4. Configure desired capabilities (Code Interpreter, File Search, File Context or OCR)
5. Add necessary tools and files
6. Set sharing permissions if desired
7. Create and start using your agent

## What's next?

LibreChat Agents usher in a new era for the app where future pipelines can be streamlined via Agents for specific tasks and workflows across your experience in LibreChat.

Future updates will include:
- General improvements to the current Agent experience
- Multi-agent orchestration for complex workflows
- Ability to customize agents for various functions: titling (chat thread naming), memory management (user context/history), and prompt enhancement (input assistance/predictions)
- More tools, configurable tool parameters, dynamic tool creation.

Furthermore, the update introduces a new paradigm for LibreChat, as its underlying architecture provides a much needed refresh for the app, optimizing both the user experience and overall app performance.

To highlight one notable optimization, an AI generation of roughly 1000 tokens will transfer about 1 MB of data using traditional endpoints (at the time of writing, any endpoint option besides Agents and AWS Bedrock).

Using an agent, the same generation will transfer about about 52 kb of data, a 95% reduction in data transfer, which is that much less of a load on the server and the user's device.

---

AI Agents in LibreChat provide a powerful way to create specialized assistants without coding knowledge while maintaining the flexibility to work with your preferred AI models and providers.

---

#LibreChat #AIAssistants #NoCode #OpenSource



================================================
FILE: pages/docs/features/artifacts.mdx
================================================
---
title: Artifacts - Generative UI
description: Discover LibreChat's revolutionary Artifacts feature for instant creation of React components, HTML code, and Mermaid diagrams.
---

# Artifacts: Generate React, HTML & Diagrams Instantly

- **Generative UI:** Create React components, HTML code, and Mermaid diagrams
- **Flexible Integration:** Use any model you have set up
- **Iterative Design:** Rapidly improve and refine generated outputs
- **Agent Integration:** Configure artifacts at the agent level (recommended approach)

> **Note:** The preferred way to use artifacts is now through the [Agents feature](/docs/features/agents#artifacts), which allows for more granular control by enabling/disabling artifacts at the agent level rather than app-wide.

<div align="center">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/GfTj7O4gmd0?si=NrtqGoodGpfANBfT" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="true"></iframe>
</div>

## Key Features of LibreChat's Code Artifacts:

- Instant prototyping of websites and UIs with generative AI
- Effortless creation of dynamic, responsive layouts
- Interactive learning environment for React and HTML
- Complex idea visualization using Mermaid diagrams
- AI-powered intuitive, user-centric design iterations
- Free and open-source alternative to paid AI tools

Experience the future of UI/UX design and development with LibreChat's generative capabilities. Bring your ideas to life faster than ever before!

## Content-Security-Policy

You may need to need to update your web server's Content-Security-Policy to include `frame-src 'self' https://*.codesandbox.io` in order to load generated HTML apps in the Artifacts panel. This is a dependency of the [sandpack](https://sandpack.codesandbox.io/) library.

## Self-Hosting the Sandpack Bundler

Artifacts in LibreChat use CodeSandbox's Sandpack library to securely render HTML/JS code. By default, LibreChat connects to CodeSandbox's public CDN, which may also transmit telemetry for its usage.

For enhanced privacy, security compliance, or isolated network environments, you can [self-host the bundler.](https://sandpack.codesandbox.io/docs/guides/hosting-the-bundler)

### Why Self-Host the Sandpack Bundler?

[Self-hosting the bundler](https://sandpack.codesandbox.io/docs/guides/hosting-the-bundler) provides several advantages:

- **Privacy & Security**: Keep code execution within your own infrastructure
- **Reliability**: Remove dependency on external services
- **Performance**: Reduce latency by hosting the bundler in your network
- **Compliance**: Meet organizational data handling requirements
- **Control**: Configure and customize to your specific needs

### Configuration

Once you have a self-hosted bundler set up, simply configure LibreChat to use it by setting the environment variable:

```bash
# `.env` file
SANDPACK_BUNDLER_URL=http://your-bundler-url
```

### General Considerations

Self-hosting the bundler introduces some additional overhead:

- **CORS Configuration**: You'll need to manage cross-origin resource sharing policies
- **Security Management**: You become responsible for the security of the bundler service
- **Maintenance**: Updates and patches will need to be applied manually
- **Resource Requirements**: Additional server resources will be needed to host the bundler

For detailed instructions on setting up and maintaining your self-hosted bundler, refer to the [forked CodeSandbox repository](https://github.com/LibreChat-AI/codesandbox-client) tailored for LibreChat deployment, including the removal of telemetry from Sandpack.

---

AI News 2024: The Original and BEST Open-Source AI Chat Platform, LibreChat Releases Code Artifacts! This Generative UI Tool Can Generate React, HTML & Diagrams Instantly from a Single Prompt in Your Browser with Any LLM, OpenAI, Anthropic, LLAMA, Mistral, DeepSeek Coder & More Thanks to OLLAMA Integration.

#AINews2024 #OpenSourceAI #GenerativeUI #CodeArtifacts #WebDevelopment #ReactJS #LLM #AIPrototyping #DevTools #OLLAMA #LibreChat



================================================
FILE: pages/docs/features/authentication.mdx
================================================
---
title: Authentication
description: Quick overview of the user authentication system of LibreChat, which offers secure and easy email and social logins.
--- 

# User Authentication System

LibreChat has a user authentication system that allows users to sign up and log in securely and easily. The system is scalable and can handle a large number of concurrent users without compromising performance or security.

By default, we have email signup and login enabled, which means users can create an account using their email address and a password. They can also reset their password if they forget it.

Additionally, our system can integrate social logins from various platforms such as Google, GitHub, Discord, OpenID, and more. This means users can log in using their existing accounts on these platforms, without having to create a new account or remember another password.

**For further details, refer to the configuration guides provided here: [Authentication](/docs/configuration/authentication)**

<Callout type="warning" title="Important">
- When you run the app for the first time, you need to create a new account by clicking on "Sign up" on the login page. The first account you make will be the admin account. The admin account doesn't have any special features right now, but it might be useful for the admin dashboard to manage other users later.
- The first account created should ideally be a local account (email and password).
</Callout>


<div style={{padding: "20px", display: "flex", justifyContent: "center", alignItems: "center", flexDirection: "column"}}>
  <div className="image-light-theme">
    ![Image for Light Theme](https://github.com/danny-avila/LibreChat/assets/32828263/786fa525-73c4-4640-b4cf-91925ad8802e)
  </div>

  <div className="image-dark-theme">
    ![Image for Dark Theme](https://github.com/danny-avila/LibreChat/assets/32828263/dddc34c6-9602-4177-89e8-4c0db01b0eac)
  </div>
</div>


================================================
FILE: pages/docs/features/code_interpreter.mdx
================================================
---
title: Code Interpreter API
description: Execute code securely and manage files seamlessly with LibreChat's Code Interpreter API
---

# Code Interpreter API

## Introduction

LibreChat's Code Interpreter API provides a secure and hassle-free way to execute code and manage files through a simple API interface. Whether you're using it through LibreChat's Agents or integrating it directly into your applications, the API offers a powerful sandbox environment for running code in multiple programming languages.

<video 
  controls
  width="100%"
  style={{ marginTop: '2rem', marginBottom: '2rem' }}
>
  <source src="https://firebasestorage.googleapis.com/v0/b/superb-reporter-407417.appspot.com/o/Sequence%2001.mp4?alt=media&token=595b0716-12f1-4516-be93-c7004a85b540" type="video/mp4" />
  Your browser does not support the video tag.
</video>

## Subscription 
**Access to this feature requires an [API subscription, get started here](https://code.librechat.ai/pricing).**

## Getting Started

1. Visit [code.librechat.ai](https://code.librechat.ai/pricing) to get your API key
2. Integrate the API into your application or use it through LibreChat
3. Start executing code and generating files securely

## Key Features

### Supported Languages

Execute code in multiple programming languages:
- Python, Node.js (JS/TS), Go, C/C++, Java, PHP, Rust, Fortran, Rscript

### Seamless File Handling

- Upload files for processing
- Download generated outputs
- Secure file management
- Session-based file organization

### Security & Convenience

- Secure sandboxed execution environment
- No local setup required
- No server deployment needed
- No configuration management

## Using the API

### In LibreChat

The API has first-class support in LibreChat through two main methods:

1. **[AI Agents](/docs/features/agents#code-interpreter)**: Enable Code Interpreter in your agent's configuration to allow it to execute code and process files automatically.

2. **Manual Execution**: Use the "Run Code" button in code blocks within the chat interface, as shown here:

![Code Interpreter in LibreChat](/images/agents/run_code.png)

To set up your API key:

a. Per-user setup: input your API key in LibreChat when prompted (using the above methods)
b. Global setup: use `LIBRECHAT_CODE_API_KEY` environment variable in the .env file of your project (provides access to all users)

### Direct API Integration

The Code Interpreter API can be integrated into any application using a simple API key authentication:

1. Get your API key from [code.librechat.ai](https://code.librechat.ai/pricing)
2. Include the API key in your requests using the `x-api-key` header

## Core Functionality

### Code Execution

- Run code snippets in supported languages
- Receive stdout/stderr output
- Get execution statistics (memory usage, CPU time)
- Handle program arguments
- Access execution status and results

### File Operations

- Upload input files
- Download generated outputs
- List available files
- Delete unnecessary files
- Manage file sessions

### Limitations
- Code cannot access the network
- Only 10 files can be generated per run
- Execution limits vary by plan:
  - **Hobby**: 
    - 256 MB RAM per execution
    - 25 MB per file upload
    - 750 requests per month
  - **Enthusiast**: 
    - 512 MB RAM per execution
    - 50 MB per file upload
    - 3,000 requests per month
  - **Pro**: 
    - 512 MB RAM per execution
    - 150 MB per file upload
    - 7,000 requests per month
- The [Enterprise Plan](https://code.librechat.ai/pricing) provides custom limits and features

## Use Cases

- **Code Testing**: Test code snippets in multiple languages
- **File Processing**: Transform and analyze files programmatically
- **AI Applications**: Execute AI-generated code securely
- **Development Tools**: Build interactive coding environments
- **Objective Logic**: Verify code logic and correctness, improving AI models

## Why a Paid API?

While LibreChat remains free and open source under the MIT license, the Code Interpreter API is offered as a paid service for several key reasons:

1. **Project Sustainability**: Subscribing to an API plan provides direct support to the project's development, even more effectively than [GitHub Sponsors](https://github.com/sponsors/danny-avila). Your subscription helps ensure LibreChat's continued growth and improvement.

2. **Technical Considerations**: Including code execution capabilities in the core project would add significant complexity and hardware requirements that not all users need. The API service eliminates these concerns while maintaining a lightweight core application.

3. **Managed Service Benefits**:
   - No complex configuration
   - Immediate availability
   - Regular updates and maintenance
   - Professional support
   - Secure, sandboxed environment

4. **Intellectual Property Protection**: The Code Interpreter's architecture represents significant innovation in secure, scalable sandbox technology. While similar solutions exist, they often lack the comprehensive security measures and scalability features that make this implementation unique. Keeping this component as a closed-source API helps protect these innovations and ensures the service maintains its high security and performance standards.

Even if you only use code execution occasionally, subscribing helps support LibreChat's development while enhancing your experience with professional-grade features. It's a win-win that keeps the core project free while offering optional advanced capabilities for those who need them.

---

## Conclusion

The Code Interpreter API provides a secure, convenient way to execute code and manage files without the hassle of setting up and maintaining execution environments. Whether you're using it through LibreChat's Agents or integrating it directly into your applications, it offers a robust solution for code execution needs.

For detailed technical specifications and API reference, please visit our [API Documentation](https://code.librechat.ai/docs).

#LibreChat #CodeExecution #API #Development


================================================
FILE: pages/docs/features/fork.mdx
================================================
---
title: Forking Messages and Conversations
description: A guide on how to use the "Fork Messages/Conversations" feature in our app to create new conversations from specific messages with desired behavior.
---

import Image from 'next/image'

# Forking Messages and Conversations

<Image
src="https://github.com/danny-avila/LibreChat/assets/32828263/5187e8e2-b1c1-4954-979c-9c3b8cfabe9e"
width={1024}
height={512}
/>

## Why Fork Conversations?

Think of forking like creating a new path in your chat - it's super handy when you want to:

### Keep Things On Track
Sometimes you'll hit on an interesting side topic but don't want to derail your main conversation. Forking lets you dive into that rabbit hole while keeping your original chat clean

### Play "What If"
Want to see how different approaches might play out? Fork the chat and try various angles. It's like having multiple parallel conversations, each exploring a different possibility

### Save Important Context 
When you fork, you can bring along as much or as little of the previous chat as you need. This way, you're not starting completely from scratch, and the AI still has the background it needs

### Share Specific Parts
Need to show someone just part of your conversation? Instead of copying and pasting or sharing your whole chat history, you can fork just the relevant bit and share that

### Keep Ideas Organized
Long chats can spawn lots of different ideas. Rather than letting them get tangled together, you can fork each one into its own thread and develop them separately

### Try Different Approaches
Sometimes you might wonder if there's a better way to ask something. Forking lets you experiment with different ways of talking to the AI without messing up your main conversation

## Forking Options

Use these settings to fork messages with the desired behavior.

Forking refers to creating a new conversation that starts/ends from specific messages in the current conversation, creating a copy according to the options selected.

The "target message" refers to either the message the popup was opened from, or, if you check "Start fork here", the latest message in the conversation.


### Visible messages only: 
This option forks only the visible messages; in other words, the direct path to the target message, without any branches.

<div style={{padding: "20px", display: "flex", justifyContent: "center", alignItems: "center", flexDirection: "column"}}>
  <div className="image-light-theme">
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/873bdba1-de1f-4b84-a996-b2dbfc866d55" alt="1-L" style={{borderRadius: "10px"}} />
  </div>

  <div className="image-dark-theme">
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/0ed6ea88-5840-4dda-8f8b-305a4c34a050" alt="1-D" style={{borderRadius: "10px"}} />
  </div>
</div>


### Include related branches:
This option forks the visible messages, along with related branches; in other words, the direct path to the target message, including branches along the path.

<div style={{padding: "20px", display: "flex", justifyContent: "center", alignItems: "center", flexDirection: "column"}}>
  <div className="image-light-theme">
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/e633f701-acf5-4878-bdd1-29abacb3e3e7" alt="2-L" style={{borderRadius: "10px"}} />
  </div>

  <div className="image-dark-theme">
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/0c297451-990b-4ab2-9ff2-bc3958ab7129" alt="2-D" style={{borderRadius: "10px"}} />
  </div>
</div>

### Include all to/from here (default):
This option forks all messages leading up to the target message, including its neighbors; in other words, all message branches, whether or not they are visible or along the same path, are included.

<div style={{padding: "20px", display: "flex", justifyContent: "center", alignItems: "center", flexDirection: "column"}}>
  <div className="image-light-theme">
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/d19b427b-e018-41e6-ab1a-6306a94be26b" alt="3-L" style={{borderRadius: "10px"}} />
  </div>

  <div className="image-dark-theme">
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/ae3e5086-7b8f-417f-8b6e-073776536a49" alt="3-D" style={{borderRadius: "10px"}} />
  </div>
</div>

## Additional Options

- **Start fork here:** If checked, forking will commence from this message to the latest message in the conversation, according to the behavior selected above.

<div style={{padding: "20px", display: "flex", justifyContent: "center", alignItems: "center", flexDirection: "column"}}>
  <div className="image-light-theme">
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/801e50e4-749a-42f3-83bd-a3fc06c6e189" alt="4-L" style={{borderRadius: "10px"}} />
  </div>

  <div className="image-dark-theme">
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/bb2f2e39-091e-4b5b-926d-bf36c7a65079" alt="4-D" style={{borderRadius: "10px"}} />
  </div>
</div>

- **Remember:** Check to remember the options you select for future usage, making it quicker to fork conversations as preferred.

<div style={{padding: "20px", display: "flex", justifyContent: "center", alignItems: "center", flexDirection: "column"}}>
  <div className="image-light-theme">
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/9a9f61db-c3ec-4139-8f3a-e25557d95066" alt="5-L" style={{borderRadius: "10px"}} />
  </div>

  <div className="image-dark-theme">
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/a567965f-881e-423b-9eec-e3004643a560" alt="5-D" style={{borderRadius: "10px"}} />
  </div>
</div>

- Alternatively you can control the default behavior in the settings menu:

<div style={{padding: "20px", display: "flex", justifyContent: "center", alignItems: "center", flexDirection: "column"}}>
  <div className="image-light-theme">
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/c85fabb7-7d92-4e63-9190-ac3f4470b505" alt="6-L" />
  </div>

  <div className="image-dark-theme">
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/2ba9dfc0-8b21-4297-bf67-9bf6b2ff0aa2" alt="6-D" />
  </div>
</div>



================================================
FILE: pages/docs/features/import_convos.mdx
================================================
---
title: Import Conversations
description: Conversations Import allows user to import conversations exported from other GPT chat applications. Currently, we support importing conversations from ChatGPT, ChatbotUI v1, and LibreChat itself.  
---
# Import Conversations

Conversations Import allows user to import conversations exported from other GPT chat applications. Currently, we support importing conversations from [ChatGPT](https://chatgpt.com/), [ChatbotUI v1](https://github.com/mckaywrigley/chatbot-ui/tree/b865b0555f53957e96727bc0bbb369c9eaecd83b?tab=readme-ov-file#legacy-code), and LibreChat itself.

The Import functionality is available in the "Settings" -> "Data Controls" section.

## How to import conversations from Chat GPT

1. Follow the [ChatGPT export instructions](https://help.openai.com/en/articles/7260999-how-do-i-export-my-chatgpt-history-and-data) to export your conversations.
2. You should get a link to download archive in you email.
3. Download the archive. It should be a zip file with random name like: _d119d98bb3711aff7a2c73bcc7ea53d96c984650d8f7e033faef78386a9907-2024-01-01-10-30-00.zip_
4. Extract the content of the zip file.
5. Navigate to LibreChat Settings -> Data Controls
<div style={{padding: "20px", display: "flex", justifyContent: "center", alignItems: "center", flexDirection: "column"}}>
  <div className="image-light-theme">
    ![Image for Light Theme](https://github.com/danny-avila/LibreChat/assets/32828263/3c71b4f8-3d67-4293-ae89-fea4c59312d8)
  </div>

  <div className="image-dark-theme">
    ![Image for Dark Theme](https://github.com/danny-avila/LibreChat/assets/32828263/14151a19-94ca-45b3-a79e-134d961e474c)
  </div>
</div>
6. Click on the "Import" button and select `conversations.json` file from the extracted archive. It will start importing the conversations.
7. Shortly you will get a notification that the import is complete.
<div style={{padding: "20px", display: "flex", justifyContent: "center", alignItems: "center", flexDirection: "column"}}>
    ![import-success](https://github.com/danny-avila/LibreChat/assets/32828263/597eefec-0b8e-4fbf-ac27-2405472c195f)
</div>



================================================
FILE: pages/docs/features/index.mdx
================================================
---
title: Overview
description: 'Explore the features of LibreChat, including AI model selection, conversation branching, multimodal chat, and more.'
---

import LCLogo from '@/components/lcLogo'
import Carousel from '@/components/carousel/Carousel'
import Github from '@/components/icons/github'
import Discord from '@/components/icons/discord'
import { Mail } from 'lucide-react'
import Image from 'next/image'

<div style={{ marginBottom: '30px', marginTop: '10px' }}>
  <LCLogo />
</div>

# ✨ Features

### 🔧 [Code Interpreter API](/docs/features/code_interpreter)

- **Secure, Sandboxed Execution**: Run code in Python, Node.js (JS/TS), Go, C/C++, Java, PHP, Rust, and Fortran without local setup.
- **Seamless File Handling**: Upload and process files directly, download outputs, and manage file sessions effortlessly.
- **No Privacy Concerns**: The Code Interpreter is fully isolated, ensuring secure and private execution of your code.
- **[Learn More →](/docs/features/code_interpreter)**

### 🤝 [Agents](/docs/features/agents)

- **No-Code Custom Assistants**: Build specialized, AI-driven helpers that work with any supported model—no coding required.
- **Flexible & Extensible**: Attach tools like DALL-E-3, semantic search, and calculators, or integrate file management and code execution seamlessly.
- **Paradigm Shift for LibreChat**: Agents define a new era of customizable workflows and context-aware interactions, enabling you to shape LibreChat into the tool you need.
- **[Learn More →](/docs/features/agents)**

### 🪄 **[Artifacts](/docs/features/artifacts)**

- **Generative UI:** React, HTML, Mermaid diagrams
- Use any model you have setup, and iterate on created outputs

### 🖥️ **Intuitive UI**

- **Seamless User Experience**: Our platform mirrors the intuitive interface of ChatGPT, ensuring that users feel comfortable and familiar while navigating.
- **Dark Mode**: Reduce eye strain with our elegant dark mode, perfect for prolonged usage.
- **Streaming Capabilities**: Enjoy real-time streaming of AI responses for an interactive experience.
- **Latest Updates**: Stay ahead with the most recent features and improvements released regularly.

### 🤖 **AI Model Selection**

#### **Diverse Model Options**

- **[AWS Bedrock](/docs/configuration/pre_configured_ai/bedrock)**: Access a variety of models through Amazon's Bedrock service.
- **[Azure OpenAI](/docs/configuration/azure)**: Integrate with Microsoft's Azure for powerful cloud-based services.
- **[Anthropic (Claude)](/docs/configuration/pre_configured_ai/anthropic)**: Experience cutting-edge AI technology from Anthropic.
- **[OpenAI](/docs/configuration/pre_configured_ai/openai)**: Utilize the robust and renowned GPT models from OpenAI for a wide range of applications.
- **[Google](/docs/configuration/pre_configured_ai/google)**: Use Google's premier machine learning offerings.
- **[Plugins](/docs/configuration/tools)**: Extend functionalities with a variety of plugins.
- **[Assistants API](/docs/configuration/pre_configured_ai/assistants)**: Seamlessly connect with APIs, including Azure Assistants, for expanded functionalities.

### ✅ **Compatibility Across AI Services**

#### **Remote & Local Integration**

- **Groq**: Fast, efficient AI inference with custom processors
- **Deepseek**: Cutting-edge Open-source AI models
- **Ollama**: Generative AI models for interactive applications and chatbots.
- **Cohere**: Natural language processing APIs for business applications.
- **Mistral AI**: Open-source high-performance generative AI.
- **Apple MLX**: Apple's framework for machine learning and AI integration.
- **koboldcpp**: AI-assisted storytelling and content generation tools.
- **OpenRouter**: API gateway for routing AI model requests.
- **together.ai**: Collaborative platform for AI model development and deployment.
- **Perplexity**: AI-driven search engine for contextual answers.
- **ShuttleAI**: Automated machine learning platform for rapid deployment.
- **[Learn how to configure Custom endpoints like the ones above](/docs/quick_start/custom_endpoints)**

### 💾 **[Create, Save, & Share Custom Presets](/docs/user_guides/presets)**

- **Personalization**: Save your favorite AI and configuration settings as presets.
- **Easy Sharing**: Share presets with colleagues or teams.

### 🔀 **Switch Between AI Endpoints and Presets, Mid-Chat**

- **Dynamic Configuration**: Change AI models/providers and settings on-the-fly without disrupting the chat flow.

### 🔄 **Edit, Resubmit, and Continue Messages with Conversation Branching**

- **Message Flexibility**: Edit previous messages and resubmit for better responses.
- **Conversation Control**: Branch conversations to explore different discussion paths without losing context.

### 🌿 **[Fork Messages & Conversations](/docs/features/fork)**

- **Advanced Context Management**: Split messages to create multiple conversation threads, enhancing control of your conversations' context.

### 🥷 **[Temporary Chat](/docs/features/temporary_chat.mdx)**

- **Chat incognito in Librechat**: Use Temporary Chat for private conversations that won't clutter your history, appear in search, or be bookmarked – perfect for sensitive topics or quick experiments.

### 💬 **Multimodal Chat**

#### **Image Analysis**

- 📸 **Claude 3, GPT-4, Gemini Vision, Llava and Assistants**: Upload and analyze images seamlessly with these advanced models.

#### **[File Interaction](/docs/features/rag_api)**

- 🗃️ **Custom Endpoints, OpenAI, Azure, Anthropic, Google**: Chat with files using various powerful endpoints.

#### **OpenAI Assistants API**

- **Capabilities**:
  - File handling
  - Code interpretation
  - Tool integration
  - API actions 🔦

### 🌎 **Multilingual UI**

- **Broad Language Support**: Switch between multiple languages including:
  - **English**
  - **中文**
  - **Deutsch**
  - **Español**
  - **Français**
  - **Italiano**
  - **Polski**
  - **Português Brasileiro**
  - **Русский**
  - **日本語**
  - **Svenska**
  - **한국어**
  - **Tiếng Việt**
  - **繁體中文**
  - **العربية**
  - **Türkçe**
  - **Nederlands**
  - **עברית**
  - **and more...**

### 🎨 **Customizable Dropdown & Interface**

- **User Adaptability**: Interface designed to cater both to power users and newcomers, ensuring a tailored user experience.

### 📧 **[Secure Email Verification](/docs/configuration/authentication/email)**

- **Access Security**: Verify your email to secure your account and ensure dependable access.

### 🗣️ **[Hands-Free Chat](/docs/configuration/stt_tts)**
#### Speech-to-Text and Text-to-Speech

- **[Voice Interaction](/docs/configuration/stt_tts#stt)**: Utilize voice commands and responses to interact with the AI.
- **[Audio Support](/docs/configuration/stt_tts#tts)**: Automatically send and receive audio messages.
- **Vendor Support**:
  - **OpenAI**
  - **Azure OpenAI**
  - **Elevenlabs**
  - **Local AI**

### 📥 **[Conversation Import](/docs/features/import_convos)**

- **Compatibility**: Import existing conversations from platforms like:
  - **LibreChat**
  - **[ChatGPT](/docs/features/import_convos#how-to-import-conversations-from-chat-gpt)**
  - **Chatbot UI**

### 📤 **Conversation Export**

- **Multi-format Saving**: Export chats as:
  - **Screenshots**
  - **Markdown**
  - **Text**
  - **JSON**

### 🔍 **[Search Functionality](/docs/configuration/meilisearch)**

- **Efficient Navigation**: Quickly find specific messages or entire conversations with advanced search features using **Meilisearch**.

### 🔌 **[Plugins and Extensions](/docs/configuration/tools)**

- **Enhanced Functionality**:
  - Web access
  - Image generation with **DALL-E-3**, **DALL-E-2**, and **Stable Diffusion**
  - And more

### 👥 **[Multi-User Secure Authentication](/docs/configuration/authentication)**

- **User Management**: Secure and manage multiple users with authentication controls and token spend tools.

### ⚙️ **Comprehensive Configuration Options**

#### 🚀 **Deployment Flexibility**

- **[Local Deployment](/docs/local)**: Run your AI services entirely offline.
- **[Cloud Deployment](/docs/remote)**: Utilize the cloud for broader access and scalability.
- **[Proxy & Reverse Proxy Configuration](/docs/remote/nginx)**: Optimize routing and security.
- **[Docker Support](/docs/local/docker)**: Quick and consistent deployment with Docker.
- **[Firebase CDN](/docs/configuration/firebase)**: Fast and reliable content delivery via Firebase CDN.
- **[Logging System](/docs/configuration/logging)**: Integrated logging to monitor system performance and activity.

### 🔒 **Authentication and Security**

#### 🔑 **Flexible Authentication Options**

- **[Token Usage](/docs/configuration/token_usage)**: Efficiently manage and allocate usage tokens.
- **[LDAP/AD Authentication](/docs/configuration/authentication/ldap)**: Secure integration with LDAP/Active Directory for enterprise-level authentication.
- **[OAuth2-OIDC Authentication](/docs/configuration/authentication/OAuth2-OIDC)**: Compatibility with OAuth2 and OIDC for modern authentication protocols.
- **[Email verification & Password Reset](/docs/configuration/authentication/email)**: Easy email verification and password reset functionality to ensure account security.

### 📖 **Open-Source Development**

- **Transparency and Collaboration**: Enjoy the benefits of an open-source platform, built with public contributions.

### 🧑‍🤝‍🧑 **Community-Driven Development**

- **Support & Feedback**: Engage with a vibrant community for development support and continuous improvement based on user feedback.

### **Customization**

- **A Lot of Customization Options**: Tailor the platform to your specific needs with extensive customization capabilities, ensuring a personalized user experience.



================================================
FILE: pages/docs/features/mod_system.mdx
================================================
---
title: Automated Moderation
description: The Automated Moderation System uses a scoring mechanism to track user violations. As users commit actions like excessive logins, registrations, or messaging, they accumulate violation scores. Upon reaching a set threshold, the user and their IP are temporarily banned. This system ensures platform security by monitoring and penalizing rapid or suspicious activities.
---

## Automated Moderation System (optional)
The Automated Moderation System uses a scoring mechanism to track user violations. As users commit actions like excessive logins, registrations, or messaging, they accumulate violation scores. Upon reaching a set threshold, the user and their IP are temporarily banned. This system ensures platform security by monitoring and penalizing rapid or suspicious activities.

In production, you should have Cloudflare or some other DDoS protection in place to really protect the server from excessive requests, but these changes will largely protect you from the single or several bad actors targeting your deployed instance for proxying.

### Notes

- Uses Caching for basic security and violation logging (bans, concurrent messages, exceeding rate limits)
    - In the near future, I will add **Redis** support for production instances, which can be easily injected into the current caching setup
- Exceeding any of the rate limiters (login/registration/messaging) is considered a violation, default score is 1
- Non-browser origin is a violation
- Default score for each violation is configurable
- Enabling any of the limiters and/or bans enables caching/logging
- Violation logs can be found in the data folder, which is created when logging begins: `librechat/data`
  - **Only violations are logged**
  - `violations.json` keeps track of the total count for each violation per user
  - `logs.json` records each individual violation per user
- Ban logs are stored in MongoDB under the `logs` collection. They are transient as they only exist for the ban duration
    - If you would like to remove a ban manually, you would have to remove them from the database manually and restart the server
    - **Redis** support is also planned for this.

### Rate Limiters

The project's current rate limiters are as follows (see below under setup for default values):

- Login and registration rate limiting
- `Optional:{:hack}` Concurrent Message limiting (only X messages at a time per user)
- `Optional:{:hack}` Message limiting (how often a user can send a message, configurable by IP and User)
- `Optional:{:hack}` File Upload limiting (configurable through [`librechat.yaml` config file](/docs/configuration/librechat_yaml/object_structure/config#ratelimits))

**For further details, refer to the configuration guide provided here: [Automated Moderation](/docs/configuration/mod_system)**


================================================
FILE: pages/docs/features/ocr.mdx
================================================
---
title: File Context (OCR)
description: Learn how to use LibreChat's OCR capability to extract text from images and documents for AI processing.
---

# File Context via Optical Character Recognition (OCR)

LibreChat's OCR (Optical Character Recognition) feature enables AI agents to extract and process text from images and documents. This capability enhances the AI's ability to work with visual content, making it possible to analyze, understand, and respond to information contained in images.

## Overview

OCR functionality in LibreChat allows agents to:

- Extract text from images and documents
- Maintain document structure and formatting
- Process complex layouts including multi-column text
- Handle tables, equations, and other specialized content
- Work with multilingual content

## Availability

Currently, OCR is **only available as an agent capability**. This means you must use an agent via the Agents endpoint to leverage OCR functionality.

## Configuration

OCR can be enabled in the LibreChat configuration file (`librechat.yaml`). The OCR configuration supports two strategies:

1. **Mistral OCR** (Default and currently the only available option)
2. **Custom OCR** (Planned for future releases)

### Basic Configuration Example

If using the Mistral OCR API, you only need the following environment variables to get started:

```.env
# `.env`
OCR_API_KEY=your-mistral-api-key
# OCR_BASEURL=https://api.mistral.ai/v1 # this is the default value
```

For additional, detailed configuration options, see the [OCR Config Object Structure](/docs/configuration/librechat_yaml/object_structure/ocr).

```yaml
# `librechat.yaml`
ocr:
  mistralModel: "mistral-ocr-latest"  # Optional: Specify Mistral model, defaults to "mistral-ocr-latest"
  apiKey: "your-mistral-api-key"        # Optional: Defaults to OCR_API_KEY env variable
  baseURL: "https://api.mistral.ai/v1"  # Optional: Defaults to OCR_BASEURL env variable, or Mistral's API if no variable set
  strategy: "mistral_ocr"               # Optional: Defaults to "mistral_ocr" (only option currently available)
```

## Mistral OCR

Currently, LibreChat uses Mistral's OCR API as the default and only available OCR provider. Mistral OCR offers state-of-the-art document understanding capabilities.

### Key Features of Mistral OCR

- **Document Structure Preservation**: Maintains formatting like headers, paragraphs, lists, and tables
- **Multilingual Support**: Processes text in multiple languages and scripts
- **Complex Layout Handling**: Handles multi-column text and mixed content
- **Mathematical Expression Recognition**: Accurately processes equations and formulas
- **High-Speed Processing**: Processes up to 2000 pages per minute

### Important Considerations

- **Cost**: Using Mistral OCR may incur costs as it's a paid API service (though free trials may be available)
- **Data Privacy**: Data processed through Mistral OCR is subject to Mistral's cloud environment and their terms of service
- **Document Limitations**: 
  - Maximum file size: 50 MB
  - Maximum document length: 1,000 pages

### Future Plans

- Mistral plans to make their OCR API available through their cloud partners, such as GCP and AWS, and enterprise self-hosting for organizations with stringent data privacy requirements ([source](https://mistral.ai/fr/news/mistral-ocr)).
- LibreChat will continue to support Mistral OCR and explore additional OCR providers, including open-source solutions, for enhanced functionality.
- LibreChat currently does not include the parsed image content from the OCR process in its responses, even though services like [Mistral's OCR API may provide](https://docs.mistral.ai/api/#tag/ocr) these in the result. This feature may be supported in future updates.

## Using File Context (OCR) in LibreChat

LibreChat provides two main ways to use OCR functionality:

### 1. Upload as Text in Chat

In any chat conversation, you can use OCR to extract text from images or documents:

1. Click the attachment icon in the chat input
2. Select "Upload as Text" from the menu
3. Choose an image or document file
4. The OCR system will process the file and insert the extracted text into your message

![Upload as Text option in the attachment menu](/images/ocr/upload_as_text.png)

### 2. File Context for Agents

When working with agents, you can add documents as context using OCR:

1. Open the Agent Builder panel or edit an existing agent
2. In the File Context section, click "Upload File Context"
3. Select a document or image file
4. The OCR system will extract text from the file and add it to the agent's instructions

![File Context using OCR for agents](/images/ocr/file_context_ocr.png)

Files uploaded as "Context" are processed using OCR to extract text, which is then added to the Agent's instructions. This is ideal for documents, images with text, or PDFs where you need the full text content of a file to be available to the agent.

**Note,** the OCR is performed at the time of upload and is not stored as a separate file, rather purely as text in the database.

## Example Use Cases

- **Document Analysis**: Extract and analyze text from scanned documents, PDFs, or images
- **Data Extraction**: Pull specific information from forms, receipts, or invoices
- **Research Assistance**: Process academic papers, articles, or books
- **Language Translation**: Extract text from foreign language documents for translation
- **Content Digitization**: Convert printed materials into digital, searchable text

## Limitations

- OCR accuracy may vary depending on image quality, document complexity, and text clarity
- Some specialized formatting or unusual layouts might not be perfectly preserved
- Very large documents may be truncated due to token limitations of the underlying AI models

## Future Enhancements

LibreChat plans to expand OCR capabilities in future releases:

- Support for custom OCR providers
- A `user_provided` strategy option that will allow users to choose their preferred OCR service
- Integration with open-source OCR solutions
- Enhanced document processing options
- More granular control over OCR settings

---

For more information on configuring OCR, see the [OCR Config Object Structure](/docs/configuration/librechat_yaml/object_structure/ocr).



================================================
FILE: pages/docs/features/password_reset.mdx
================================================
---
title: Password Reset
description: This feature enables email-based password reset functionality for your LibreChat server.
---

# Password Reset

<div
  style={{
    padding: '20px',
    display: 'flex',
    justifyContent: 'center',
    alignItems: 'center',
    flexDirection: 'column',
  }}
>
  <img
    src="https://github.com/danny-avila/LibreChat/assets/32828263/498c588c-6e50-4aed-9815-5e06a5409966"
    alt="password reset"
    style={{ borderRadius: '10px' }}
  />
</div>

## Overview

This feature enables email-based password reset functionality for your LibreChat server. You can configure it to work with various email services, including Gmail and custom mail servers.

## Key Features

- Supports multiple email services, including Gmail and custom mail servers
- Allows for basic and advanced configuration options
- Enables email-based password reset functionality for your LibreChat server

## Setup Options

- Basic Configuration: Use predefined services with minimal configuration
- Advanced Configuration: Configure generic SMTP services or customize settings for predefined providers

**For further details, refer to the configuration guides provided here: [Password Reset](/docs/configuration/authentication/email)**



================================================
FILE: pages/docs/features/plugins.mdx
================================================
---
title: Plugins
description: This doc introduces the plugins endpoint, which enables you to use different LLMs and tools with more flexibility and control. You can change your settings and plugins on the fly, and use plugins to access various sources of information and assistance.
---
# Plugins (Deprecated)

**Note: This feature is deprecated in favor of [Agents](/docs/features/agents) and will be removed in the future.**

![intro-1](https://github.com/danny-avila/LibreChat/assets/32828263/7db788a5-2173-4115-b34b-43ea132dae69)


The plugins endpoint opens the door to prompting LLMs in new ways other than traditional input/output prompting.

The first step is using chain-of-thought prompting & **["agency"](https://zapier.com/blog/ai-agent/)** for using plugins/tools in a fashion mimicing the official ChatGPT Plugins feature.

More than this, you can use this endpoint for changing your conversation settings mid-conversation. Unlike the official ChatGPT site and all other endpoints, you can switch models, presets, and settings mid-convo, even when you have no plugins selected. This is useful if you first want a creative response from GPT-4, and then a deterministic, lower cost response from GPT-3. Soon, you will be able to use Google, HuggingFace, local models, all in this or a similar endpoint in the same modular manner.

## Using Plugins 

The LLM process when using Plugins is illustrated below.

![intro-2](https://github.com/danny-avila/LibreChat/assets/32828263/789406e1-7345-43d2-823b-8aed0588bb78)

**When you open the settings with the Plugins endpoint selected, you will view the default settings for the Completion Phase.**

Clicking on **"Show Agent Settings"** will allow you to modify parameters for the thinking phase

![intro-3](https://github.com/danny-avila/LibreChat/assets/32828263/d9a43517-5b35-4786-a126-0adf62b5b087)

---

![intro-4](https://github.com/danny-avila/LibreChat/assets/32828263/12a51feb-c030-4cf0-8429-16360270988d)

- You can specify which plugins you would like to select from by installing/uninstalling them in the Plugin store

## Notes
- Every additional plugin selected will increase your token usage as there are detailed instructions the LLM needs for each one
- For best use, be selective with plugins per message and narrow your requests as much as possible
- If you need help coming up with a good plugin prompt, ask the LLM for suggestions before using one!
- Chain-of-thought prompting (plugin use) will always be more expensive than regular input/output prompting, so be sure it meets your need.
- Currently, the cheapest use will be to use gpt-3.5 for both phases
- From my testing, the best "bang for your buck" will be to use gpt-3.5 for the thinking phase, and gpt-4 for completion.
- Adding to above, if you ask for a poem and an image at the same time, it may work, but both may suffer in quality
  - Instead, ask for a poem first with creative settings
  - Then, ask for a good prompt for Stable Diffusion based on the poem
  - Finally, use the Stable Diffusion plugin by referencing the pre-generated prompt
- Presets are only available when no Plugins are selected as the final review of the thinking phase has a specific system message.

## Showcase

![introduction-5](https://github.com/danny-avila/LibreChat/assets/32828263/40cd1989-437f-49bb-9055-010e3efc468b)

![introduction-6](https://github.com/danny-avila/LibreChat/assets/32828263/b009a094-7311-45fb-a7ea-f5010f32ec45)

## Configuration

See our Tools and Plugins configuration guides for more info: [Tools and Plugins Configuration](/docs/configuration/tools)



================================================
FILE: pages/docs/features/rag_api.mdx
================================================
---
title: RAG API (Chat with Files)
description: Retrieval-Augmented Generation (RAG) API for document indexing and retrieval using Langchain and FastAPI. This API integrates with LibreChat to provide context-aware responses based on user-uploaded files.
---

# RAG API

The **RAG (Retrieval-Augmented Generation) API** is a powerful tool that integrates with LibreChat to provide context-aware responses based on user-uploaded files.

It leverages LangChain, PostgresQL + PGVector, and Python FastAPI to index and retrieve relevant documents, enhancing the conversational experience.

**For further details, refer to the configuration guide provided here: [RAG API Configuration](/docs/configuration/rag_api)**

![image](https://github.com/danny-avila/LibreChat/assets/110412045/f1298f66-bf1d-4499-a582-23430b481f17)

---

**Currently, this feature is available to all Custom Endpoints, OpenAI, Azure OpenAi, Anthropic, and Google.**

OpenAI Assistants have their own implementation of RAG through the "Retrieval" capability. Learn more about it [here.](https://platform.openai.com/docs/assistants/tools/knowledge-retrieval) 

It will still be useful to implement usage of the RAG API with the Assistants API since OpenAI charges for both file storage, and use of "Retrieval," and will be introduced in a future update.

Plugins support is not enabled as the whole "plugin/tool" framework will get a complete rework soon, making tools available to most endpoints (ETA Summer 2024).

**Still confused about RAG?** [Read the section I wrote below](#what-is-rag) explaining the general concept in more detail with a link to a helpful video.

## What is RAG?

RAG, or Retrieval-Augmented Generation, is an AI framework designed to improve the quality and accuracy of responses generated by large language models (LLMs). It achieves this by grounding the LLM on external sources of knowledge, supplementing the model's internal representation of information.


## Features

- **Document Indexing**: The RAG API indexes user-uploaded files, creating embeddings for efficient retrieval.
- **Semantic Search**: It performs semantic search over the indexed documents to find the most relevant information based on the user's input.
- **Context-Aware Responses**: By augmenting the user's prompt with retrieved information, the API enables LibreChat to generate more accurate and contextually relevant responses.
- **Asynchronous Processing**: The API supports asynchronous operations for improved performance and scalability.
- **Flexible Configuration**: It allows customization of various parameters such as chunk size, overlap, and embedding models.

### Key Benefits of RAG

1. **Access to up-to-date and reliable facts**: RAG ensures that the LLM has access to the most current and reliable information by retrieving relevant facts from an external knowledge base.
2. **Transparency and trust**: Users can access the model's sources, allowing them to verify the accuracy of the generated responses and build trust in the system.
3. **Reduced data leakage and hallucinations**: By grounding the LLM on a set of external, verifiable facts, RAG reduces the chances of the model leaking sensitive data or generating incorrect or misleading information.
4. **Lower computational and financial costs**: RAG reduces the need for continuous training and updating of the model's parameters, potentially lowering the computational and financial costs of running LLM-powered chatbots in an enterprise setting.

### How RAG Works

RAG consists of two main phases: retrieval and content generation.

1. **Retrieval Phase**: Algorithms search for and retrieve snippets of information relevant to the user's prompt or question from an external knowledge base. In an open-domain, consumer setting, these facts can come from indexed documents on the internet. In a closed-domain, enterprise setting, a narrower set of sources are typically used for added security and reliability.
2. **Generative Phase**: The retrieved external knowledge is appended to the user's prompt and passed to the LLM. The LLM then draws from the augmented prompt and its internal representation of its training data to synthesize a tailored, engaging answer for the user. The answer can be passed to a chatbot with links to its sources.

### Challenges and Ongoing Research

While RAG is currently one of the best-known tools for grounding LLMs on the latest, verifiable information and lowering the costs of constant retraining and updating, it's not perfect. Some challenges include:

1. **Recognizing unanswerable questions**: LLMs need to be explicitly trained to recognize questions they can't answer based on the available information. This may require fine-tuning on thousands of examples of answerable and unanswerable questions.
2. **Improving retrieval and generation**: Ongoing research focuses on innovating at both ends of the RAG process: improving the retrieval of the most relevant information possible to feed the LLM, and optimizing the structure of that information to obtain the richest responses from the LLM.

In summary, RAG is a powerful framework that enhances the capabilities of LLMs by grounding them on external, verifiable knowledge. It helps to ensure more accurate, up-to-date, and trustworthy responses while reducing the costs associated with continuous model retraining. As research in this area progresses, we can expect further improvements in the quality and efficiency of LLM-powered conversational AI systems.

For a more detailed explanation of RAG, you can watch this informative video by IBM on Youtube:

<iframe width="560" height="315" src="https://www.youtube.com/embed/T-D1OfcDW1M?si=SDVz1Fxsoi_z4S89" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>


## Conclusion

The RAG API is a powerful addition to LibreChat, enabling context-aware responses based on user-uploaded files. By leveraging Langchain and FastAPI, it provides efficient document indexing, retrieval, and generation capabilities. With its flexible configuration options and seamless integration, the RAG API enhances the conversational experience in LibreChat.

For more detailed information on the RAG API, including API endpoints, request/response formats, and advanced configuration, please refer to the official RAG API documentation.



================================================
FILE: pages/docs/features/search.mdx
================================================
---
title: Search
description: Quickly search in past conversations with LibreChat's integrated Meilisearch
---

LibreChat has integrated **Meilisearch** to enhance the user experience by providing a fast and efficient way to search through past conversations. Meilisearch is a powerful, open-source search engine that is known for its speed and ease of use, making it an excellent choice for applications like LibreChat that require quick access to a large volume of data.

<div style={{padding: "20px", display: "flex", justifyContent: "center", alignItems: "center", flexDirection: "column"}}>
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/60ad41b0-1869-4ee9-848b-502b3a5557b5" alt="search for a banana" style={{borderRadius: "10px"}} />
</div>


The integration allows users to:
- Perform **full-text searches** on their conversation history.
- Utilize **typo-tolerance** for more forgiving search queries.
- Experience **instant results** as they type, thanks to Meilisearch's as-you-type search capabilities.

This feature significantly improves the functionality of LibreChat, making it easier for users to find specific messages or topics within their chat history. It's a testament to the platform's commitment to providing a seamless and user-friendly experience. For those interested in the technical details or implementation, further information can be found in the [Meilisearch Configuration Guide](/docs/configuration/meilisearch).



================================================
FILE: pages/docs/features/temporary_chat.mdx
================================================
---
title: Temporary Chat
description: Temporary chats allow users to exclude specific conversations from their chat history, search results, and bookmarks, providing a private and focused experience.
---

# Temporary Chat

Keep your chat history clean and focused by using temporary chats for sensitive topics, quick experiments, or anything you don't need to permanently save. These chats are excluded from search results, cannot be bookmarked, and are automatically deleted after 30 days.

## Activating Temporary Chat

1.  Open the model list dropdown menu located on the top left side of LibreChat.
2.  Toggle the "Temporary Chat" slider to the ON position.

![Temporary Chat Toggle](https://github.com/user-attachments/assets/ebd5370d-7fac-45af-b0d3-2de59df506e9)

3.  A notification will appear above the chat input area, confirming that Temporary Chat mode is active. Pressing (x) will disable the Temporary Chat.

![Temporary Chat Notification](https://github.com/user-attachments/assets/ed19485a-99d3-45a2-a148-7824031f9022)


## What Happens When a Chat is Marked as Temporary?

*   Temporary Chats do not appear in the chat history sidebar.
*   Temporary Chats are excluded from search results.
*   Temporary Chats cannot be bookmarked.
*   Temporary Chats are stored in the database for 30 days and then automatically deleted.



================================================
FILE: pages/docs/features/url_query.mdx
================================================
---
title: Query Parameters - Dynamic Chat Configuration via URL
description: Learn how to configure chat conversations using URL query parameters in LibreChat. Set models, endpoints, and conversation settings dynamically.
---

# URL Query Parameters

LibreChat supports dynamic configuration of chat conversations through URL query parameters. This feature allows you to initiate conversations with specific settings, models, and endpoints directly from the URL.

### Chat Paths

Query parameters must follow a valid chat path:
- For new conversations: `/c/new?`
- For existing conversations: `/c/[conversation-id]?` (where conversation-id is an existing one)

Examples:

```bash
https://your-domain.com/c/new?endpoint=ollama&model=llama3%3Alatest
https://your-domain.com/c/03debefd-6a50-438a-904d-1a806f82aad4?endpoint=openAI&model=o1-mini
```

## Basic Usage

The most common parameters to use are `endpoint` and `model`. Using both is recommended for the most predictable behavior:

```bash
https://your-domain.com/c/new?endpoint=azureOpenAI&model=o1-mini
```

### URL Encoding

Special characters in query params must be properly URL-encoded to work correctly. Common characters that need encoding:

- `:` → `%3A`
- `/` → `%2F`
- `?` → `%3F`
- `#` → `%23`
- `&` → `%26`
- `=` → `%3D`
- `+` → `%2B`
- Space → `%20` (or `+`)

Example with special characters:
```ts
Original: `Write a function: def hello()`
Encoded: `/c/new?prompt=Write%20a%20function%3A%20def%20hello()`
```

You can use JavaScript's built-in `encodeURIComponent()` function to properly encode prompts:
```javascript
const prompt = "Write a function: def hello()";
const encodedPrompt = encodeURIComponent(prompt);
const url = `/c/new?prompt=${encodedPrompt}`;
console.log(url);
```

Try running the code in your browser console to see the encoded URL (browser shortcut: `Ctrl+Shift+I`).

### Endpoint Selection

The `endpoint` parameter can be used alone:
```bash
https://your-domain.com/c/new?endpoint=google
```

When only `endpoint` is specified:
- It will use the last selected model from localStorage
- If no previous model exists, it will use the first available model in the endpoint's model list

#### Notes

- The `endpoint` value must be one of the following:
```bash
openAI, azureOpenAI, google, anthropic, assistants, azureAssistants, bedrock, agents
```
- If using a [custom endpoint](/docs/quick_start/custom_endpoints), you can use its name as the value (case-insensitive)

```bash
# using `endpoint=perplexity` for a custom endpoint named `Perplexity`
https://your-domain.com/c/new?endpoint=perplexity&model=llama-3.1-sonar-small-128k-online
```

### Model Selection

The `model` parameter can be used alone:
```bash
https://your-domain.com/c/new?model=gpt-4o
```

When only `model` is specified:
- It will only select the model if it's available in the current endpoint
- The current endpoint is either the default endpoint or the last selected endpoint

### Prompt Parameter

The `prompt` parameter allows you to pre-populate the chat input field:
```bash
https://your-domain.com/c/new?prompt=Explain quantum computing
```

You can also use `q` as a shorthand, which is interchangeable with `prompt`:
```bash
https://your-domain.com/c/new?q=Explain quantum computing
```

You can combine these with other parameters:
```bash
https://your-domain.com/c/new?endpoint=anthropic&model=claude-3-5-sonnet-20241022&prompt=Explain quantum computing
```

### Automatic Prompt Submission

The `submit` parameter allows you to automatically submit the prompt without manual intervention:
```bash
https://your-domain.com/c/new?prompt=Explain quantum computing&submit=true
```

This feature is particularly useful for:
- Creating automated workflows (e.g., Raycast, Alfred, Automater)
- Building external integrations

You can combine it with other parameters for complete automation:
```bash
https://your-domain.com/c/new?endpoint=openAI&model=gpt-4&prompt=Explain quantum computing&submit=true
```

### Special Endpoints

#### Agents
You can directly load an agent using its ID without specifying the endpoint:
```bash
https://your-domain.com/c/new?agent_id=your-agent-id
```
This will automatically set the endpoint to `agents`.

#### Assistants
Similarly, you can load an assistant directly:
```bash
https://your-domain.com/c/new?assistant_id=your-assistant-id
```
This will automatically set the endpoint to `assistants`.

## Supported Parameters

LibreChat supports a wide range of parameters for fine-tuning your conversation settings:

### LibreChat Settings
- `maxContextTokens`: Override the system-defined context window
- `resendFiles`: Control file resubmission in subsequent messages
- `promptPrefix`: Set custom instructions/system message
- `imageDetail`: 'low', 'auto', or 'high' for image quality
  - Note: while this is a LibreChat-specific parameter, it only affects the following endpoints:
  - OpenAI, Custom Endpoints, which are OpenAI-like, and Azure OpenAI, for which this defaults to 'auto'
- `spec`: Select a specific LibreChat [Model Spec](/docs/configuration/librechat_yaml/object_structure/model_specs) by name.
  - Note: other parameters will not take effect, only those defined by the model spec.

### Model Parameters
Different endpoints support various parameters:

**OpenAI, Custom, Azure OpenAI:**
```bash
# Note: these should be valid numbers according to the provider's API
temperature, presence_penalty, frequency_penalty, stop, top_p, max_tokens
```

**Google, Anthropic:**
```bash
# Note: these should be valid numbers according to the provider's API
topP, topK, maxOutputTokens
```

**Anthropic Specific:**

Set this to `true` or `false` to toggle the "prompt-caching":
```bash
promptCache
```

More info: https://www.anthropic.com/news/prompt-caching

**Bedrock:**
```bash
# Bedrock region
region=us-west-2
# Bedrock equivalent of `max_tokens`
maxTokens=200
```

**Assistants/Azure Assistants:**
```bash
# overrides existing assistant instructions for current run
instructions=your+instructions
```
```bash
# Adds the current date and time to `additional_instructions` for each run.
append_current_datetime=true
```

## More Info

For more information on any of the above, refer to [Model Spec Preset Fields](/docs/configuration/librechat_yaml/object_structure/model_specs), which shares most parameters.

**Example with multiple parameters:**
```bash
https://your-domain.com/c/new?endpoint=google&model=gemini-2.0-flash-exp&temperature=0.7&prompt=Oh hi mark
```

## ⚠️ Warning

Exercise caution when using query parameters:
- Misuse or exceeding provider limits may result in API errors
- If you encounter bad request errors, reset the conversation by clicking "New Chat"
- Some parameters may have no effect if they're not supported by the selected endpoint

## Best Practices

1. Always use both `endpoint` and `model` when possible
2. Verify parameter support for your chosen endpoint
3. Use reasonable values within provider limits
4. Test your parameter combinations before sharing URLs

## Parameter Validation

All parameters are validated against LibreChat's schema before being applied. Invalid parameters or values will be ignored, and valid settings will be applied to the conversation.

---

This feature enables powerful use cases like:
- Sharing specific conversation configurations
- Creating bookmarks for different chat settings
- Automating chat setup through URL parameters

---

#LibreChat #ChatConfiguration #AIParameters #OpenSource #URLQueryParameters



================================================
FILE: pages/docs/local/_meta.ts
================================================
export default {
  index: 'Intro',
  docker: '🐋 Docker',
  npm: '📦 npm',
  helm_chart: '🪖 Helm Chart',
}



================================================
FILE: pages/docs/local/docker.mdx
================================================
---
title: 🐋 Docker
description: How to install LibreChat locally with Docker
---

# Local Installation of LibreChat with Docker

For most scenarios, Docker Compose is the recommended installation method due to its simplicity, ease of use, and reliability.

## Prerequisites

- [`Git`](https://git-scm.com/downloads)
- [`Docker`](https://www.docker.com/products/docker-desktop/)

---

## Installation Steps

Follow these steps to set up LibreChat with the default configuration:

```bash filename="Clone the Repository"
git clone https://github.com/danny-avila/LibreChat.git
```

```bash filename="Navigate to the LibreChat Directory"
cd LibreChat
```

```bash filename="Create a .env File from .env.example"
cp .env.example .env
```

> **Note:** **If you're using Windows, you might need to use `copy` instead of `cp`.**

```sh filename="Start LibreChat"
docker compose up -d
```

<Callout type="success" title="Access LibreChat!" emoji="🎉">
  **Visit [http://localhost:3080/](http://localhost:3080/)**
</Callout>

## Update LibreChat

The following commands will fetch the latest LibreChat project changes, including any necessary changes to the docker compose files, as well as the latest prebuilt images.

> **Note:** you may need to prefix commands with sudo according to your environment permissions.

```bash filename="Stop the running container(s)""
docker compose down
```

```bash filename="Pull latest project changes"
git pull
```

```bash filename="Pull the latest LibreChat image""
docker compose pull
```

```bash filename="Start LibreChat"
docker compose up
```

## Additional Setup

Unlock additional features by exploring our configuration guides to learn how to set up:

- Custom endpoints
- Other advanced configuration options
- And more

This will enable you to customize your LibreChat experience with optional features.

**see also:**
- [Docker Override](/docs/configuration/docker_override)
- [User Authentication System Setup](/docs/configuration/authentication)
- [AI Setup](/docs/configuration/pre_configured_ai)
- [Custom Endpoints & Configuration](/docs/configuration/librechat_yaml)
- [Manage Your MongoDB Database](/blog/2023-11-30_mongoexpress)


================================================
FILE: pages/docs/local/helm_chart.mdx
================================================
---
title: 🪖 Helm Chart
description: Instructions for deploying LibreChat on Kubernetes using Helm
---

# Deployment a LibreChat Helm Chart

The following instructions guide you to deploy LibreChat on Kubernetes using Helm. At the moment, this installation method only provides running the LibreChat components, without any additional services like MongoDB or Redis. You will need to provide your own MongoDB and Redis instances.

Note: this method was contributed by the community. If you are familiar with Helm and Kubernetes, use this guide as a reference and adjust to your needs. You can also reference other helm charts made by the community below in the [Community Helm Charts](#community-helm-charts) section.

## Prerequisites

* A running Kubernetes cluster
* `kubectl` installed
* Having a MongoDB instance running that can be accessed from the Kubernetes cluster
* Helm installed on your local machine

## Configuration
Similar to other Helm charts, there exists a [values file](https://github.com/danny-avila/LibreChat/blob/main/helmchart/values.yaml) that serves two primary functions: it outlines the default settings and indicates which configurations are adjustable.

Essentially, any setting within this values file can be modified in two main ways:

- The first method involves creating a separate override file and specifying it when executing the install command.
- The second method involves directly setting each variable with the installation command itself. If you're planning to change numerous variables, it's advisable to use the override file approach to avoid an overly lengthy command. Conversely, for fewer adjustments, directly setting variables with the installation command might be more convenient.


The very end of the file sets some of [environment variables](/docs/configuration/dotenv) of the application, that should look familiar if you deployed the application before. It is the base configuration without any sensitive data. 

```
  env:
    # Full list of possible values
    # https://github.com/danny-avila/LibreChat/blob/main/.env.example
    ALLOW_EMAIL_LOGIN: "true"
    ALLOW_REGISTRATION: "true"
    ALLOW_SOCIAL_LOGIN: "false"
    ALLOW_SOCIAL_REGISTRATION: "false"
    APP_TITLE: "Librechat"
    CUSTOM_FOOTER: "Provided with ❤️"
    DEBUG_CONSOLE: "true"
    DEBUG_LOGGING: "true"
    DEBUG_OPENAI: "true"
    DEBUG_PLUGINS: "true"
    DOMAIN_CLIENT: ""
    DOMAIN_SERVER: ""
    ENDPOINTS: "openAI,azureOpenAI,bingAI,chatGPTBrowser,google,gptPlugins,anthropic"
    SEARCH: false 
```

However, like the comment says, you could have a look at which environment variables are generally available to be modified.

Because with only these variables set the application won't start correctly. We need to set some more variables, but those contain sensitive data. We will show 2 different ways to make use of Kubernetes features in order to configure those in a secure way. 

### Create one Kubernetes Secret with different entries 
Assuming you have `kubectl` installed on your machine and you are connected to your Kubernetes cluster you can run the following command to create a respective Kubernetes secret, that can be used by the helm chart.

```
kubectl create secret generic librechat \
--from-literal=CREDS_KEY=0963cc1e5e5df9554c8dd32435d0eb2b1a8b6edde6596178d5c5418ade897673 \
--from-literal=CREDS_IV=46d727a066d5d8c4ebc94305d028fecc \
--from-literal=MONGO_URI=mongodb+srv://<user>:<password>@<mongodb-url> \
--from-literal=JWT_SECRET=83e5c1f0e037e4f027dbdb332d54ca1bd1f12af6798700c207ed817ebd7c544b \ --from-literal=JWT_REFRESH_SECRET=83e5c1f0c037e4f027dbab332d54ca1bd1f12af6798700c207ed817ebd7c544
```

- Remember to use your own values for securing sensitive environment variables like `CREDS_KEY`, `CREDS_IV`, `JWT_SECRET`, and `JWT_REFRESH_SECRET`.
- Use the [Credentials Generator](/toolkit/creds_generator) to quickly generate secure values for these variables.

### Create one Kubernetes Secret for each configuration
This one is a bit more complicated but also allows for more fine-grained control over the secrets. For each secret, you would like to create you can run the following command. 

```
kubectl create secret generic librechat-creds-key \
--from-literal=CREDS_KEY=0963cc1e5e5df9554c8dd32435d0eb2b1a8b6edde6596178d5c5418ade897673
```
... and so on for each secret.


## Install Helm Chart
In the root directory run: 

`helm install <deployment-name> helmchart`

Example: `helm install librechat helmchart --set config.envSecrets.secretRef=librechat` (using one Kubernetes secret for all credentials). 

If you used the approach where you created one Kubernetes secret for each credential you will need to do a more extensive configuration which is best placed in a separate file. Create a file with the following content: 

```
config:
  envSecrets:
    secretKeyRef:
    - name: CREDS_KEY
      secretName: librechat-creds-iv
      secretKey: CREDS_KEY
    <...>
```

After that you can run the following command: `helm install librechat helmchart --values <values-override-filel>`
     

## Uninstall Helm Chart

In order to uninstall the Helm Chart simply run: `helm uninstall <deployment-name>`

Example: `helm uninstall librechat`

## Community Helm Charts

- [LibreChat Helm Chart by Blue Atlas Helm Charts](https://github.com/bat-bs/helm-charts/tree/main/charts/librechat)
- Example submitted by [@dimaby](https://github.com/dimaby) on GitHub: [PR #2879](https://github.com/danny-avila/LibreChat/pull/2879)


================================================
FILE: pages/docs/local/index.mdx
================================================
---
title: Intro
description: How to install LibreChat locally
---

import { DockerIcon, NpmIcon } from '@/components/svg/'

# Install LibreChat Locally

**We recommend using `Docker Compose{:hack}` to install LibreChat**, as it is the easiest, simplest, and most reliable method for getting started. With Docker, you can quickly spin up a container that includes everything you need to run LibreChat, including MongoDB, MeiliSearch, as well as the rag_api & vectordb for file support across all endpoints. This approach ensures consistency across different environments and eliminates the need for manual setup of dependencies, making it ideal for most use cases.

Alternatively, you can install LibreChat using the `npm{:zig}` install method. However, this method requires manual setup of MongoDB, MeiliSearch, and the "rag_api + vectordb" dependencies, which can be time-consuming and error-prone.

Whether you choose the Docker, npm install method, or Helm chart for Kubernetes, we have detailed instructions to help you get started with LibreChat.

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <Cards num={3} style={{ display: 'flex', flexDirection: 'row' }}>
    <Cards.Card
      icon={<DockerIcon />}
      title="Docker Install"
      image
      arrow
      href="/docs/local/docker"
    />
    <Cards.Card icon={<NpmIcon />} title="npm Install" image arrow href="/docs/local/npm" />
    <Cards.Card icon="🪖" title="Helm Chart" image arrow href="/docs/local/helm_chart" />
  </Cards>
</div>


================================================
FILE: pages/docs/local/npm.mdx
================================================
---
title: 📦 npm
description: How to install LibreChat locally using npm
---

# Install LibreChat Locally Using npm

For most scenarios, Docker Compose is the recommended installation method due to its simplicity, ease of use, and reliability. If you prefer using npm, you can follow these instructions.

## Prerequisites

- Node.js 18+: [https://nodejs.org/en/download](https://nodejs.org/en/download)
- Git: https://git-scm.com/download/
- MongoDB (Atlas or Community Server)
  - [MongoDB Atlas](/docs/configuration/mongodb/mongodb_atlas)
  - [MongoDB Community Server](/docs/configuration/mongodb/mongodb_community)

## Installation Steps

### Preparation

Run the following commands in your terminal:

```bash filename="Clone the Repository"
git clone https://github.com/danny-avila/LibreChat.git
```

```bash filename="Navigate to the LibreChat Directory"
cd LibreChat
```

```bash filename="Create a .env File from .env.example"
cp .env.example .env
```

> **Note:** **If you're using Windows 10, you might need to use `copy` instead of `cp`.**

```bash filename="Update the MONGO_URI"
Important: Edit the newly created `.env` file to update the `MONGO_URI` with your own MongoDB instance URI.
```

<Callout title="Update the MONGO_URI" emoji="❗">
  Edit the newly created `.env` file to update the `MONGO_URI` with your own
</Callout>

### Build and Start

Once you've completed the preparation steps, run the following commands:

```bash filename="Install dependencies"
npm ci
```

```bash filename="Build the Frontend"
npm run frontend
```

```bash filename="Start LibreChat!"
npm run backend
```

<Callout type="success" title="Access LibreChat!" emoji="🎉">
  **Visit [http://localhost:3080/](http://localhost:3080/)**
</Callout>

<Callout type="example" title="Tip" emoji="🔥">
  - Next time you want to start LibreChat, you only need to execute `npm run backend`
</Callout>

## Update LibreChat

To update LibreChat to the latest version, run the following commands:

<Callout type="warning" emoji="">
  First, stop LibreChat (if you haven't already).
</Callout>

```bash filename="Pull latest project changes"
git pull
```

```bash filename="Update dependencies"
npm ci
```

```bash filename="Rebuild the Frontend"
npm run frontend
```

```bash filename="Start LibreChat!"
npm run backend
```

## Additional Setup

Unlock additional features by exploring our configuration guides to learn how to set up:

- Meilisearch integration
- RAG API connectivity
- Custom endpoints
- Other advanced configuration options
- And more

This will enable you to customize your LibreChat experience with optional features.

**see also:**
- [User Authentication System Setup](/docs/configuration/authentication)
- [AI Setup](/docs/configuration/pre_configured_ai)
- [Custom Endpoints & Configuration](/docs/configuration/librechat_yaml)


================================================
FILE: pages/docs/quick_start/_meta.ts
================================================
export default {
  index: 'Overview',
  local_setup: 'Local Setup',
  custom_endpoints: 'Custom Endpoints',
}



================================================
FILE: pages/docs/quick_start/custom_endpoints.mdx
================================================
import AdditionalLinks from '@/components/repeated/AdditionalLinks.mdx';

# Custom Endpoints

LibreChat supports OpenAI API compatible services using the `librechat.yaml` configuration file.

This guide assumes you have already set up LibreChat using Docker, as shown in the **[Local Setup Guide](/docs/quick_start/local_setup).**

## Step 1. Create or Edit a Docker Override File

- Create a file named `docker-compose.override.yml` file at the project root (if it doesn't already exist).
- Add the following content to the file:

```yaml
services:
  api:
    volumes:
    - type: bind
      source: ./librechat.yaml
      target: /app/librechat.yaml
```

> Learn more about the [Docker Compose Override File here](/docs/configuration/docker_override).

## Step 2. Configure `librechat.yaml`

- **Create a file named `librechat.yaml`** at the project root (if it doesn't already exist).
- **Add your custom endpoints:** you can view compatible endpoints in the [AI Endpoints section](/docs/configuration/librechat_yaml/ai_endpoints).
  - The list is not exhaustive and generally every OpenAI API-compatible service should work.
  - There are many options for Custom Endpoints. View them all here: [Custom Endpoint Object Structure](/docs/configuration/librechat_yaml/object_structure/custom_endpoint).
- As an example, here is a configuration for both **OpenRouter** and **Ollama**:

   ```yaml
   version: 1.1.4
   cache: true
   endpoints:
     custom:
       - name: "OpenRouter"
         apiKey: "${OPENROUTER_KEY}"
         baseURL: "https://openrouter.ai/api/v1"
         models:
           default: ["gpt-3.5-turbo"]
           fetch: true
         titleConvo: true
         titleModel: "current_model"
         summarize: false
         summaryModel: "current_model"
         forcePrompt: false
         modelDisplayLabel: "OpenRouter"
       - name: "Ollama"
         apiKey: "ollama"
         baseURL: "http://host.docker.internal:11434/v1/"
         models:
           default: [
             "llama3:latest",
             "command-r",
             "mixtral",
             "phi3"
             ]
           fetch: true # fetching list of models is not supported
         titleConvo: true
         titleModel: "current_model"
   ```

## Step 3. Configure .env File

- **Edit your existing `.env` file** at the project root 
    - Copy `.env.example` and rename to `.env` if it doesn't already exist.
- According to the config above, the environment variable `OPENROUTER_KEY` is expected and should be set:

```bash
OPENROUTER_KEY=your_openrouter_api_key
```

**Notes:**
- As way of example, this guide assumes you have setup Ollama independently and is accessible to you at `http://host.docker.internal:11434`
    - "host.docker.internal" is a special DNS name that resolves to the internal IP address used by the host.
    - You may need to change this to the actual IP address of your Ollama instance.
- In a future guide, we will go into setting up Ollama along with LibreChat.

## Step 4. Run the App

- Now that your files are configured, you can run the app:

```bash
docker compose up
```

Or, if you were running the app before, you can restart the app with:

```bash
docker compose restart
```

> Note: Make sure your Docker Desktop or Docker Engine is running before executing the command.

## Conclusion

**That's it!** You have now configured **Custom Endpoints** for your LibreChat instance. 

<AdditionalLinks />


================================================
FILE: pages/docs/quick_start/index.mdx
================================================
# Quick Start

Follow the guides below to get up and running with LibreChat as quickly as possible.

#### [Local Setup](./quick_start/local_setup)

Use this easy setup guide and start using LibreChat as quickly as possible.

#### [Custom Endpoints](./quick_start/custom_endpoints)

Configure custom endpoints for services like OpenRouter, Deepseek, Ollama, Mistral AI, groq, Databricks, and others.



================================================
FILE: pages/docs/quick_start/local_setup.mdx
================================================
import AdditionalLinks from '@/components/repeated/AdditionalLinks.mdx';

# Local Setup Guide

This is a condensed version of our [Local Installation Guide](/docs/local/)

## Step 1. Download the Project

### Manual Download

1. **Go to the Project Page**: Visit [https://github.com/danny-avila/LibreChat](https://github.com/danny-avila/LibreChat).

2. **Download the ZIP File**: Click the green "Code" button, then click "Download ZIP."

3. **Extract the ZIP File**: Find the downloaded ZIP file, right-click, and select "Extract All...".

### Using Git

Run the following [git](https://git-scm.com/) command in your terminal, from the desired parent directory:

```bash
git clone https://github.com/danny-avila/LibreChat.git
```

## Step 2. Install Docker

1. **Download**: Go to [Docker Desktop Download Page](https://www.docker.com/products/docker-desktop) and download Docker Desktop.
2. **Install**: Open the installer and follow the instructions.
3. **Run**: Open Docker Desktop to ensure it is running.

**Notes:**
- Docker Desktop is recommended for most users. If you are looking for an advanced docker/container setup, especially for a remote server installation, see our [Ubuntu Docker Deployment Guide](/docs/remote/docker_linux).
- You may need to restart your computer after installation.

## Step 3. Run the App

1. **Navigate to the Project Directory**

2. **Create and Configure .env File**:
   - Copy the contents of `.env.example` to a new file named `.env`.
   - Fill in any necessary values.
      - For an in-depth environment configuration, see the [.env File Configuration Guide](/docs/configuration/dotenv).

3. **Start the Application**:
   - Run the following command:

   ```bash
   docker compose up -d
   ```

## Conclusion

**That's it!** You should now have **LibreChat** running locally on your machine. Enjoy!

---

<AdditionalLinks />



================================================
FILE: pages/docs/remote/_meta.ts
================================================
export default {
  index: 'Intro',
  digitalocean: 'Digital Ocean',
  docker_linux: 'Docker (Remote Linux)',
  huggingface: 'Hugging Face',
  railway: 'Railway',
  tunnels_proxies: {
    // "title": "Contributing",
    type: 'separator',
  },
  cloudflare: 'Cloudflare',
  nginx: 'Nginx',
  ngrok: 'ngrok',
  traefik: 'Traefik',
}



================================================
FILE: pages/docs/remote/cloudflare.mdx
================================================
---
title: Cloudflare
description: Quick Guide to Domain Registration and Cloudflare Tunnel Configuration
---

# Comprehensive Guide to Domain Registration and Configuring Cloudflare Tunnel

## Registering Your Domain

### Step 1: Choose Your Domain Name
- Select a domain name that aligns with your brand identity and is memorable. Avoid complex spellings.
- Use online tools from domain registrars like Cloudflare, Namecheap, GoDaddy, etc., to check if your preferred domain name is available.

### Step 2: Select a Domain Registrar
- Evaluate registrars based on their pricing, customer support, additional features (such as email, SSL certificates), and user reviews.
- Pay special attention to privacy protection services (WHOIS privacy), as this helps keep your personal information private.

### Step 3: Purchase and Register the Domain
- Follow the registrar's purchase process, which involves providing your contact information and completing the payment.
- Thoroughly read the terms of service and note the domain's expiration date to avoid unexpected lapses.

## Configuring Cloudflare Tunnels

### Step 1: Add Your Domain to Cloudflare
- If your domain was purchased via Cloudflare, this step is skipped as your domain is already configured to use Cloudflare's nameservers.
- Log into your Cloudflare account and add your new domain. Follow the instructions to replace your domain's nameservers with those provided by Cloudflare, which is necessary for activating Cloudflare's services.

### Step 3: Create a Tunnel

<Cards>
   <Cards.Card href="https://one.dash.cloudflare.com/" title="Step 1" image arrow>
      ![image](https://github.com/danny-avila/LibreChat/assets/32828263/ae54133f-34ed-476d-9004-d269a8707609)
   </Cards.Card>
   <Cards.Card href="https://one.dash.cloudflare.com/" title="Step 2" image arrow>
      ![image](https://github.com/danny-avila/LibreChat/assets/32828263/643e878e-e418-48aa-ac53-6c144ed463f0)
   </Cards.Card>
   <Cards.Card href="https://one.dash.cloudflare.com/" title="Step 3" image arrow>
      ![image](https://github.com/danny-avila/LibreChat/assets/32828263/e219fd49-baad-4da0-84e7-09e754068c57)
   </Cards.Card>
</Cards>

![image](https://github.com/danny-avila/LibreChat/assets/32828263/c851d405-1e90-4ad1-b65e-03cee5ae7111)

#### Name your Tunnel

![image](https://github.com/danny-avila/LibreChat/assets/32828263/212b8b95-5901-40d4-bc46-3b3b3997767e)

#### Install Cloudflare's `cloudflared` on Your Server

![image](https://github.com/danny-avila/LibreChat/assets/32828263/82520d5c-8524-415f-82ae-c297bde3288d)

<Callout type="tip" title="Tip">
- For continuous operation, consider setting up `cloudflared` to run as a service on your system. This ensures the tunnel remains active after reboots and crashes.
</Callout>

- Download and install the [`cloudflared`](https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/downloads/) tool from Cloudflare's official site onto your server. This software will facilitate the secure connection between your domain and the internal services.
- Authenticate `cloudflared` using your Cloudflare account credentials to link it to your domain.

#### Configure your Tunnel

- Initiate a tunnel using either the Cloudflare dashboard under the "Tunnels" section

![image](https://github.com/danny-avila/LibreChat/assets/32828263/0232a545-115a-4c91-99e3-1240542bbea2)


### Step 6: Verify the Tunnel
- Test the connection by accessing your domain/subdomain in a web browser, ensuring it resolves to your server via the Cloudflare Tunnel without errors.
- Monitor your tunnel's performance and status directly from the Cloudflare dashboard under the "Tunnels" section.

![image](https://github.com/danny-avila/LibreChat/assets/32828263/2ad741d5-8cdd-46c6-982c-7fa05a5858a8)

## Conclusion
By securing your domain registration and configuring a Cloudflare Tunnel, you strengthen the security and reliability of connecting to your network services. Regularly review and update your domain and tunnel settings to adapt to any new requirements or changes in network configuration. Stay proactive about maintaining your online presence and security stance.


================================================
FILE: pages/docs/remote/digitalocean.mdx
================================================
---
title: DigitalOcean
description: These instructions are designed for someone starting from scratch for a Docker Installation on a remote Ubuntu server using one of the cheapest tiers (6 USD/mo)
---

# Digital Ocean Setup

> These instructions + the [docker guide]() are designed for someone starting from scratch for a Docker Installation on a remote Ubuntu server. You can skip to any point that is useful for you. There are probably more efficient/scalable ways, but this guide works really great for my personal use case.

**There are many ways to go about this, but I will present to you the best and easiest methods I'm aware of. These configurations can vary based on your liking or needs.**

Digital Ocean is a great option for deployment: you can benefit off a **free [200 USD credit](https://m.do.co/c/4486923fcf00)** (for 60 days), and one of the cheapest tiers (6 USD/mo) will work for LibreChat in a low-stress, minimal-user environment. Should your resource needs increase, you can always upgrade very easily.

Digital Ocean is also my preferred choice for testing deployment, as it comes with useful resource monitoring and server access tools right out of the box.

**Using the following Digital Ocean link will directly support the project by helping me cover deployment costs with credits!**

## **Click the banner to get a $200 credit and to directly support LibreChat!**

_You are free to use this credit as you wish!_

[![DigitalOcean Referral Badge](https://web-platforms.sfo2.cdn.digitaloceanspaces.com/WWW/Badge%201.svg)](https://www.digitalocean.com/?refcode=4486923fcf00&utm_campaign=Referral_Invite&utm_medium=Referral_Program&utm_source=badge)

_Note: you will need a credit card or PayPal to sign up. I'm able to use a prepaid debit card through PayPal for my billing_

## Table of Contents

- **[Part I: Starting from Zero](#part-i-starting-from-zero)**
  - [1. DigitalOcean signup](#1-click-here-or-on-the-banner-above-to-get-started-on-digitalocean)
  - [2. Access console](#2-access-your-droplet-console)
  - [3. Console user setup](#3-once-you-have-logged-in-immediately-create-a-new-non-root-user)
  - [4. Firewall Setup](#4-firewall-setup)
- **[Part II: Installing Docker & Other Dependencies](#part-ii-installing-docker-and-other-dependencies)**

## Part I: Starting from Zero:

### **1. [Click here](https://m.do.co/c/4486923fcf00) or on the banner above to get started on DigitalOcean**

Once you're logged in, you will be greeted with a [nice welcome screen](https://cloud.digitalocean.com/welcome).

![image](https://github.com/danny-avila/LibreChat/assets/110412045/b7a71eae-770e-4c69-a5d4-d21b939d64ed)

### **a) Click on ["Explore our control panel"](https://cloud.digitalocean.com/projects) or simply navigate to the [Projects page](https://cloud.digitalocean.com/projects)**

Server instances are called **"droplets"** in digitalocean, and they are organized under **"Projects."**

### **b) Click on "Spin up a Droplet" to start the setup**

![image](https://github.com/danny-avila/LibreChat/assets/110412045/6046e8cd-ff59-4795-a29a-5f44ab2f0a6d)

Adjust these settings based on your needs, as I'm selecting the bare minimum/cheapest options that will work.

- **Choose Region/Datacenter:** closest to you and your users
- **Choose an image:** Ubuntu 22.04 (LTS) x64
- **Choose Size:** Shared CPU, Basic Plan
  - CPU options: Regular, 6 USD/mo option (0.009 USD/hour, 1 GB RAM / 1 CPU / 25 GB SSD / 1000 GB transfer)
  - No additional storage
- **Choose Authentication Method:** Password option is easiest but up to you
  - Alternatively, you can setup traditional SSH.
- **Recommended:** Add improved metrics monitoring and alerting (free)
  - You might be able to get away with the $4/mo option by not selecting this, but not yet tested
- **Finalize Details:**
  - Change the hostname to whatever you like, everything else I leave default (1 droplet, no tags)
  - Finally, click "Create Droplet"

![image](https://github.com/danny-avila/LibreChat/assets/110412045/ac90d40e-3ac6-482f-885c-58058c5e3f76)

After creating the droplet, it will now spin up with a progress bar.

### **2. Access your droplet console**

Once it's spun up, **click on the droplet** and click on the Console link on the right-hand side to start up the console.

![image](https://github.com/danny-avila/LibreChat/assets/110412045/47c14280-fe48-49b9-9997-ff4d9c83212c)

![image](https://github.com/danny-avila/LibreChat/assets/110412045/d5e518fd-4941-4b35-86cc-69f8f65ec8eb)

Launching the Droplet console this way is the easiest method but you can also SSH if you set it up in the previous step.

To keep this guide simple, I will keep it easy and continue with the droplet console. Here is an [official DigitalOcean guide for SSH](https://docs.digitalocean.com/products/droplets/how-to/connect-with-ssh/) if you are interested. As mentioned before, the [Hetzner guide](./hetzner_ubuntu.md) has good instructions for this that can apply here.

### **3. Once you have logged in, immediately create a new, non-root user:**

**Note:** you should remove the greater/less than signs anytime you see them in this guide

```bash
# example: adduser danny
adduser <yourusername>
# you will then be prompted for a password and user details
```

Once you are done, run the following command to elevate the user

```bash
# example: usermod -aG sudo danny
usermod -aG sudo <yourusername>
```

**Make sure you have done this correctly by double-checking you have sudo permissions:**

```bash
getent group sudo | cut -d: -f4
```

**Switch to the new user**

```bash
# example: su - danny
su - <yourusername>
```

### **4. Firewall Setup**

It's highly recommended you setup a simple firewall for your setup.

Click on your droplet from the projects page again, and goto the Networking tab on the left-hand side under your ipv4:

![image](https://github.com/danny-avila/LibreChat/assets/110412045/20a2f31b-83ec-4052-bca7-27a672c3770a)

Create a firewall, add your droplet to it, and add these inbound rules (will work for this guide, but configure as needed)

![image](https://github.com/danny-avila/LibreChat/assets/110412045/d9bbdd7b-3702-4d2d-899b-c6457e6d221a)

---

This concludes the initial setup. For the subsequent steps, please proceed to the next guide:**[Docker Deployment Guide](/docs/remote/docker_linux)**, which will walk you through the remaining installation process.



================================================
FILE: pages/docs/remote/docker_linux.mdx
================================================
---
title: Docker (Remote Linux)
description: These instructions are designed for someone starting from scratch for a Docker Installation on a remote Ubuntu server
---

# Ubuntu Docker Deployment Guide

In order to use this guide you need a remote computer or VM deployed. While you can use this guide with a local installation, keep in mind that it was originally written for cloud deployment.

> ⚠️ This guide was originally designed for [Digital Ocean](/docs/remote/digitalocean), so you may have to modify the instruction for other platforms, but the main idea remains unchanged.

## Part I: Installing Docker and Other Dependencies:

There are many ways to setup Docker on Linux systems. I'll walk you through the best and the recommended way [based on this guide](https://www.smarthomebeginner.com/install-docker-on-ubuntu-22-04/).

> Note that the "Best" way for Ubuntu docker installation does not mean the "fastest" or the "easiest". It means, the best way to install it for long-term benefit (i.e. faster updates, security patches, etc.).

### **1. Update and Install Docker Dependencies**

First, let's update our packages list and install the required docker dependencies.

```bash
sudo apt update
```

Then, use the following command to install the dependencies or pre-requisite packages.

```bash
sudo apt install apt-transport-https ca-certificates curl software-properties-common gnupg lsb-release
```

#### **Installation Notes**

- Input "Y" for all [Y/n] (yes/no) terminal prompts throughout this entire guide.
- After the first [Y/n] prompt, you will get the first of a few **purple screens** asking to restart services.
  - Each time this happens, you can safely press ENTER for the default, already selected options:

![image](https://github.com/danny-avila/LibreChat/assets/110412045/05cf165b-d3d8-475a-93b3-254f3c63f59b)

- If at any point your droplet console disconnects, do the following and then pick up where you left off:
  - Access the console again as indicated above
  - Switch to the user you created with `su - <yourusername>`

### **2. Add Docker Repository to APT Sources**

While installing Docker Engine from Ubuntu repositories is easier, adding official docker repository gives you faster updates. Hence why this is the recommended method.

First, let us get the GPG key which is needed to connect to the Docker repository. To that, use the following command.

```bash
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
```

Next, add the repository to the sources list. While you can also add it manually, the command below will do it automatically for you.

```bash
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
```

The above command will automatically fill in your release code name (jammy for 22.04, focal for 20.04, and bionic for 18.04).

Finally, refresh your packages again.

```bash
sudo apt update
```

If you forget to add the GPG key, then the above step would fail with an error message. Otherwise, let's get on with installing Docker on Ubuntu.

### **3. Install Docker**

> What is the difference between docker.io and docker-ce?

> docker.io is the docker package that is offered by some popular Linux distributions (e.g. Ubuntu/Debian). docker-ce on the other hand, is the docker package from official Docker repository. Typically docker-ce more up-to-date and preferred.

We will now install the docker-ce (and not docker.io package)

```bash
sudo apt install docker-ce
```

Purple screen means press ENTER. :)

Recommended: you should make sure the created user is added to the docker group for seamless use of commands:

```bash
sudo usermod -aG docker $USER
```

Now let's reboot the system to make sure all is well.

```bash
sudo reboot
```

After rebooting, if using the browser droplet console, you can click reload and wait to get back into the console.

![image](https://github.com/danny-avila/LibreChat/assets/110412045/2ad7b739-a3db-4744-813f-39af7dabfce7)

**Reminder:** Any time you reboot with `sudo reboot`, you should switch to the user you setup as before with `su - <yourusername>`.

### **4. Verify that Docker is Running on Ubuntu**

There are many ways to check if Docker is running on Ubuntu. One way is to use the following command:

```bash
sudo systemctl status docker
```

You should see an output that says **active (running)** for status.

![image](https://github.com/danny-avila/LibreChat/assets/110412045/6baea405-8dfb-4d9d-9327-6e9ecf800471)

Exit this log by pressing CTRL (or CMD) + C.

### **5. Install the Latest Version of Docker Compose**

The version of docker-compose packaged with the Linux distribution is probably old and will not work for us.

Checking the releases on the [Docker Compose GitHub](https://github.com/docker/compose/releases), the last release is v2.26.1 (as of 4/6/24).

You will have to manually download and install it. But fear not, it is quite easy.

First, download the latest version of Docker Compose using the following command:

```bash
sudo curl -L https://github.com/docker/compose/releases/download/v2.26.1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose
```

Next, make it executable using the following command:

```bash
sudo chmod +x /usr/local/bin/docker-compose
```

Docker Compose should now be installed on your Ubuntu system. Let's check to be sure.

```bash
docker-compose -v
# output should be: Docker Compose version v2.20.2
```

If you get a permission denied error, like I did, reboot/switch to your created user again, and run `sudo chmod +x /usr/local/bin/docker-compose` again

#### Note on Docker Compose Commands

As of Docker Compose v2, `docker-compose` is now `docker compose`. This guide will use the old commands for now, but you should be aware of this change and that `docker compose` is often preferred.

### **6. As part of this guide, I will recommend you have git and npm installed:**

Though not technically required, having git and npm will make installing/updating very simple:

```bash
sudo apt install git nodejs npm
```

Cue the matrix lines.

You can confirm these packages installed successfully with the following:

```bash
git --version
node -v
npm -v
```

![image](https://github.com/danny-avila/LibreChat/assets/110412045/fbba1a38-95cd-4e8e-b813-04001bb82b25)

> Note: this will install some pretty old versions, for npm in particular. For the purposes of this guide, however, this is fine, but this is just a heads up in case you try other things with node in the droplet. Do look up a guide for getting the latest versions of the above as necessary.

**Ok, now that you have set up the Droplet, you will now setup the app itself**

---

## Part II: Setup LibreChat

### **1. Clone down the repo**

From the _droplet_ commandline (as your user, not root):

```bash
# clone down the repository
git clone https://github.com/danny-avila/LibreChat.git

# enter the project directory
cd LibreChat/
```

### **2. Create LibreChat Config and Environment files**

#### Config (librechat.yaml) File

Next, we create the [LibreChat Config file](/docs/configuration/librechat_yaml), AKA `librechat.yaml`, allowing for customization of the app's settings as well as [custom endpoints](/docs/configuration/librechat_yaml/ai_endpoints).

Whether or not you want to customize the app further, it's required for the `deploy-compose.yml` file we are using, so we can create one with the bare-minimum value to start:

```bash
nano librechat.yaml
```

You will enter the editor screen, and you can paste the following:

```yaml
# For more information, see the Configuration Guide:
# https://docs.librechat.ai/install/configuration/custom_config.html

# Configuration version (required)
version: 1.0.5
# This setting caches the config file for faster loading across app lifecycle
cache: true
```

Exit the editor with `CTRL + X`, then `Y` to save, and `ENTER` to confirm.

#### Environment (.env) File

The default values are enough to get you started and running the app, allowing you to provide your credentials from the web app.

```bash
# Copies the example file as your global env file
cp .env.example .env
```

However, it's **highly recommended** you adjust the "secret" values from their default values for added security. The API startup logs will warn you if you don't.

For conveninence, you can run this to generate your own values:

[https://www.librechat.ai/toolkit/creds_generator](https://www.librechat.ai/toolkit/creds_generator)

```bash
nano .env

# FIND THESE VARIABLES AND REPLACE THEIR DEFAULT VALUES!

# Must be a 16-byte IV (32 characters in hex)

CREDS_IV=e2341419ec3dd3d19b13a1a87fafcbfb

# Must be 32-byte keys (64 characters in hex)

CREDS_KEY=f34be427ebb29de8d88c107a71546019685ed8b241d8f2ed00c3df97ad2566f0
JWT_SECRET=16f8c0ef4a5d391b26034086c628469d3f9f497f08163ab9b40137092f2909ef
JWT_REFRESH_SECRET=eaa5191f2914e30b9387fd84e254e4ba6fc51b4654968a9b0803b456a54b8418
```

If you'd like to provide any credentials for all users of your instance to consume, you should add them while you're still editing this file:

```bash
OPENAI_API_KEY=sk-yourKey
```

As before, exit the editor with `CTRL + X`, then `Y` to save, and `ENTER` to confirm.

**That's it!**

For thorough configuration, however, you should edit your .env file as needed, and do read the comments in the file and the resources below.

```bash
# if editing the .env file
nano .env
```

This is one such env variable to be mindful of. This disables external signups, in case you would like to set it after you've created your account.

```shell
ALLOW_REGISTRATION=false
```

**Resources:**
- [Tokens/Apis/etc](/docs/configuration/pre_configured_ai)
- [User/Auth System](/docs/configuration/authentication)

### **3. Start docker**

```bash
# should already be running, but just to be safe
sudo systemctl start docker

# confirm docker is running
docker info
```

Now we can start the app container. For the first time, we'll use the full command and later we can use a shorthand command

```bash
sudo docker-compose -f ./deploy-compose.yml up -d
```

![image](https://github.com/danny-avila/LibreChat/assets/110412045/5e2f6627-8ca4-4fa3-be73-481539532ee7)

It's safe to close the terminal if you wish -- the docker app will continue to run.

> Note: this is using a special compose file optimized for this deployed environment. If you would like more configuration here, you should inspect the deploy-compose.yml and Dockerfile.multi files to see how they are setup. We are not building the image in this environment since it's not enough RAM to properly do so. Instead, we pull the latest dev-api image of librechat, which is automatically built after each push to main.

> If you are setting up a domain to be used with LibreChat, this compose file is using the nginx file located in client/nginx.conf. Instructions on this below in part V.

### **4. Once the app is running, you can access it at `http://yourserverip`**

#### Go back to the droplet page to get your server ip, copy it, and paste it into your browser!

![image](https://github.com/danny-avila/LibreChat/assets/110412045/d8bbad29-6015-46ec-88ce-a72a43d8a313)

#### Sign up, log in, and enjoy your own privately hosted, remote LibreChat :)

![image](https://github.com/danny-avila/LibreChat/assets/110412045/85070a54-eb57-479f-8011-f63c14116ee3)

![image](https://github.com/danny-avila/LibreChat/assets/110412045/b3fc2152-4b6f-46f9-81e7-4200b76bc468)

## Part III: Updating LibreChat

I've made this step pretty painless, provided everything above was installed successfully and you haven't edited the git history.

> Note: If you are working on an edited branch, with your own commits, for example, such as with edits to client/nginx.conf, you should inspect config/deployed-update.js to run some of the commands manually as you see fit. See part V for more on this.

Run the following for an automated update

```bash
npm run update:deployed
```

**Stopping the docker container**

```bash
npm run stop:deployed
```

> This simply runs `docker-compose -f ./deploy-compose.yml down`

**Starting the docker container**

```bash
npm run start:deployed
```

> This simply runs `docker-compose -f ./deploy-compose.yml up -d`

**Check active docker containers**

```bash
docker ps
```

You can update manually without the scripts if you encounter issues.

```bash filename="Stop the running container(s)""
docker compose -f ./deploy-compose.yml down
```

```bash filename="Pull latest project changes"
git pull 
```

```bash filename="Pull the latest LibreChat image""
docker compose -f ./deploy-compose.yml pull
```

```bash filename="Start LibreChat"
docker compose -f ./deploy-compose.yml up
```

## Part IV: Editing the NGINX file (for custom domains and advanced configs)

In case you would like to edit the NGINX file for whatever reason, such as pointing your server to a custom domain, use the following:

```bash filename="First, stop the active instance if running"
npm run stop:deployed
```
```bash filename="now you can safely edit"
nano client/nginx.conf
```

I won't be walking you through custom domain setup or any other changes to NGINX, you can look into the [Cloudflare guide](/docs/remote/cloudflare), the [Traefik guide](/docs/remote/traefik) or the [NGINX guide](/docs/remote/nginx) to get you started with custom domains.

However, I will show you what to edit on the LibreChat side for a custom domain with this setup.

Since NGINX is being used as a proxy pass by default, I only edit the following:

```shell
# before
server_name localhost;

# after
server_name custom.domain.com;
```

> Note: this works because the deploy-compose.yml file is using NGINX by default, unlike the main docker-compose.yml file. As always, you can configure the compose files as you need.

Now commit these changes to a separate branch:

```bash
# create a new branch
# example: git checkout -b edit
git checkout -b <branchname>

# stage all file changes
git add .
```

To commit changes to a git branch, you will need to identify yourself on git. These can be fake values, but if you would like them to sync up with GitHub, should you push this branch to a forked repo of LibreChat, use your GitHub email

```bash
# these values will work if you don't care what they are
git config --global user.email "you@example.com"
git config --global user.name "Your Name"

# Now you can commit the change
git commit -m "edited nginx.conf"
```

Updating on an edited branch will work a little differently now

```bash
npm run rebase:deployed
```

You should be all set!

> **Warning** You will experience merge conflicts if you start significantly editing the branch and this is not recommended unless you know what you're doing

> Note that any changes to the code in this environment won't be reflected because the compose file is pulling the docker images built automatically by GitHub

## Part V: Use the Latest Stable Release instead of Latest Main Branch

By default, this setup will pull the latest updates to the main branch of Librechat. If you would rather have the latest "stable" release, which is defined by the [latest tags](https://github.com/danny-avila/LibreChat/releases), you will need to edit deploy-compose.yml and commit your changes exactly as above in Part V. Be aware that you won't benefit from the latest feature as soon as they come if you do so.

Let's edit `deploy-compose.yml`:

```bash
nano deploy-compose.yml
```

Change `librechat-dev-api` to `librechat-api`:

```yaml
image: ghcr.io/danny-avila/librechat-api:latest
```

Stage and commit as in Part V, and you're all set!



================================================
FILE: pages/docs/remote/huggingface.mdx
================================================
---
title: HuggingFace
description: Easily deploy LibreChat on Hugging Face Spaces
---

# Hugging Face Deployment

## Create and Configure your Database (Required)

The first thing you need is to create a MongoDB Atlas Database and get your connection string.

Follow the instructions in this document: **[MongoDB Atlas](/docs/configuration/mongodb/mongodb_atlas)**

## Getting Started

**1.** Login or Create an account on **[Hugging Face](https://huggingface.co/)**

**2.** Visit **[https://huggingface.co/spaces/LibreChat/template](https://huggingface.co/spaces/LibreChat/template)** and click on `Duplicate this Space` to copy the LibreChat template into your profile. 

> Note: It is normal for this template to have a runtime error, you will have to configure it using the following guide to make it functional.

  ![image](https://github.com/fuegovic/LibreChat/assets/32828263/fd684254-cbe0-4039-ba4a-7c492b16a453)

**3.** Name your Space and Fill the `Secrets` and `Variables`
 
  >You can also decide here to make it public or private

  ![image](https://github.com/fuegovic/LibreChat/assets/32828263/13a039b9-bb78-4d56-bab1-74eb48171516)

You will need to fill these values:

| Secrets | Values |
| --- | --- |
| MONGO_URI | * use these instruction to get the string: https://librechat.ai/docs/configuration/mongodb/mongodb_atlas |
| OPENAI_API_KEY | `user_provided` | 
| BINGAI_TOKEN | `user_provided` | 
| CHATGPT_TOKEN | `user_provided` |
| ANTHROPIC_API_KEY | `user_provided` |
| GOOGLE_KEY | `user_provided` |
| CREDS_KEY | * see bellow |
| CREDS_IV | * see bellow |
| JWT_SECRET | * see bellow |
| JWT_REFRESH_SECRET | * see bellow |

> ⬆️ **Leave the value field blank for any endpoints that you wish to disable.** 

> ⚠️ setting the API keys and token to `user_provided` allows you to provide them safely from the webUI

> * For `CREDS_KEY`, `CREDS_IV` and `JWT_SECRET` use this tool: **[Credentials Generator](/toolkit/creds_generator)**
> * Run the tool a second time and use the new `JWT_SECRET` value for the `JWT_REFRESH_SECRET`

| Variables | Values |
| --- | --- |
| APP_TITLE | LibreChat |
| ALLOW_REGISTRATION | true |

## Deployment

**1.** When you're done filling the `secrets` and `variables`, click `Duplicate Space` in the bottom of that window

  ![image](https://github.com/fuegovic/LibreChat/assets/32828263/55d596a3-2be9-4e14-ac0d-0b493d463b1b)


**2.** The project will now build, this will take a couple of minutes

  ![image](https://github.com/fuegovic/LibreChat/assets/32828263/f9fd10e4-ae50-4b5f-a9b5-0077d9e4eaf6)


**3.** When ready, `Building` will change to `Running` 

  ![image](https://github.com/fuegovic/LibreChat/assets/32828263/91442e84-9c9e-4398-9011-76c479b6f272)

  And you will be able to access LibreChat!

  ![image](https://github.com/fuegovic/LibreChat/assets/32828263/cd5950d4-ecce-4f13-bbbf-b9109e462e10)

## Update
  To update LibreChat, simply select `Factory Reboot` from the ⚙️Settings menu

  ![image](https://github.com/fuegovic/LibreChat/assets/32828263/66f20129-0ffd-44f5-b91c-fcce1932112f)


## Conclusion
  You can now access it with from the current URL. If you want to access it without the Hugging Face overlay, you can modify this URL template with your info:

  `https://username-projectname.hf.space/` 
  
  e.g. `https://cooluser-librechat.hf.space/`

**🎉 Congratulation, you've sucessfully deployed LibreChat on Hugging Face! 🤗**


## Meilisearch Setup (Optional)

To enable the search functionality in LibreChat, you'll need to deploy and configure a Meilisearch instance.  Here's how:

**1. Duplicate the Meilisearch Space:**

Visit this link: [https://huggingface.co/spaces/LibreChat/meilisearch](https://huggingface.co/spaces/LibreChat/meilisearch) and click "Duplicate this Space".

**2. Configure the Meilisearch Space:**

   *   **Visibility:** Set the visibility to "public".

   *   **MEILI_MASTER_KEY:** Generate a secure 16-character master key. You can use a tool like [https://randomkeygen.com/](https://randomkeygen.com/) to generate a random key.  Set this key as the value for the `MEILI_MASTER_KEY` environment variable in the Meilisearch space.  *Important: Keep this key secure!*

   *   **MEILI_ENV:** Set the `MEILI_ENV` environment variable to `production`.

**3. Duplicate the Space:**

Click the "Duplicate Space" button.

**4. Configure LibreChat to use Meilisearch:**

   *   **Edit the Dockerfile:** Go to your LibreChat space (the one you duplicated from the main LibreChat template). Navigate to "Files" -> "Dockerfile" and click "Edit".

   *   **Uncomment and Modify Lines:**  Uncomment/edit the following lines in the Dockerfile.  These lines will contain `ENV SEARCH` and `ENV MEILI_*`.  Make sure to replace `<YOUR_MEILISEARCH_SPACE_URL>` with the actual URL of your Meilisearch deployment on Hugging Face Spaces. It should look something like https://username-meilisearch.hf.space/.  *Update the username to match your username!*

       ```dockerfile
       ENV SEARCH=true
       ENV MEILI_NO_ANALYTICS=true
       ENV MEILI_HOST=<YOUR_MEILISEARCH_SPACE_URL>
       ```

   *   **Commit Changes:** Commit your changes to the `main` branch.

**5. Add the `MEILI_MASTER_KEY` Secret to LibreChat:**

   *   Go to your LibreChat space's settings (the LibreChat deployment, not the Meilisearch one).

   *   Click "New secret".

   *   **Name:** Enter `MEILI_MASTER_KEY`.

   *   **Value:**  Enter the *same* master key you used when setting up the Meilisearch space.

**6. Verify the Setup:**

   After LibreChat rebuilds and starts running, you should see a search option in the top left of the LibreChat interface.  If you don't see it, double-check that you've followed all the steps correctly.



================================================
FILE: pages/docs/remote/index.mdx
================================================
---
title: Intro
description: Introduction to deploying LibreChat, offering a comparison of various hosting and network services
---

# Deployment Introduction

Welcome to the introductory guide for deploying LibreChat. This document provides an initial overview, featuring a comparison table and references to detailed guides, ensuring a thorough understanding of deployment strategies.

In this guide, you will explore various options to efficiently deploy LibreChat in a variety of environments, customized to meet your specific requirements.

## Minimum Requirements

The minimum requirements for running LibreChat:

- 1 GiB RAM
- 1 vCPU

**Note:** With everything enabled, you might consider increasing the RAM to 2GB for smoother operation.

## Comparative Table

> Note that the "Recommended" label indicates that these services are well-documented, widely used within the community, or have been successfully deployed by a significant number of users. As a result, we're able to offer better support for deploying LibreChat on these services

### Hosting Services

| **Service**                                | **Domain**                | **Pros**                                                   | **Cons**                               | **Comments**                                            |      **Recommended**    |
|--------------------------------------------|---------------------------|------------------------------------------------------------|----------------------------------------|---------------------------------------------------------|-------------------------|
| [DigitalOcean](/docs/remote/digitalocean)  | Cloud Infrastructure      | Intuitive interface, stable pricing                        | Smaller network footprint              | Optimal for enthusiasts & small to medium businesses    | ✅ Well Known, Reliable |
| [HuggingFace](/docs/remote/huggingface)    | AI/ML Solutions           | ML/NLP specialization                                      | Focused on ML applications             | Excellent for AI/ML initiatives                         | ✅ Free                 |
| [Railway](/docs/remote/railway)            | App Deployment            | Simplified app deployment                                  | Limited access to containers           | Very easy to get started                                | ✅ Easy                 |

### Network Services 

| **Service**                           | **Domain**                  | **Pros**                                            | **Cons**                                         | **Comments**                                    |
|---------------------------------------|-----------------------------|-----------------------------------------------------|--------------------------------------------------|-------------------------------------------------|
| [Cloudflare](/docs/remote/cloudflare) | Web Performance & Security  | Global CDN, DDoS protection, ease of use            | Customer support can be slow                     | Top choice for security enhancements            |
| [Nginx](/docs/remote/nginx)           | Web Server, Reverse Proxy   | High performance, stability, resource efficiency    | Manual setup, limited extensions                 | Widely used for hosting due to its performance  |
| [ngrok](/docs/remote/ngrok)           | Secure Tunneling            | Easy to use, free tier available, secure tunneling  | Requires client download, complex domain routing | Handy for local development tests               |
| [Traefik](/docs/remote/traefik)       | Reverse Proxy, Load Balancer| Automatic service discovery, native cluster support | Configuration can be complex for beginners       | Ideal for microservices and dynamic environments|



**Cloudflare** is known for its extensive network that speeds up and secures internet services, with an intuitive user interface and robust security options on premium plans.

**Ngrok** is praised for its simplicity and the ability to quickly expose local servers to the internet, making it ideal for demos and testing.

**Nginx** is a high-performance web server that is efficient in handling resources and offers stability. It does, however, require manual setup and has fewer modules and extensions compared to other servers.

**Traefik**  is renowned for its automatic configuration updates and ease of deployment in container environments, appealing to DevOps for its integration with various back-ends and dynamic reconfiguration. It thrives in microservices architectures but may pose challenges for those new to cloud-native technologies.

## Cloud Vendor Integration and Configuration

The integration level with cloud vendors varies: from platforms enabling single-click LibreChat deployments like [Railway](/docs/remote/railway), through platforms leveraging Infrastructure as Code tools such as Azure with Terraform, to more traditional VM setups requiring manual configuration, exemplified by [DigitalOcean](/docs/remote/digitalocean), Linode, and Hetzner.

## Essential Security Considerations

Venturing into the digital landscape reveals numerous threats to the security and integrity of your online assets. To safeguard your digital domain, it is crucial to implement robust security measures.

When deploying applications on a global scale, it is essential to consider the following key factors to ensure the protection of your digital assets:

1. Encrypting data in transit: Implementing HTTPS with SSL certificates is vital to protect your data from interception and eavesdropping attacks.
2. Global accessibility implications: Understand the implications of deploying your application globally, including the legal and compliance requirements that vary by region.
3. Secure configuration: Ensure that your application is configured securely, including the use of secure protocols, secure authentication, and authorization mechanisms.

If you choose to use IaaS or Tunnel services for your deployment, you may need to utilize a reverse proxy such as [Nginx](/docs/remote/nginx), [Traefik](/docs/remote/traefik) or [Cloudflare](/docs/remote/cloudflare) to name a few.

Investing in the appropriate security measures is crucial to safeguarding your digital assets and ensuring the success of your global deployment.

## Choosing the Cloud vendor (e.g. platform)

Choosing a cloud vendor, for the "real deployment" is crucial as it impacts cost, performance, security, and scalability. You should consider factors such as data center locations, compliance with industry standards, compatibility with existing tools, and customer support.

There is a lot of options that differ in many aspects. In this section you can find some options that the team and the community uses that can help you in your first deployment.
Once you gain more knowledge on your application usage and audience you will probably be in a position to decide what cloud vendor fits you the best for the long run.

As said the cloud providers / platforms differ in many aspects. For our purpose we can assume that in our context your main concerns is will ease of use, security and (initial) cost. In case that you have more concerns like scaling, previous experience with any of the platforms or any other specific feature then you probably know better what platform fit's you and you can jump directly to the information that you are seeking without following any specific guide.

## Choosing the Right Deployment Option for Your Needs

The deployment options are listed in order from most effort and control to least effort and control

> Each deployment option has its advantages and disadvantages, and the choice ultimately depends on the specific needs of your project.

### 1. IaaS (Infrastructure as a Service)

Infrastructure as a Service (IaaS) refers to a model of cloud computing that provides fundamental computing resources, such as virtual servers, network, and storage, on a pay-per-use basis. IaaS allows organizations to rent and access these resources over the internet, without the need for investing in and maintaining physical hardware. This model provides scalability, flexibility, and cost savings, as well as the ability to quickly and easily deploy and manage infrastructure resources in response to changing business needs.

- [DigitalOcean](/docs/remote/digitalocean): User-friendly interface with predictable pricing.
- Linode: Renowned for excellent customer support and straightforward pricing.
- Hetzner: Prioritizes privacy and cost-effectiveness, ideal for European-centric deployments.

#### For Iaas we recommend Docker Compose

**Why Docker Compose?** We recommend Docker Compose for consistent deployments. This guide clearly outlines each step for easy deployment: [Docker - Linux remote install guide](/docs/remote/docker_linux)

**Note:** There are two docker compose files in the repo

1. **Development Oriented docker compose `docker-compose.yml`**
2. **Deployment Oriented docker compose `deploy-compose.yml`**

The main difference is that `deploy-compose.yml` includes Nginx, making its configuration internal to Docker.

> Look at the [Nginx Guide](/docs/remote/nginx) for more information

### 2. IaC (Infrastructure as Code)

Infrastructure as Code (IaC) refers to the practice of managing and provisioning computing infrastructures through machine-readable definition files, as opposed to physical hardware configuration or interactive configuration tools. This approach promotes reproducibility, disposability, and scalability, particularly in modern cloud environments. IaC allows for the automation of infrastructure deployment, configuration, and management, resulting in faster, more consistent, and more reliable provisioning of resources.

- Azure: Comprehensive services suitable for enterprise-level deployments

**Note:** Digital Ocean, Linode, Hetzner also support IaC. While we lack a specific guide, you can try to adapt the adapt the Azure Guide for Terraform and help us contribute to its enhancement.

### 3. PaaS (Platform as a Service)

Platform as a Service (PaaS) is a model of cloud computing that offers a development and deployment environment in the cloud. It provides a platform for developers to build, test, and deploy applications, without the need for managing the underlying infrastructure. PaaS typically includes a range of resources such as databases, middleware, and development tools, enabling users to deliver simple cloud-based apps to sophisticated enterprise applications. This model allows for faster time-to-market, lower costs, and easier maintenance and scaling, as the service provider is responsible for maintaining the infrastructure, and the customer can focus on building, deploying and managing their applications.

- [Hugging Face](/docs/remote/huggingface): Tailored for machine learning and NLP projects.
- Render: Simplifies deployments with integrated CI/CD pipelines.
- Heroku: Optimal for startups and quick deployment scenarios.

### 4. One Click Deployment (PaaS)

- [Railway](/docs/remote/railway): Popular one-click deployment solution
- Zeabur: Pioneering effortless one-click deployment solutions.

## Other / Network Services

### 1. Tunneling

Tunneling services allow you to expose a local development server to the internet, making it accessible via a public URL. This is particularly useful for sharing work, testing, and integrating with third-party services. It allows you to deploy your development computer for testing or for on-prem installation.

- [Ngrok](/docs/remote/ngrok): Facilitates secure local tunneling to the internet.
- [Cloudflare](/docs/remote/cloudflare): Enhances web performance and security.

### 2. DNS Service

- Cloudflare DNS service is used to manage and route internet traffic to the correct destinations, by translating human-readable domain names into machine-readable IP addresses. Cloudflare is a provider of this service, offering a wide range of features such as security, performance, and reliability. The Cloudflare DNS service provides a user-friendly interface for managing DNS records, and offers advanced features such as traffic management, DNSSEC, and DDoS protection.

see also: [Cloudflare Guide](/docs/remote/cloudflare)

## Conclusion

In conclusion, the introduction of our deployment guide provides an overview of the various options and considerations for deploying LibreChat. It is important to carefully evaluate your needs and choose the path that best aligns with your organization's goals and objectives. Whether you prioritize ease of use, security, or affordability, our guide provides the necessary information to help you successfully deploy LibreChat and achieve your desired outcome. We hope that this guide will serve as a valuable resource for you throughout your deployment journey.

Remember, our community is here to assist. Should you encounter challenges or have queries, our [Discord channel](https://discord.librechat.ai) and [troubleshooting discussion](https://github.com/danny-avila/LibreChat/discussions/categories/troubleshooting) are excellent resources for support and advice.



================================================
FILE: pages/docs/remote/nginx.mdx
================================================
---
title: Secure Deployment with Nginx
description: Step-by-step guide for securing your LibreChat deployment with Nginx as a reverse proxy and HTTPS
---

# Secure Deployment with Nginx

This guide covers the essential steps for securing your LibreChat deployment with an SSL/TLS certificate for HTTPS, setting up Nginx as a reverse proxy, and configuring your domain.

## Prerequisites

1. A cloud server (e.g., AWS, Google Cloud, Azure, Digital Ocean).
2. A registered domain name.
3. Terminal access to your cloud server.
4. Node.js and NPM installed on your server.

## Initial Setup

### Pointing Your Domain to Your Server

Before proceeding with certificate acquisition, point your domain to your cloud server's IP address. This step is foundational and must precede SSL certificate setup due to the time DNS records may require to propagate globally.

1. Log in to your domain registrar's control panel.
2. Navigate to DNS settings.
3. Create an `A record` pointing your domain to the IP address of your cloud server.
4. Wait for the DNS changes to propagate globally (you can check by pinging your domain: `ping your_domain.com`).

## Obtain an SSL/TLS Certificate

To secure your LibreChat application with HTTPS, you'll need an SSL/TLS certificate. Let's Encrypt offers free certificates:

1. Install Certbot:
    - For Ubuntu: `sudo apt-get install certbot python3-certbot-nginx`
    - For CentOS: `sudo yum install certbot python2-certbot-nginx`

2. Obtain the Certificate:
    - Run `sudo certbot --nginx` to obtain and install the certificate automatically for Nginx.
    - Follow the on-screen instructions. Certbot will ask for information and complete the validation process.
    - Once successful, Certbot will store your certificate files.

## Set Up Nginx as a Reverse Proxy

Nginx acts as a reverse proxy, forwarding client requests to your LibreChat application. There are two deployment options:

### Option A: Using the `deploy-compose.yml` Docker Compose (Recommended)

The `deploy-compose.yml` file includes an Nginx container and uses the `client/nginx.conf` file for Nginx configuration. However, since `sudo certbot --nginx` extracts the certificate to the host configuration, you need to duplicate the certificate to the Docker containers.

1. Update `client/nginx.conf` with your domain and certificate paths.
2. Update `deploy-compose.yml` in the `client` section to mount the certificate files from the host:

```yaml
client:
  # ...
  volumes:
    - ./client/nginx.conf:/etc/nginx/conf.d/default.conf
    - /etc/letsencrypt/live/<your-domain>:/etc/letsencrypt/live/<your-domain>
    - /etc/letsencrypt/archive/<your-domain>:/etc/letsencrypt/archive/<your-domain>
    - /etc/letsencrypt/options-ssl-nginx.conf:/etc/letsencrypt/options-ssl-nginx.conf
    - /etc/letsencrypt/ssl-dhparams.pem:/etc/letsencrypt/ssl-dhparams.pem
```

3. Stop any running instance: `npm run stop:deployed`
4. Commit the changes to a new Git branch.
5. Rebase the deployed instance: `npm run rebase:deployed`

### Option B: Host-based Deployment without Docker

If you're not using Docker, you can install and configure Nginx directly on the host:

1. Install Nginx:
    - Ubuntu: `sudo apt-get install nginx`
    - CentOS: `sudo yum install nginx`

2. Start Nginx: `sudo systemctl start nginx`

3. Open the Nginx configuration file: `sudo nano /etc/nginx/sites-available/default`

4. Replace the file content with the following, ensuring to replace `your_domain.com` with your domain and `app_port` with your application's port:

```nginx filename="/etc/nginx/sites-available/default"
server {
    listen 80;
    server_name your_domain.com;

    location / {
        proxy_pass http://localhost:app_port;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
    }
}
```

5. Check the Nginx configuration: `sudo nginx -t`
6. Reload Nginx: `sudo systemctl reload nginx`

## Run the Application

1. Navigate to your application's directory:

```bash filename="Replace 'LibreChat' with your actual application directory.
cd LibreChat
```

2. Start your application using Docker Compose:

```bash filename="Start your application"
sudo docker-compose -f ./deploy-compose.yml up -d
```

## Web Application Firewall

Nginx can be configured to act as a web application firewall (WAF) by leveraging the OWASP Core Rule Set (CRS), which provides a robust set of rules to protect against common web application vulnerabilities and attacks. Using OWASP CRS with Nginx can enhance the security of your LibreChat deployment by adding an additional layer of protection.

1. Install OWASP CRS:

    - Ubuntu: `sudo apt-get install nginx-modsecurity-crs`

2. Enable ModSecurity in Nginx:
    - Open your Nginx configuration file (e.g., `/etc/nginx/nginx.conf`).
    - Add the following lines inside the `http` block:

      ```yaml filename="nginx.conf"
      modsecurity on;
      modsecurity_rules_file /usr/share/nginx/modsecurity-crs/nginx-modsecurity.conf;
      ```

3. Configure OWASP CRS:
    - The OWASP CRS package typically includes a configuration file (e.g., `/etc/nginx/modsecurity.d/nginx-modsecurity.conf`) where you can adjust various settings and rulesets based on your requirements.

4. Reload Nginx:
    - `sudo systemctl reload nginx`

By enabling OWASP CRS in your Nginx configuration, you can leverage the comprehensive set of rules provided by the project to detect and mitigate various web application vulnerabilities and attacks, such as SQL injection, cross-site scripting (XSS), remote file inclusion, and more.

## Static File Caching and Compression

LibreChat now supports static file caching and compression natively. If you're using NGINX to handle compression, you should disable compression in LibreChat to avoid redundant processing. You can do this by setting the `DISABLE_COMPRESSION` environment variable to `true` in your LibreChat configuration.

```.env
# .env file
DISABLE_COMPRESSION=true
```

This will prevent LibreChat from compressing static files, allowing NGINX to handle compression more efficiently.

For more information on static file handling in LibreChat, including caching options, refer to the [Static File Handling](/docs/configuration/dotenv#static-file-handling) documentation.


================================================
FILE: pages/docs/remote/ngrok.mdx
================================================
---
title: ngrok
description: Use Ngrok to tunnel your local server to the internet.
---

# Ngrok Installation

To use ngrok for tunneling your local server to the internet, follow these steps:

## Sign up

1. Go to **[https://ngrok.com/](https://ngrok.com/)** and sign up for an account.

## Docker Installation

1. Copy your auth token from: **[https://dashboard.ngrok.com/get-started/your-authtoken](https://dashboard.ngrok.com/get-started/your-authtoken)**
2. Open a terminal and run the following command: `docker run -d -it -e NGROK_AUTHTOKEN=<your token> ngrok/ngrok http 80`

## Windows Installation

1. Download the ZIP file from: **[https://ngrok.com/download](https://ngrok.com/download)**
2. Extract the contents of the ZIP file using 7zip or WinRar.
3. Run `ngrok.exe`.
4. Copy your auth token from: **[https://dashboard.ngrok.com/get-started/your-authtoken](https://dashboard.ngrok.com/get-started/your-authtoken)**
5. In the `ngrok.exe` terminal, run the following command: `ngrok config add-authtoken <your token>`
6. If you haven't done so already, start LibreChat normally.
7. In the `ngrok.exe` terminal, run the following command: `ngrok http 3080`

You will see a link that can be used to access LibreChat.
![ngrok-1](https://github.com/danny-avila/LibreChat/assets/32828263/3cb4b063-541f-4f0a-bea8-a04dd36e6bf4)

## Linux Installation

1. Copy the command from: **[https://ngrok.com/download](https://ngrok.com/download)** choosing the **correct** architecture.
2. Run the command in the terminal
3. Copy your auth token from: **[https://dashboard.ngrok.com/get-started/your-authtoken](https://dashboard.ngrok.com/get-started/your-authtoken)**
4. run the following command: `ngrok config add-authtoken <your token>`
5. If you haven't done so already, start LibreChat normally.
6. run the following command: `ngrok http 3080`

## Mac Installation

1. Download the ZIP file from: **[https://ngrok.com/download](https://ngrok.com/download)**
2. Extract the contents of the ZIP file using a suitable Mac application like Unarchiver.
3. Open Terminal.
4. Navigate to the directory where you extracted ngrok using the `cd` command.
5. Run ngrok by typing `./ngrok`.
6. Copy your auth token from: **[https://dashboard.ngrok.com/get-started/your-authtoken](https://dashboard.ngrok.com/get-started/your-authtoken)**
7. In the terminal where you ran ngrok, enter the following command: `ngrok authtoken <your token>`
8. If you haven't done so already, start LibreChat normally.
9. In the terminal where you ran ngrok, enter the following command: `./ngrok http 3080`



================================================
FILE: pages/docs/remote/railway.mdx
================================================
---
title: Railway (one-click)
description: Deploying LibreChat on Railway
---

# Deploying LibreChat on Railway (One-Click Install)

Railway provides a one-click install option for deploying LibreChat, making the process even simpler. Here's how you can do it:

## Steps

### **Visit the LibreChat repository**

Go to the [LibreChat repository](https://github.com/danny-avila/LibreChat) on GitHub.

### **Create a Railway account**

[Sign up for a Railway account](https://railway.app?referralCode=HI9hWz) if you don't already have one (this link includes a referral code that supports the LibreChat project).

### **Click the "Deploy on Railway" button**

<p align="left">
    <a href="https://railway.app/template/b5k2mn?referralCode=HI9hWz">
        <img src="https://railway.app/button.svg" alt="Deploy on Railway" height="30"/>
    </a>
</p>

(The button is also available in the repository's README file)


### **Configure environment variables**

Railway will automatically detect the required environment variables for LibreChat. Review the configuration of the three containers and click `Save Config` after reviewing each of them.

![image](https://github.com/danny-avila/LibreChat/assets/32828263/4417e997-621c-44b6-8d2d-94d7e4e1a2bf)

The default configuration will get you started, but for more advanced features, you can consult our documentation on the subject: [Environment Variables](/docs/configuration/dotenv)

### **Deploy**

Once you've filled in the required environment variables, click the "Deploy" button. Railway will handle the rest, including setting up a PostgreSQL database and building/deploying your LibreChat instance.

![image](https://github.com/danny-avila/LibreChat/assets/32828263/d94e20c6-0ae7-42af-8937-7fbd34d63a3b)

### **Access your LibreChat instance**

After the deployment is successful, Railway will provide you with a public URL where you can access your LibreChat instance.

That's it! You have successfully deployed LibreChat on Railway using the one-click install process. You can now start using and customizing your LibreChat instance as needed.

## Additional Tips

- Regularly check the LibreChat repository for updates and redeploy your instance to receive the latest features and bug fixes.
- You can find the "redeploy" option in Railway after you login by clicking the 3 dots to the right of "view logs" button

For more detailed instructions and troubleshooting, refer to the official LibreChat documentation and the Railway guides.



================================================
FILE: pages/docs/remote/traefik.mdx
================================================
---
title: Traefik
description: Learn how to use Traefik as a reverse proxy and load balancer to expose your LibreChat instance securely over HTTPS with automatic SSL/TLS certificate management.
---

# Using Traefik with LibreChat on Docker

[Traefik](https://traefik.io/) is a modern HTTP reverse proxy and load balancer that makes it easy to deploy and manage your services. If you're running LibreChat on Docker, you can use Traefik to expose your instance securely over HTTPS with automatic SSL certificate management.

## Prerequisites

- Docker and Docker Compose installed on your system
- A domain name pointing to your server's IP address

## Configuration

### Configure Traefik and LibreChat

    In your docker-compose.override.yml file, add the following configuration:

```yaml filename="docker-compose.override.yml"
version: '3'

services:
    api:
      labels:
        - "traefik.enable=true"
        - "traefik.http.routers.librechat.rule=Host(`your.domain.name`)"
        - "traefik.http.routers.librechat.entrypoints=websecure"
        - "traefik.http.routers.librechat.tls.certresolver=leresolver"
        - "traefik.http.services.librechat.loadbalancer.server.port=3080"
      networks:
        - librechat_default
      volumes:
        - ./librechat.yaml:/app/librechat.yaml
  
    traefik:
      image: traefik:v3.0
      ports:
        - "80:80"
        - "443:443"
      volumes:
        - "/var/run/docker.sock:/var/run/docker.sock:ro"
        - "./letsencrypt:/letsencrypt"
      networks:
        - librechat_default
      command:
        - "--log.level=DEBUG"
        - "--api.insecure=true"
        - "--providers.docker=true"
        - "--providers.docker.exposedbydefault=false"
        - "--entrypoints.web.address=:80"
        - "--entrypoints.websecure.address=:443"
        - "--certificatesresolvers.leresolver.acme.tlschallenge=true"
        - "--certificatesresolvers.leresolver.acme.email=your@email.com"
        - "--certificatesresolvers.leresolver.acme.storage=/letsencrypt/acme.json"

# other configs here #

# NOTE: This needs to be at the bottom of your docker-compose.override.yml
networks:
  librechat_default:
    external: true
```

  Replace `your@email.com` with your email address for Let's Encrypt certificate notifications.

  see: [Docker Override](/docs/configuration/docker_override) for more info.

### Start the containers

  ```bash filename="Start the containers"
  docker-compose up -d
  ```

  This will start Traefik and LibreChat containers. Traefik will automatically obtain an SSL/TLS certificate from Let's Encrypt and expose your LibreChat instance securely over HTTPS.

You can now access your LibreChat instance at `https://your.domain.name`. Traefik will handle SSL/TLS termination and reverse proxy requests to your LibreChat container.

## Additional Notes

- The Traefik configuration listens on ports 80 and 443 for HTTP and HTTPS traffic, respectively. Ensure that these ports are open on your server's firewall.
- Traefik stores SSL/TLS certificates in the `./letsencrypt` directory on your host machine. You may want to back up this directory periodically.
- For more advanced configuration options, refer to the official Traefik documentation: [https://doc.traefik.io/](https://doc.traefik.io/)

## Static File Caching and Compression

LibreChat now supports static file caching and compression natively. If you're using Traefik to handle compression, you should disable compression in LibreChat to avoid redundant processing. You can do this by setting the `DISABLE_COMPRESSION` environment variable to `true` in your LibreChat configuration.

```.env
# .env file
DISABLE_COMPRESSION=true
```

This will prevent LibreChat from compressing static files, allowing Traefik to handle compression more efficiently.

For more information on static file handling in LibreChat, including caching options, refer to the [Static File Handling](/docs/configuration/dotenv#static-file-handling) documentation.



================================================
FILE: pages/docs/translation/_meta.ts
================================================
export default {
  index: 'Intro',
}



================================================
FILE: pages/docs/translation/index.mdx
================================================
---
title: Intro
description: A guide to contributing translations for LibreChat.
---

# Translation Guide

Thank you for your interest in translating LibreChat! We rely on community contributions to make our application accessible to users around the globe. All translations are managed via [Locize](https://locize.com), a robust translation management system that seamlessly integrates with our project.

## How Translations Work

- **Centralized Management:**
All translation strings for LibreChat are maintained in one location on Locize. This centralization ensures consistency and simplifies updates across the entire application.

- **Automatic Updates:**
Changes made in Locize are automatically synchronized with our project. You can monitor the translation progress for each language through dynamic badges in our repository.

- **Community Driven:**
We welcome contributions in every language. Your help makes LibreChat accessible to a broader audience and supports users in their native languages.



## Translation Progress

Below is our current translation progress for some of the supported languages. Feel free to check these badges and help us improve the translations further:

| Language                            | Translation Progress Badge                                                                                                                                                                                                                                                                                       |
|-------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **English (en)**                    | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'en'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="EN Badge" />           |
| **Arabic (ar)**                     | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'ar'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="AR Badge" />           |
| **German (de)**                     | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'de'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="DE Badge" />           |
| **Spanish (es)**                    | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'es'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="ES Badge" />           |
| **Estonian (et)**                   | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'et'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="ET Badge" />           |
| **Persian (fa)**                    | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'fa'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="FA Badge" />           |
| **Finnish (fi)**                    | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'fi'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="FI Badge" />           |
| **French (fr)**                     | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'fr'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="FR Badge" />           |
| **Hebrew (he)**                     | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'he'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="HE Badge" />           |
| **Hungarian (hu)**                  | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'hu'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="HU Badge" />           |
| **Indonesian (id)**                 | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'id'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="ID Badge" />           |
| **Italian (it)**                    | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'it'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="IT Badge" />           |
| **Japanese (ja)**                   | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'ja'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="JA Badge" />           |
| **Georgian (ka)**                   | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'ka'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="KA Badge" />           |
| **Korean (ko)**                     | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'ko'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="KO Badge" />           |
| **Dutch (nl)**                      | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'nl'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="NL Badge" />           |
| **Polish (pl)**                     | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'pl'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="PL Badge" />           |
| **Portuguese (pt-PT)**              | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'pt-PT'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="PT-PT Badge" />     |
| **Brazilian Portuguese (pt-BR)**    | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'pt-BR'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="PT-BR Badge" />     |
| **Russian (ru)**                    | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'ru'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="RU Badge" />           |
| **Swedish (sv)**                    | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'sv'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="SV Badge" />           |
| **Thai (th)**                       | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'th'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="TH Badge" />           |
| **Turkish (tr)**                    | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'tr'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="TR Badge" />           |
| **Vietnamese (vi)**                 | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'vi'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="VI Badge" />           |
| **Chinese (Simplified) (zh-Hans)**  | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'zh-Hans'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="ZH-HANS Badge" /> |
| **Chinese (Traditional) (zh-Hant)** | <img src="https://img.shields.io/badge/dynamic/json?style=for-the-badge&color=2096F3&label=Locize&query=$.versions%5B'latest'%5D.languages%5B'zh-Hant'%5D.translatedPercentage&url=https%3A%2F%2Fapi.locize.app%2Fbadgedata%2F4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%25+translated" alt="ZH-HANT Badge" /> |

---



## Getting Started

Before you begin translating, please follow the steps below to set up your Locize account and start contributing.




### Step 1: Create a Locize Account

1. Visit the Registration Page

    Choose your preferred language and click the corresponding link to register:

    - **[English (en)](https://www.locize.app/register?invitation=tTHVYwI9TzWuOeJnKyWxSuuaDrSS5TsZqRh0BAHzebuuljz5OUXnlqbksio5WnXp)**
    - **[Arabic (ar)](https://www.locize.app/register?invitation=Cn7hc6teYyY89nvQdosfXkubbj2MZi2XMzcxUP6fGglcPkAgd1AwS5Pfbr1Wu4lz)**
    - **[German (de)](https://www.locize.app/register?invitation=rAXIyYNuO53txcygphdOClUR5YnNccd1MZ1Vs66p4ziOqHfM3MFiKnymdK6wLMpW)**
    - **[Spanish (es)](https://www.locize.app/register?invitation=gkrRvUjxvFnRfUtynbaREj2zdbvd2FU95OFGcixGMOkcCxSwmrvOBclBZJWmERw6)**
    - **[Estonian (et)](https://www.locize.app/register?invitation=q1ye9gNpYsVKvs2JS5CEjp4SBy6ovq2aUeIhAMsRoW2iVcdfxpc4GiOaHGDV85VZ)**
    - **[Persian (fa)](https://www.locize.app/register?invitation=DfuKT56Y4KxlPOm6biYl9zbgMo3kBjWJA3Flbd7X5H4hJluUWTsFiQ3MQH07fH0d)**
    - **[Finnish (fi)](https://www.locize.app/register?invitation=weERAttD7ax0Zfo5w9LyfWbaP50WPbS4Vk7BF1P6wM0fe5Q0xlrJACVjTAGbkv2c)**
    - **[French (fr)](https://www.locize.app/register?invitation=wz5EbZiwE9Bxev4TTAyG09PKvnFSQoOaLomxVgEKOyaLhm9oCDdhDi4TkJp8rcq5)**
    - **[Hebrew (he)](https://www.locize.app/register?invitation=3LSHQxUsHYumNhw5ZJNu6Re6x699i6RGnpdKzt2BrDeNFNObxjj2lKXfZFUhb1jP)**
    - **[Hungarian (hu)](https://www.locize.app/register?invitation=3TaJ2SnDnYfTJQLalKK751cQ04AK5A2gRahC2poeZYXj8MymZ5T0rt4V688e0H7M)**
    - **[Indonesian (id)](https://www.locize.app/register?invitation=S94A2D6glr7w64Nf27XwmvzkcnTRAWhxFoxcl7ZRnmm1dMx23kfpZCx8ROlqb6X2)**
    - **[Italian (it)](https://www.locize.app/register?invitation=LI6zvAH82797Gro6lc490d34zrw8vthQPdN01n9nfk2c1LW5VVpf4Db6WL5GFHir)**
    - **[Japanese (ja)](https://www.locize.app/register?invitation=VVJLuv7WjjBs0wShxZheCIvwzk7XszVsHCAMZmCQ34SJYrwIeB1GW6u2vkYNJdzk)**
    - **[Georgian (ka)](https://www.locize.app/register?invitation=WK5hB5nznxioaDGOXXPDmcULrNfFA3wkcvS2YiUbrU6b73gKOWix5Qg7uGjpPtkH)**
    - **[Korean (ko)](https://www.locize.app/register?invitation=nbL4LZMwehlTvFKNmNGgTTJt9YsZuuyCMjF9yAc92bVIWEjAc9C1G0ujsEl76dYb)**
    - **[Dutch (nl)](https://www.locize.app/register?invitation=BQoligTqND5E4jlGmXJZVzkLOpo4pTf9wUyN19zZbgNB38JHaciJ2FnIbsLOXPbe)**
    - **[Polish (pl)](https://www.locize.app/register?invitation=ZPiib1OoPM3OHBUow79I8iQNiFk4SHO7HASO3rHdJpKSlJwA4oxsORR2w4yJaPig)**
    - **[Portuguese (pt-PT)](https://www.locize.app/register?invitation=c11DbWJk7O5TNIKon9leIfIbqARz60URaGB0WFQPT2ym3wxUDR8DgIiOlXNIBz13)**
    - **[Brazilian Portuguese (pt-BR)](https://www.locize.app/register?invitation=99JdsEgKuVR9XhQJWi91Jq0LkqigiSz4EOf4gVGSr5RwZPp6ad4JQZLHVwhumNvB)**
    - **[Russian (ru)](https://www.locize.app/register?invitation=rz4C3pVFdfr5XPPNTsYJvjjnC7vHDVWVOfyUzCf33MxhccdYB7vM7jxcHLacGl14)**
    - **[Swedish (sv)](https://www.locize.app/register?invitation=RYtYhip5O5ACNCth1cIZpByGnZC1b3JttimEe8mrz5NDyEjVAs1PVcMIQ1in4j7D)**
    - **[Thai (th)](https://www.locize.app/register?invitation=3xMjJPqupRNO2BU7K3WMWhxFtoGNzL97hxKIGjE8Yoa4v7lJj8ZjTy5p5dmcfLjW)**
    - **[Turkish (tr)](https://www.locize.app/register?invitation=x3Ov59Gdrk2b76gn5pSVCwuekDs817YOYElXJn9zCYClPG2XlBORQDRygZmdBH4B)**
    - **[Vietnamese (vi)](https://www.locize.app/register?invitation=rhADX8GuhgQmYrmbHT13YVg2WqMLJpgPdh1OBuujn9GoNUVW6RPipYvC20aH1xcQ)**
    - **[Chinese (Simplified) (zh-Hans)](https://www.locize.app/register?invitation=HXXM2h8SJsBLIPJol3usRct0aPPcr809xzfV4DQHolyEfeSjBwEjChd37vE3ZrRw)**
    - **[Chinese (Traditional) (zh-Hant)](https://www.locize.app/register?invitation=9PWBDcMascIBGG6wwobkVT6cL7p7IncFZVqwIe0e7VZd14MJOAMQGk6IjlvgmA00)**


2. **Fill in Your Details:**
Enter your email, password, and any other required information, then click **Sign Up**.

![Create Account](https://github.com/user-attachments/assets/c1ccbfd9-2131-4020-a4b3-7283bf733828)



### Step 2: Explore the Locize Dashboard

After signing up, you’ll be directed to the Locize dashboard, where you can see an overview of the translation project.

- **Dashboard Overview:**
This page displays the available languages and progress statistics for the project.

![Landing Page](https://github.com/user-attachments/assets/818b3d30-3f5a-48e6-8b36-0be3d0691045)



### Step 3: Select Your Language

1. **Open the Language Dropdown:**
Click the dropdown menu that lists all supported languages.

2. **Choose Your Preferred Language:**
For example, if you want to translate into Dutch, scroll down and select **Dutch**.

![Dropdown with Languages](https://github.com/user-attachments/assets/93f713bd-0008-43bc-ba84-b7730d4cfedf)



### Step 4: Navigate to the Translation Page

After selecting your language, click on the translation progress indicator (for example, "35.61% translated"). This will take you to the page where you can contribute translations.

![Selected Dutch Click on Translation](https://github.com/user-attachments/assets/03322ab5-82ad-4958-9008-5e7e17363ca8)



### Step 5: Contribute Your Translation

1. **Browse the Translation Strings:**
The interface displays a list of translation keys along with their original texts.

2. **Select a String to Translate:**
Click on the string you wish to work on.

3. **Enter Your Translation:**
Type your translated text into the input field provided next to the original text.

4. **Review Your Work:**
Ensure that your translation is accurate and clear.

![Start with Translating](https://github.com/user-attachments/assets/bc3e2a47-c297-476e-945b-f7f0b1356ffb)



### Step 6: Save and Submit Your Translation

1. **Submit Your Translation:**
Once you’re satisfied with your translation, click the **Save** button to submit it for review.

2. **Pending Review:**
Your submitted translation will be marked as pending and will be reviewed by project maintainers.

![Saved Submitted Translation Waiting for Review](https://github.com/user-attachments/assets/a26ae981-0c32-47a0-a296-530ce671375a)



### Step 7: Translation Approval

After review, your translation will be approved and integrated into the project.

- **Approved Translation:**
Once approved, your contribution will be reflected in the Locize dashboard and the overall translation progress.

![Translation Approved](https://github.com/user-attachments/assets/93c3e512-616f-40d9-af2f-9f7c19d53148)

---

## Handling `{{0}}` and `{{1}}` in Translation Strings

Sometimes translation strings need to include dynamic content. These dynamic parts, called **interpolations**, are represented by placeholders enclosed in double curly brackets (e.g., `{{0}}` or `{{1}}`). When translating such strings, it's important to maintain these placeholders in the correct positions.

Below are two examples to help guide you:

### Example 1: Single Interpolation

Consider the translation key `com_assistants_completed_action`. The original English text is:

```text
Talked to {{0}}
```

For the German translation, ensure the placeholder remains intact and is placed appropriately:

```text
Mit {{0}} gesprochen
```

This image shows how a single interpolation is represented in a translation string:

![Single Interpolation](https://github.com/user-attachments/assets/384ef7c1-9b02-490c-8ca4-b7f74943893f)

---

### Example 2: Multiple Interpolations

Now, look at the translation key `com_files_number_selected`, which includes two placeholders. The English version is:

```text
{{0}} of {{1}} item(s) selected
```

In the German translation, both placeholders must be preserved and positioned correctly:

```text
{{0}} von {{1}} Datei(en) ausgewählt
```

This image illustrates how multiple interpolations appear in translation strings:

![Multiple Interpolations](https://github.com/user-attachments/assets/f3376487-e092-442a-b849-b6ab5d5b390d)

---


## Adding a New Language

If you don't see your language listed in our translation table, you can help us expand our language support:

1. **Create a New Issue:**
Open a new issue in our GitHub repository: [LibreChat Issues](https://github.com/danny-avila/LibreChat/issues).

2. **Select the New Language Request Template:**
Use the **New Language Request** template and provide:
 - The full name of your language (e.g., Spanish, Mandarin).
 - The [ISO 639-1](https://www.w3schools.com/tags/ref_language_codes.asp) code for your language (e.g., `es` for Spanish).

3. **Collaborate with Maintainers:**
Our maintainers will review your request and work with you to integrate the new language. Once approved, your language will appear in the translation progress table, and you can start contributing.

---

## Need Help?

If you have any questions or need assistance, please feel free to:

- **Open an Issue:**
Submit an issue in our repository: [LibreChat Issues](https://github.com/danny-avila/LibreChat/issues).

- **Join Our Discord Community:**
Connect with fellow translators on our [Discord server](https://discord.librechat.ai).

- **Contact a Maintainer:**
Reach out directly to one of our project maintainers for additional support.

Your contributions help make LibreChat accessible to users worldwide. Thank you for supporting our project, and happy translating!


---

We thank [Locize](https://locize.com) for their translation management tools that support multiple languages in LibreChat.

<p align="center">
    <a href="https://locize.com" target="_blank" rel="noopener noreferrer">
        <img src="https://github.com/user-attachments/assets/d6b70894-6064-475e-bb65-92a9e23e0077" alt="Locize Logo" height="50"></img>
    </a>
</p>


================================================
FILE: pages/docs/user_guides/_meta.ts
================================================
export default {
  index: 'Intro',
}



================================================
FILE: pages/docs/user_guides/ai_overview.mdx
================================================
---
title: AI Overview
description: Understand the pre-configured endpoints in LibreChat
---

# AI Overview

LibreChat allows you to configure and integrate various AI providers, APIs, and their corresponding credentials. This enables you to utilize different AI models, settings, and functionalities based on your needs and requirements.

## Key Concepts

- **Endpoints**: An endpoint refers to an AI provider, configuration, or API that determines the available models and settings for a chat request. Examples include OpenAI, Google, Plugins, Anthropic, and others.

- **Presets**: A preset is a saved combination of an endpoint, model, and conversation settings. You can create and manage multiple presets to suit different use cases.

- **Default Endpoint**: If you have multiple endpoints configured, you can specify a default endpoint to be used when creating a new conversation.

- **Default Preset**: Similarly, you can set a default preset to be used automatically when starting a new conversation.

## Functionality

1. **AI Providers**: Set up various pre-configured AI providers by providing the necessary credentials and API keys.

2. **Manage Endpoints**: Enable or disable different endpoints based on your requirements.

3. **Create and Manage Presets**: Define and save specific combinations of endpoints, models, and conversation settings as presets.

4. **Set Default Endpoint and Preset**: Specify a default endpoint and preset to streamline the process of starting new conversations. Here's a video to demonstrate: **[Setting a Default Preset](https://github.com/danny-avila/LibreChat/assets/110412045/bbde830f-18d9-4884-88e5-1bd8f7ac585d)**

5. **Customize Configurations**: Explore advanced configuration options, such as adding custom endpoints like Ollama, Mistral AI or Openrouter.



================================================
FILE: pages/docs/user_guides/index.mdx
================================================
---
title: Intro
description: Collection of "user guides" providing an overview of various features offered by LibreChat
---

import Image from 'next/image'

# User Guides

Whether you're a new user or looking to explore more advanced features, this comprehensive collection is designed to help you navigate through the various functionalities of LibreChat seamlessly.

<div style={{padding: "20px", display: "flex", justifyContent: "center", alignItems: "center", flexDirection: "column"}}>
  <div className="image-light-theme">
    <Image src="https://github.com/danny-avila/LibreChat/assets/32828263/cf0f3231-287a-407f-bd4d-3d5bad94e893" alt="ipad-light" width={1024} height={512} style={{borderRadius: "5px"}} />
  </div>

  <div className="image-dark-theme">
    <Image src="https://github.com/danny-avila/LibreChat/assets/32828263/a03ee02d-5099-4220-95b0-bfa2d3b00b4d" alt="ipad-dark" width={1024} height={512} style={{borderRadius: "5px"}} />
  </div>
</div>



================================================
FILE: pages/docs/user_guides/mongodb.mdx
================================================
---
title: MongoDB
description: Why LibreChat Uses MongoDB
---

# Why LibreChat Uses MongoDB

MongoDB, a popular NoSQL database, was chosen as the core database for LibreChat due to its flexibility, scalability, and ability to handle diverse data structures efficiently. Here are some key reasons why MongoDB is an excellent fit for LibreChat:

<div style={{padding: "20px", display: "flex", justifyContent: "center", alignItems: "center", flexDirection: "column"}}>
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/84d4a608-1f73-41c9-a026-7772a74d205b" alt="mongodb" style={{borderRadius: "10px"}} />
</div>

## 1. Flexible Data Model
MongoDB's document-based data model allows for storing and retrieving data in a flexible and dynamic manner. Unlike traditional relational databases, MongoDB doesn't require a fixed schema, making it easier to adapt to changing data requirements. This flexibility is essential for LibreChat, as it needs to store various types of data, such as conversation histories, user profiles, presets, API keys, and more, without being constrained by a rigid table structure.

## 2. Efficient Storage of Conversation Histories
One of the primary use cases for LibreChat is to store and retrieve conversation histories. MongoDB's ability to store nested data structures as JSON-like documents makes it an excellent choice for storing conversation histories, which can include complex data structures like messages, timestamps, and metadata.

## 3. Secure Storage of Sensitive Data
LibreChat handles sensitive data, such as API keys and encrypted user passwords. MongoDB's built-in support for data encryption at rest and in transit ensures that this sensitive information remains secure and protected from unauthorized access.

## 4. Horizontal Scalability
As LibreChat grows and attracts more users, its data storage requirements will increase. MongoDB's horizontal scalability allows for scaling out by adding more servers to a cluster, providing the ability to handle larger amounts of data and higher traffic loads without compromising performance.

## 5. Cross-Device Accessibility
LibreChat aims to provide a seamless experience across multiple devices, allowing users to access their data and conversation histories from different devices. MongoDB's replication and sharding capabilities ensure that data is consistently available and accessible, enabling users to pick up their conversations where they left off, regardless of the device they're using.

## 6. Developer Productivity
MongoDB's intuitive query language and rich ecosystem of tools and libraries contribute to faster development cycles and increased developer productivity. This aligns well with LibreChat's goal of being an open-source project, fostering collaboration and contributions from the developer community.

By leveraging MongoDB's strengths, LibreChat can efficiently manage and store diverse data structures, ensure data security and availability, and provide a seamless cross-device experience for its users. MongoDB's flexibility, scalability, and developer-friendly features make it an ideal choice for powering the core functionalities of LibreChat.


## Note

<Callout type="warning" title="CPU compatibility">
**Note:** If you're running LibreChat on a processor that doesn't have SSE4.2, AVX support, or other required CPU features, you'll need to use an older but compatible version of MongoDB with the Docker installation. Specifically, you should use the `mongo:4.4.18` image, which is compatible with processors without these features.

To use this older MongoDB version with the LibreChat Docker installation, you'll need to utilize the `docker-compose.override.yml` file. This override file allows you to specify the MongoDB version you want to use, overriding the default version included in the main `docker-compose.yml` file.

For more information on using the `docker-compose.override.yml` file and configuring an older MongoDB version for your Docker installation, please refer to our [Docker Override Configuration Guide](/docs/configuration/docker_override).
</Callout>


================================================
FILE: pages/docs/user_guides/presets.mdx
================================================
---
title: Presets
description: The "presets" feature in LibreChat is a powerful tool that allows users to save and load predefined settings for their conversations. Users can import and export these presets as JSON files, set a default preset, and share them with others on Discord.
---
# Guide to Using the "Presets" Feature

The "presets" feature in our app is a powerful tool that allows users to save and load predefined settings for their conversations. Users can import and export these presets as JSON files, set a default preset, and share them with others on Discord.

![image](https://github.com/danny-avila/LibreChat/assets/32828263/8c39ad89-71ae-42c6-a792-3db52d539fcd)

## Create a Preset:

- Go in the model settings

![image](https://github.com/danny-avila/LibreChat/assets/32828263/2fc883e9-f4a3-47ac-b375-502e82234194)

- Choose the model, give it a name, some custom instructions, and adjust the parameters if needed

![image](https://github.com/danny-avila/LibreChat/assets/32828263/090dc065-f9ea-4a43-9380-e6d504e64992)

- Test it

![image](https://github.com/danny-avila/LibreChat/assets/32828263/8a383495-0d5e-4ab7-93a7-eca5388c3f6f)

- Go back in the model advanced settings, and tweak it if needed. When you're happy with the result, click on `Save As Preset` (from the model advanced settings)

![image](https://github.com/danny-avila/LibreChat/assets/32828263/96fd88ec-b4b6-4de0-a7d7-f156fdace354)

- Give it a proper name, and click save

![image](https://github.com/danny-avila/LibreChat/assets/32828263/76ad8db4-a949-4633-8a5f-f9e8358d57f3)

- Now you can select it from the preset menu! 

![image](https://github.com/danny-avila/LibreChat/assets/32828263/81271990-2739-4f5c-b1a5-7d7deeaa385c)

## Parameters Explained:

- **Preset Name:**
  - This is where you name your preset for easy identification.

- **Endpoint:**
  - Choose the endpoint, such as openAI, that you want to use for processing the conversation.

- **Model:**
  - Select the model like `gpt-3.5-turbo` that will be used for generating responses.

- **Custom Name:**
  - Optionally provide a custom name for your preset. This is the name that will be shown in the UI when using it.

- **Custom Instructions:**
  - Define instructions or guidelines that will be displayed before each prompt to guide the user in providing input.

- **Temperature:**
  - Adjust this parameter to control the randomness of the model's output. A higher value makes the output more random, while a lower value makes it more focused and deterministic.

- **Top P:**
  - Control the nucleus sampling parameter to influence the diversity of generated text. Lower values make text more focused while higher values increase diversity.

- **Frequency Penalty:**
  - Use this setting to penalize frequently occurring tokens and promote diversity in responses.

- **Presence Penalty:**
  - Adjust this parameter to penalize new tokens that are introduced into responses, controlling repetition and promoting consistency.

## Importing/Exporting Presets

You can easily import or export presets as JSON files by clicking on either 'Import' or 'Export' buttons respectively. This allows you to share your customized settings with others or switch between different configurations quickly.

![image](https://github.com/danny-avila/LibreChat/assets/32828263/b9ef56e2-393e-45eb-b72b-8d568a13a015)

To export a preset, first go in the preset menu, then click on the button to edit the selected preset

![image](https://github.com/danny-avila/LibreChat/assets/32828263/3fb065e6-977b-49b4-9fc6-de55b9839031)

Then in the bottom of the preset settings you'll have the option to export it.

<p align="left">
    <img src="https://github.com/danny-avila/LibreChat/assets/32828263/a624345f-e3e6-4192-8384-293ba6ce54cc" width="50%"/>
</p>

## Setting Default Preset

Choose a preset as default so it loads automatically whenever you start a new conversation. This saves time if you often use specific settings.

![image](https://github.com/danny-avila/LibreChat/assets/32828263/5912650d-49b6-40d3-b9ad-ff2ff7fbe3e7)
![image](https://github.com/danny-avila/LibreChat/assets/32828263/dcfb5e27-f60b-419e-b387-25db85fa6a63)

## Sharing on Discord

Join us on [discord](https://discord.librechat.ai) and see our **[#presets ](https://discord.com/channels/1086345563026489514/1093249324797935746)** channel where thousands of presets are shared by users worldwide. Check out pinned posts for popular presets!

